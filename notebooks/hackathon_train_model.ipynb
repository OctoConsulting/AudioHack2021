{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1df4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import warnings                               # Suppressing annoying Librosa warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from os import walk\n",
    "import shutil                                 # For shell commands, accessing and manipulating files and directories\n",
    "\n",
    "from icecream import ic\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd                           # Data analysis\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import metrics as sk\n",
    "\n",
    "import librosa                                # Manipulating sound files\n",
    "from librosa import display\n",
    "import IPython.display as ipd\n",
    "import soundfile\n",
    "import glob \n",
    "\n",
    "\n",
    "import tensorflow as tf                       # Deep Learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LambdaCallback, TensorBoard\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt               # Visualizations\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import normalize   # Data Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f26d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/model0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131beebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, _ = librosa.load('./data/sounds/0000.WAV')\n",
    "sample = np.array(sample)\n",
    "sample = np.reshape(sample, (1, sample.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7214fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_dict = {0: -30,\n",
    " 1: 0,\n",
    " 2: 12,\n",
    " 3: 20,\n",
    " 4: 23,\n",
    " 5: 30,\n",
    " 6: 40,\n",
    " 7: 60,\n",
    " 8: 90,\n",
    " 9: 130,\n",
    " 10: 180}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c84ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_inference(inference, window=False):\n",
    "\n",
    "    path = './compass.png'\n",
    "\n",
    "    #Creates a Tkinter-compatible photo image, which can be used everywhere Tkinter expects an image object.\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((300,300), Image.ANTIALIAS)\n",
    "    img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "    # Start coordinate, here (0, 0) \n",
    "    # represents the top left corner of image \n",
    "    start_point = (150, 150) \n",
    "\n",
    "    length = 71\n",
    "    angle = np.radians(inference)\n",
    "\n",
    "    # End coordinate\n",
    "    end_point = (int(150 + length * np.sin(angle)), int(150 - length * np.cos(angle)))\n",
    "\n",
    "    # Green color in BGR \n",
    "    color = (255, 0, 0) \n",
    "\n",
    "    # Line thickness of 9 px \n",
    "    thickness = 5\n",
    "\n",
    "\n",
    "    img = cv2.arrowedLine(np.asarray(img), start_point, end_point,\n",
    "                                         color, thickness) \n",
    "    # text\n",
    "    text = f'xxx m to {inference} degrees'\n",
    "\n",
    "    # font\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # org\n",
    "    org = (35, 290)\n",
    "\n",
    "    # fontScale\n",
    "    fontScale = 0.75\n",
    "\n",
    "    # Red color in BGR\n",
    "    color = (255, 0, 0)\n",
    "\n",
    "    # Line thickness of 2 px\n",
    "    thickness = 2\n",
    "\n",
    "    # Using cv2.putText() method\n",
    "    image = cv2.putText(np.asarray(img), text, org, font, fontScale, \n",
    "                     color, thickness, cv2.LINE_AA, False)\n",
    "\n",
    "    if window:\n",
    "        img = ImageTk.PhotoImage(Image.fromarray(img), master=window)\n",
    "    else:\n",
    "        img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80f400d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(angle=False):\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Join\")\n",
    "    window.geometry(\"300x300\")\n",
    "    window.configure(background='grey')\n",
    "\n",
    "    img = Image.open('./compass.png')\n",
    "    img = img.resize((300,300), Image.ANTIALIAS)\n",
    "    img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "    img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "\n",
    "    \n",
    "    if angle:\n",
    "        path = './compass.png'\n",
    "\n",
    "        #Creates a Tkinter-compatible photo image, which can be used everywhere Tkinter expects an image object.\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((300,300), Image.ANTIALIAS)\n",
    "        img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "        # Start coordinate, here (0, 0) \n",
    "        # represents the top left corner of image \n",
    "        start_point = (150, 150) \n",
    "\n",
    "        length = 71\n",
    "        angle = np.radians(angle)\n",
    "\n",
    "        # End coordinate\n",
    "        end_point = (int(150 + length * np.sin(angle)), int(150 - length * np.cos(angle)))\n",
    "\n",
    "        # Green color in BGR \n",
    "        color = (255, 0, 0) \n",
    "\n",
    "        # Line thickness of 9 px \n",
    "        thickness = 5\n",
    "\n",
    "\n",
    "        img = cv2.arrowedLine(np.asarray(img), start_point, end_point,\n",
    "                                             color, thickness) \n",
    "        img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "        \n",
    "\n",
    "            \n",
    "    panel = tk.Label(window, image = img)\n",
    "\n",
    "    #The Pack geometry manager packs widgets in rows or columns.\n",
    "    panel.pack(side = \"bottom\", fill = \"both\", expand = \"yes\")\n",
    "\n",
    "    #Start the GUI\n",
    "\n",
    "    window.lift()\n",
    "    window.attributes('-topmost',True)\n",
    "    #window.after_idle(root.attributes,'-topmost',False)\n",
    "\n",
    "    window.mainloop()\n",
    "    \n",
    "    return window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "011e7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your folder path is \" ./unity \"\n",
      "Added:  0016.wav\n",
      "Angle = 40\n"
     ]
    }
   ],
   "source": [
    "window = tk.Tk()\n",
    "window.title(\"Join\")\n",
    "window.geometry(\"300x300\")\n",
    "window.configure(background='grey')\n",
    "\n",
    "img = Image.open('./compass.png')\n",
    "img = img.resize((300,300), Image.ANTIALIAS)\n",
    "img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "\n",
    "panel = tk.Label(window, image = img)\n",
    "\n",
    "#The Pack geometry manager packs widgets in rows or columns.\n",
    "panel.pack(side = \"bottom\", fill = \"both\", expand = \"yes\")\n",
    "\n",
    "#Start the GUI\n",
    "\n",
    "window.lift()\n",
    "window.attributes('-topmost',True)\n",
    "#window.after_idle(root.attributes,'-topmost',False)\n",
    "\n",
    "path = './unity'\n",
    "print('Your folder path is \"',path,'\"')\n",
    "before = dict ([(f, None) for f in os.listdir (path)])\n",
    "\n",
    "def dothis():\n",
    "    global before\n",
    "    after = dict ([(f, None) for f in os.listdir (path)])\n",
    "    added = [f for f in after if not f in before]\n",
    "    if added:\n",
    "            print(\"Added: \", \", \".join (added))\n",
    "            sample, _ = librosa.load('./unity/'+added[-1])\n",
    "            sample = np.array(sample)\n",
    "            sample = np.reshape(sample, (1, sample.shape[0], 1))\n",
    "            print(f'Angle = {ang_dict[np.argmax(model.predict(sample))]}')\n",
    "            before = dict ([(f, None) for f in os.listdir (path)])\n",
    "            change_image(sample)\n",
    "    window.after(1000,dothis)\n",
    "\n",
    "def change_image(obs):\n",
    "    global window\n",
    "    img = visualize_inference(ang_dict[np.argmax(model.predict(obs))], window=window)\n",
    "    panel.configure(image=img)\n",
    "    panel.image = img\n",
    "\n",
    "    \n",
    "    \n",
    "window.after(500, dothis)\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './unity'\n",
    "print('Your folder path is \"',path,'\"')\n",
    "before = dict ([(f, None) for f in os.listdir (path)])\n",
    "\n",
    "window = create_window()\n",
    "\n",
    "while 1:\n",
    "        after = dict ([(f, None) for f in os.listdir (path)])\n",
    "        added = [f for f in after if not f in before]\n",
    "        if added:\n",
    "                print(\"Added: \", \", \".join (added))\n",
    "                sample, _ = librosa.load('./unity/'+added[-1])\n",
    "                sample = np.array(sample)\n",
    "                sample = np.reshape(sample, (1, sample.shape[0], 1))\n",
    "                print(f'Angle = {ang_dict[np.argmax(model.predict(sample))]}')\n",
    "                before = dict ([(f, None) for f in os.listdir (path)])\n",
    "                \n",
    "\n",
    "                #window.panel.configure(image=img)\n",
    "                #window.panel.image = img\n",
    "                \n",
    "                img = visualize_inference(ang_dict[np.argmax(model.predict(sample))], window=window)\n",
    "                window = create_window(ang_dict[np.argmax(model.predict(sample))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e7f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import tkinter.filedialog\n",
    "from tkinter.filedialog import askdirectory\n",
    "from PIL import  Image\n",
    "\n",
    "\n",
    "class GUI(Frame):\n",
    "    \n",
    "    def __init__(self, master=None):\n",
    "        Frame.__init__(self, master)\n",
    "        w,h = 300, 300\n",
    "        master.minsize(width=w, height=h)\n",
    "        master.maxsize(width=w, height=h)\n",
    "        self.pack()\n",
    "\n",
    "        #self.file = Button(self, text='Browse', command=self.choose)\n",
    "        self.choose = Label(self, text=\"Choose file\").pack()\n",
    "        self.image = PhotoImage(file='./compass.png')\n",
    "        self.label = Label(image=self.image)\n",
    "\n",
    "\n",
    "        #self.file.pack()\n",
    "        self.label.pack()\n",
    "\n",
    "    def choose(self, angle):\n",
    "        #ifile = tkinter.filedialog.askopenfile(parent=self,mode='rb',title='Choose a file')\n",
    "        path = ifile.name\n",
    "        self.image2 = PhotoImage(file=path)\n",
    "        self.label.configure(image=self.image2)\n",
    "        self.label.image=visualize_inference(angle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './unity'\n",
    "print('Your folder path is \"',path,'\"')\n",
    "before = dict ([(f, None) for f in os.listdir (path)])\n",
    "\n",
    "window = tk.Tk()\n",
    "window.title(\"Join\")\n",
    "window.geometry(\"300x300\")\n",
    "window.configure(background='grey')\n",
    "\n",
    "img = Image.open('./compass.png')\n",
    "img = img.resize((300,300), Image.ANTIALIAS)\n",
    "img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "\n",
    "\n",
    "panel = tk.Label(window, image = img)\n",
    "\n",
    "#The Pack geometry manager packs widgets in rows or columns.\n",
    "panel.pack(side = \"bottom\", fill = \"both\", expand = \"yes\")\n",
    "\n",
    "#Start the GUI\n",
    "\n",
    "window.lift()\n",
    "window.attributes('-topmost',True)\n",
    "#window.after_idle(root.attributes,'-topmost',False)\n",
    "\n",
    "window.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "while 1:\n",
    "        after = dict ([(f, None) for f in os.listdir (path)])\n",
    "        added = [f for f in after if not f in before]\n",
    "        if added:\n",
    "                print(\"Added: \", \", \".join (added))\n",
    "                sample, _ = librosa.load('./unity/'+added[-1])\n",
    "                sample = np.array(sample)\n",
    "                sample = np.reshape(sample, (1, sample.shape[0], 1))\n",
    "                print(f'Angle = {ang_dict[np.argmax(model.predict(sample))]}')\n",
    "                before = dict ([(f, None) for f in os.listdir (path)])\n",
    "\n",
    "\n",
    "#                if 'window' in globals(): \n",
    "#                    if window.state == 'normal': \n",
    "#                        window.destroy()\n",
    "#                        created = 0\n",
    "                        \n",
    "\n",
    "\n",
    "                window = tk.Tk()\n",
    "                window.title(\"Join\")\n",
    "                window.geometry(\"300x300\")\n",
    "                window.configure(background='grey')\n",
    "                img = visualize_inference(ang_dict[np.argmax(model.predict(sample))])\n",
    "                \n",
    "                panel = tk.Label(window, image = img)\n",
    "                #The Pack geometry manager packs widgets in rows or columns.\n",
    "                panel.pack(side = \"bottom\", fill = \"both\", expand = \"yes\")\n",
    "\n",
    "                #Start the GUI\n",
    "\n",
    "                window.lift()\n",
    "                window.attributes('-topmost',True)\n",
    "                #window.after_idle(root.attributes,'-topmost',False)\n",
    "\n",
    "                window.mainloop()\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "        else:\n",
    "             before = after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06084c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import cv2\n",
    "\n",
    "#This creates the main window of an application\n",
    "window = tk.Tk()\n",
    "window.title(\"Join\")\n",
    "window.geometry(\"300x300\")\n",
    "window.configure(background='grey')\n",
    "\n",
    "path = './compass.png'\n",
    "\n",
    "#Creates a Tkinter-compatible photo image, which can be used everywhere Tkinter expects an image object.\n",
    "img = Image.open(path)\n",
    "img = img.resize((300,300), Image.ANTIALIAS)\n",
    "img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "# Start coordinate, here (0, 0) \n",
    "# represents the top left corner of image \n",
    "start_point = (150, 150) \n",
    "\n",
    "length = 71\n",
    "angle = np.radians(45)\n",
    "\n",
    "# End coordinate\n",
    "end_point = (int(150 + length * np.sin(angle)), int(150 - length * np.cos(angle)))\n",
    "  \n",
    "# Green color in BGR \n",
    "color = (255, 0, 0) \n",
    "  \n",
    "# Line thickness of 9 px \n",
    "thickness = 5\n",
    "  \n",
    "# Using cv2.arrowedLine() method \n",
    "# Draw a diagonal green arrow line\n",
    "# with thickness of 9 px \n",
    "image = cv2.arrowedLine(np.asarray(img), start_point, end_point,\n",
    "                                     color, thickness) \n",
    "\n",
    "\n",
    "img = ImageTk.PhotoImage(Image.fromarray(image))\n",
    "\n",
    "#The Label widget is a standard Tkinter widget used to display a text or image on the screen.\n",
    "panel = tk.Label(window, image = img)\n",
    "\n",
    "#The Pack geometry manager packs widgets in rows or columns.\n",
    "panel.pack(side = \"bottom\", fill = \"both\", expand = \"yes\")\n",
    "\n",
    "#Start the GUI\n",
    "\n",
    "window.lift()\n",
    "window.attributes('-topmost',True)\n",
    "#window.after_idle(root.attributes,'-topmost',False)\n",
    "\n",
    "window.mainloop()\n",
    "Tk.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4944d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for widget in window.winfo_children():\n",
    "    widget.destroy()\n",
    "window.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Image.fromarray(resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "print('Original Dimensions : ',img.shape)\n",
    " \n",
    "scale_percent = 60 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "#resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "label = tk.Label(image=Image.fromarray(resized))\n",
    "label.image = resized # keep a reference! label.pack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0308a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = -30\n",
    "\n",
    "np.radians(angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcccac",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "078c9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './lasthope/'\n",
    "files = next(os.walk(folder))\n",
    "mode = 'mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f7d78b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files[2])):\n",
    "\n",
    "    if ' (1)' in files[2][i]: ic(files[2][i])\n",
    "    re.sub(' (1)', '_', files[2][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "64d34654",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y= []\n",
    "d = []\n",
    "\n",
    "for i in range(len(files[2])):\n",
    "    x.append(files[2][i])\n",
    "    \n",
    "    y.append(int(re.split('\\.|_', files[2][i])[1]))\n",
    "    d.append(int(re.split('\\.|_', files[2][i])[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c8495e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 11,\n",
       " 11,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 135,\n",
       " 135,\n",
       " 135,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 142,\n",
       " 142,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 155,\n",
       " 155,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 167,\n",
       " 167,\n",
       " 167,\n",
       " 167,\n",
       " 167,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 170,\n",
       " 170,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 184,\n",
       " 184,\n",
       " 184,\n",
       " 185,\n",
       " 185,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 195,\n",
       " 195,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 204,\n",
       " 204,\n",
       " 205,\n",
       " 205,\n",
       " 205,\n",
       " 205,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 212,\n",
       " 212,\n",
       " 212,\n",
       " 212,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 221,\n",
       " 221,\n",
       " 221,\n",
       " 221,\n",
       " 221,\n",
       " 221,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 224,\n",
       " 224,\n",
       " 224,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 231,\n",
       " 231,\n",
       " 231,\n",
       " 231,\n",
       " 231,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 233,\n",
       " 233,\n",
       " 233,\n",
       " 233,\n",
       " 233,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 236,\n",
       " 236,\n",
       " 236,\n",
       " 236,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 23,\n",
       " ...]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bfcb0fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 20,\n",
       " 20,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 120,\n",
       " 120,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 160,\n",
       " 160,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 20,\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bins = [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 0]\n",
    "\n",
    "y_binned = [round(yy / 20)*20 % 360 for yy in y]\n",
    "\n",
    "y_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "73a8d84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110,\n",
       " 140,\n",
       " 150,\n",
       " 150,\n",
       " 20,\n",
       " 200,\n",
       " 240,\n",
       " 30,\n",
       " 60,\n",
       " 60,\n",
       " 10,\n",
       " 110,\n",
       " 270,\n",
       " 100,\n",
       " 120,\n",
       " 30,\n",
       " 120,\n",
       " 120,\n",
       " 130,\n",
       " 130,\n",
       " 140,\n",
       " 20,\n",
       " 170,\n",
       " 220,\n",
       " 230,\n",
       " 290,\n",
       " 60,\n",
       " 270,\n",
       " 270,\n",
       " 300,\n",
       " 80,\n",
       " 80,\n",
       " 100,\n",
       " 10,\n",
       " 190,\n",
       " 210,\n",
       " 280,\n",
       " 160,\n",
       " 230,\n",
       " 280,\n",
       " 160,\n",
       " 160,\n",
       " 180,\n",
       " 180,\n",
       " 190,\n",
       " 50,\n",
       " 70,\n",
       " 10,\n",
       " 100,\n",
       " 210,\n",
       " 230,\n",
       " 280,\n",
       " 90,\n",
       " 90,\n",
       " 130,\n",
       " 170,\n",
       " 250,\n",
       " 300,\n",
       " 80,\n",
       " 100,\n",
       " 110,\n",
       " 170,\n",
       " 200,\n",
       " 230,\n",
       " 250,\n",
       " 270,\n",
       " 110,\n",
       " 160,\n",
       " 240,\n",
       " 270,\n",
       " 290,\n",
       " 120,\n",
       " 180,\n",
       " 180,\n",
       " 210,\n",
       " 300,\n",
       " 40,\n",
       " 10,\n",
       " 120,\n",
       " 130,\n",
       " 180,\n",
       " 200,\n",
       " 220,\n",
       " 260,\n",
       " 280,\n",
       " 290,\n",
       " 60,\n",
       " 100,\n",
       " 160,\n",
       " 200,\n",
       " 80,\n",
       " 120,\n",
       " 130,\n",
       " 130,\n",
       " 20,\n",
       " 230,\n",
       " 240,\n",
       " 140,\n",
       " 190,\n",
       " 230,\n",
       " 290,\n",
       " 70,\n",
       " 120,\n",
       " 20,\n",
       " 200,\n",
       " 220,\n",
       " 240,\n",
       " 280,\n",
       " 120,\n",
       " 150,\n",
       " 160,\n",
       " 20,\n",
       " 190,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 20,\n",
       " 220,\n",
       " 300,\n",
       " 90,\n",
       " 20,\n",
       " 40,\n",
       " 150,\n",
       " 20,\n",
       " 20,\n",
       " 260,\n",
       " 30,\n",
       " 280,\n",
       " 10,\n",
       " 70,\n",
       " 80,\n",
       " 110,\n",
       " 120,\n",
       " 10,\n",
       " 180,\n",
       " 250,\n",
       " 260,\n",
       " 40,\n",
       " 70,\n",
       " 80,\n",
       " 100,\n",
       " 120,\n",
       " 150,\n",
       " 160,\n",
       " 160,\n",
       " 20,\n",
       " 20,\n",
       " 270,\n",
       " 290,\n",
       " 50,\n",
       " 10,\n",
       " 120,\n",
       " 160,\n",
       " 20,\n",
       " 250,\n",
       " 270,\n",
       " 60,\n",
       " 140,\n",
       " 230,\n",
       " 20,\n",
       " 170,\n",
       " 250,\n",
       " 260,\n",
       " 30,\n",
       " 40,\n",
       " 100,\n",
       " 20,\n",
       " 170,\n",
       " 170,\n",
       " 230,\n",
       " 250,\n",
       " 260,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 150,\n",
       " 20,\n",
       " 200,\n",
       " 240,\n",
       " 70,\n",
       " 70,\n",
       " 130,\n",
       " 170,\n",
       " 200,\n",
       " 280,\n",
       " 290,\n",
       " 50,\n",
       " 70,\n",
       " 100,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 180,\n",
       " 250,\n",
       " 80,\n",
       " 130,\n",
       " 190,\n",
       " 280,\n",
       " 40,\n",
       " 50,\n",
       " 70,\n",
       " 70,\n",
       " 80,\n",
       " 100,\n",
       " 20,\n",
       " 40,\n",
       " 110,\n",
       " 140,\n",
       " 170,\n",
       " 190,\n",
       " 80,\n",
       " 280,\n",
       " 40,\n",
       " 60,\n",
       " 100,\n",
       " 150,\n",
       " 170,\n",
       " 210,\n",
       " 250,\n",
       " 300,\n",
       " 50,\n",
       " 60,\n",
       " 160,\n",
       " 170,\n",
       " 190,\n",
       " 240,\n",
       " 70,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 120,\n",
       " 20,\n",
       " 220,\n",
       " 250,\n",
       " 260,\n",
       " 260,\n",
       " 280,\n",
       " 10,\n",
       " 80,\n",
       " 200,\n",
       " 210,\n",
       " 240,\n",
       " 240,\n",
       " 260,\n",
       " 290,\n",
       " 300,\n",
       " 40,\n",
       " 50,\n",
       " 100,\n",
       " 140,\n",
       " 140,\n",
       " 240,\n",
       " 30,\n",
       " 10,\n",
       " 80,\n",
       " 160,\n",
       " 160,\n",
       " 210,\n",
       " 220,\n",
       " 240,\n",
       " 240,\n",
       " 280,\n",
       " 290,\n",
       " 30,\n",
       " 70,\n",
       " 70,\n",
       " 150,\n",
       " 260,\n",
       " 30,\n",
       " 80,\n",
       " 100,\n",
       " 190,\n",
       " 200,\n",
       " 230,\n",
       " 230,\n",
       " 240,\n",
       " 40,\n",
       " 110,\n",
       " 150,\n",
       " 180,\n",
       " 230,\n",
       " 240,\n",
       " 40,\n",
       " 160,\n",
       " 300,\n",
       " 210,\n",
       " 220,\n",
       " 260,\n",
       " 260,\n",
       " 300,\n",
       " 50,\n",
       " 70,\n",
       " 90,\n",
       " 200,\n",
       " 230,\n",
       " 270,\n",
       " 270,\n",
       " 280,\n",
       " 40,\n",
       " 50,\n",
       " 80,\n",
       " 80,\n",
       " 100,\n",
       " 170,\n",
       " 210,\n",
       " 220,\n",
       " 280,\n",
       " 290,\n",
       " 30,\n",
       " 100,\n",
       " 160,\n",
       " 170,\n",
       " 190,\n",
       " 210,\n",
       " 230,\n",
       " 240,\n",
       " 260,\n",
       " 60,\n",
       " 80,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 160,\n",
       " 230,\n",
       " 270,\n",
       " 130,\n",
       " 140,\n",
       " 160,\n",
       " 240,\n",
       " 250,\n",
       " 30,\n",
       " 50,\n",
       " 50,\n",
       " 10,\n",
       " 70,\n",
       " 200,\n",
       " 200,\n",
       " 40,\n",
       " 130,\n",
       " 150,\n",
       " 150,\n",
       " 20,\n",
       " 250,\n",
       " 90,\n",
       " 150,\n",
       " 250,\n",
       " 280,\n",
       " 70,\n",
       " 120,\n",
       " 10,\n",
       " 140,\n",
       " 270,\n",
       " 280,\n",
       " 290,\n",
       " 50,\n",
       " 90,\n",
       " 100,\n",
       " 100,\n",
       " 160,\n",
       " 180,\n",
       " 190,\n",
       " 220,\n",
       " 240,\n",
       " 300,\n",
       " 30,\n",
       " 80,\n",
       " 100,\n",
       " 100,\n",
       " 120,\n",
       " 170,\n",
       " 190,\n",
       " 210,\n",
       " 220,\n",
       " 230,\n",
       " 30,\n",
       " 270,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 80,\n",
       " 90,\n",
       " 20,\n",
       " 260,\n",
       " 290,\n",
       " 290,\n",
       " 70,\n",
       " 70,\n",
       " 80,\n",
       " 210,\n",
       " 260,\n",
       " 80,\n",
       " 150,\n",
       " 180,\n",
       " 200,\n",
       " 250,\n",
       " 260,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 140,\n",
       " 200,\n",
       " 200,\n",
       " 230,\n",
       " 240,\n",
       " 250,\n",
       " 200,\n",
       " 220,\n",
       " 290,\n",
       " 10,\n",
       " 100,\n",
       " 10,\n",
       " 110,\n",
       " 170,\n",
       " 20,\n",
       " 40,\n",
       " 70,\n",
       " 80,\n",
       " 80,\n",
       " 90,\n",
       " 120,\n",
       " 20,\n",
       " 220,\n",
       " 220,\n",
       " 240,\n",
       " 260,\n",
       " 290,\n",
       " 100,\n",
       " 120,\n",
       " 130,\n",
       " 170,\n",
       " 300,\n",
       " 140,\n",
       " 150,\n",
       " 190,\n",
       " 190,\n",
       " 200,\n",
       " 20,\n",
       " 240,\n",
       " 250,\n",
       " 60,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 120,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 190,\n",
       " 300,\n",
       " 30,\n",
       " 30,\n",
       " 10,\n",
       " 160,\n",
       " 290,\n",
       " 300,\n",
       " 40,\n",
       " 50,\n",
       " 110,\n",
       " 140,\n",
       " 150,\n",
       " 170,\n",
       " 210,\n",
       " 220,\n",
       " 220,\n",
       " 230,\n",
       " 230,\n",
       " 250,\n",
       " 270,\n",
       " 300,\n",
       " 30,\n",
       " 10,\n",
       " 80,\n",
       " 90,\n",
       " 110,\n",
       " 200,\n",
       " 200,\n",
       " 50,\n",
       " 10,\n",
       " 120,\n",
       " 10,\n",
       " 160,\n",
       " 200,\n",
       " 50,\n",
       " 120,\n",
       " 150,\n",
       " 250,\n",
       " 250,\n",
       " 250,\n",
       " 290,\n",
       " 50,\n",
       " 90,\n",
       " 110,\n",
       " 130,\n",
       " 160,\n",
       " 10,\n",
       " 160,\n",
       " 180,\n",
       " 200,\n",
       " 30,\n",
       " 40,\n",
       " 10,\n",
       " 150,\n",
       " 250,\n",
       " 120,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 10,\n",
       " 120,\n",
       " 120,\n",
       " 140,\n",
       " 230,\n",
       " 250,\n",
       " 280,\n",
       " 70,\n",
       " 80,\n",
       " 120,\n",
       " 170,\n",
       " 210,\n",
       " 20,\n",
       " 260,\n",
       " 270,\n",
       " 40,\n",
       " 130,\n",
       " 170,\n",
       " 270,\n",
       " 80,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 230,\n",
       " 250,\n",
       " 290,\n",
       " 290,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 90,\n",
       " 100,\n",
       " 120,\n",
       " 130,\n",
       " 160,\n",
       " 160,\n",
       " 40,\n",
       " 60,\n",
       " 150,\n",
       " 170,\n",
       " 190,\n",
       " 220,\n",
       " 20,\n",
       " 40,\n",
       " 80,\n",
       " 120,\n",
       " 170,\n",
       " 220,\n",
       " 20,\n",
       " 290,\n",
       " 70,\n",
       " 70,\n",
       " 110,\n",
       " 160,\n",
       " 170,\n",
       " 240,\n",
       " 270,\n",
       " 290,\n",
       " 290,\n",
       " 40,\n",
       " 70,\n",
       " 10,\n",
       " 80,\n",
       " 160,\n",
       " 180,\n",
       " 250,\n",
       " 260,\n",
       " 260,\n",
       " 280,\n",
       " 200,\n",
       " 230,\n",
       " 230,\n",
       " 250,\n",
       " 260,\n",
       " 280,\n",
       " 280,\n",
       " 50,\n",
       " 110,\n",
       " 10,\n",
       " 210,\n",
       " 210,\n",
       " 40,\n",
       " 40,\n",
       " 80,\n",
       " 150,\n",
       " 180,\n",
       " 180,\n",
       " 190,\n",
       " 240,\n",
       " 250,\n",
       " 290,\n",
       " 60,\n",
       " 100,\n",
       " 110,\n",
       " 150,\n",
       " 270,\n",
       " 50,\n",
       " 60,\n",
       " 80,\n",
       " 170,\n",
       " 40,\n",
       " 50,\n",
       " 250,\n",
       " 70,\n",
       " 20,\n",
       " 230,\n",
       " 250,\n",
       " 30,\n",
       " 50,\n",
       " 100,\n",
       " 210,\n",
       " 240,\n",
       " 260,\n",
       " 270,\n",
       " 270,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 150,\n",
       " 240,\n",
       " 270,\n",
       " 60,\n",
       " 80,\n",
       " 110,\n",
       " 140,\n",
       " 200,\n",
       " 270,\n",
       " 30,\n",
       " 60,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 20,\n",
       " 260,\n",
       " 260,\n",
       " 50,\n",
       " 120,\n",
       " 140,\n",
       " 170,\n",
       " 210,\n",
       " 210,\n",
       " 70,\n",
       " 90,\n",
       " 110,\n",
       " 130,\n",
       " 180,\n",
       " 230,\n",
       " 260,\n",
       " 260,\n",
       " 50,\n",
       " 80,\n",
       " 90,\n",
       " 200,\n",
       " 200,\n",
       " 220,\n",
       " 280,\n",
       " 290,\n",
       " 70,\n",
       " 100,\n",
       " 10,\n",
       " 190,\n",
       " 220,\n",
       " 230,\n",
       " 270,\n",
       " 80,\n",
       " 180,\n",
       " 180,\n",
       " 190,\n",
       " 210,\n",
       " 210,\n",
       " 20,\n",
       " 240,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 100,\n",
       " 100,\n",
       " 130,\n",
       " 300,\n",
       " 140,\n",
       " 200,\n",
       " 200,\n",
       " 240,\n",
       " 80,\n",
       " 100,\n",
       " 120,\n",
       " 130,\n",
       " 190,\n",
       " 220,\n",
       " 60,\n",
       " 110,\n",
       " 120,\n",
       " 200,\n",
       " 220,\n",
       " 230,\n",
       " 280,\n",
       " 300,\n",
       " 40,\n",
       " 40,\n",
       " 50,\n",
       " 50,\n",
       " 10,\n",
       " 130,\n",
       " 150,\n",
       " 180,\n",
       " 120,\n",
       " 130,\n",
       " 10,\n",
       " 80,\n",
       " 80,\n",
       " 160,\n",
       " 220,\n",
       " 220,\n",
       " 290,\n",
       " 60,\n",
       " 80,\n",
       " 90,\n",
       " 90,\n",
       " 120,\n",
       " 200,\n",
       " 230,\n",
       " 250,\n",
       " 260,\n",
       " 300,\n",
       " 70,\n",
       " 140,\n",
       " 170,\n",
       " 240,\n",
       " 250,\n",
       " 80,\n",
       " 80,\n",
       " 160,\n",
       " 180,\n",
       " 20,\n",
       " 50,\n",
       " 10,\n",
       " 120,\n",
       " 120,\n",
       " 140,\n",
       " 270,\n",
       " 270,\n",
       " 280,\n",
       " 160,\n",
       " 220,\n",
       " 140,\n",
       " 170,\n",
       " 20,\n",
       " 230,\n",
       " 160,\n",
       " 210,\n",
       " 220,\n",
       " 220,\n",
       " 240,\n",
       " 250,\n",
       " 10,\n",
       " 70,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 210,\n",
       " 230,\n",
       " 280,\n",
       " 30,\n",
       " 90,\n",
       " 10,\n",
       " 100,\n",
       " 140,\n",
       " 160,\n",
       " 190,\n",
       " 260,\n",
       " 30,\n",
       " 300,\n",
       " 300,\n",
       " 100,\n",
       " 150,\n",
       " 180,\n",
       " 200,\n",
       " 30,\n",
       " 300,\n",
       " 40,\n",
       " 130,\n",
       " 150,\n",
       " 160,\n",
       " 200,\n",
       " 210,\n",
       " 40,\n",
       " 60,\n",
       " 90,\n",
       " 10,\n",
       " 170,\n",
       " 190,\n",
       " 250,\n",
       " 260,\n",
       " 280,\n",
       " 280,\n",
       " 30,\n",
       " 40,\n",
       " 60,\n",
       " 120,\n",
       " 200,\n",
       " 210,\n",
       " 210,\n",
       " 220,\n",
       " 250,\n",
       " 260,\n",
       " 260,\n",
       " 270,\n",
       " 280,\n",
       " 10,\n",
       " 120,\n",
       " 270,\n",
       " 270,\n",
       " 60,\n",
       " 110,\n",
       " 20,\n",
       " 30,\n",
       " 50,\n",
       " 90,\n",
       " 120,\n",
       " 10,\n",
       " 160,\n",
       " 220,\n",
       " 220,\n",
       " 250,\n",
       " 260,\n",
       " 10,\n",
       " 90,\n",
       " 10,\n",
       " 140,\n",
       " 180,\n",
       " 210,\n",
       " 40,\n",
       " 50,\n",
       " 50,\n",
       " 80,\n",
       " 100,\n",
       " 100,\n",
       " 110,\n",
       " 280,\n",
       " 60,\n",
       " 90,\n",
       " 180,\n",
       " 180,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 50,\n",
       " 10,\n",
       " 20,\n",
       " 200,\n",
       " 200,\n",
       " 220,\n",
       " 220,\n",
       " 250,\n",
       " 30,\n",
       " 50,\n",
       " 10,\n",
       " 140,\n",
       " 180,\n",
       " 200,\n",
       " 20,\n",
       " 200,\n",
       " 250,\n",
       " 10,\n",
       " 280,\n",
       " 30,\n",
       " 60,\n",
       " 10,\n",
       " 190,\n",
       " 200,\n",
       " 90,\n",
       " 110,\n",
       " 130,\n",
       " 170,\n",
       " 280,\n",
       " 70,\n",
       " 90,\n",
       " 110,\n",
       " 120,\n",
       " 260,\n",
       " 270,\n",
       " 280,\n",
       " 10,\n",
       " 140,\n",
       " 200,\n",
       " 250,\n",
       " 260,\n",
       " 40,\n",
       " 10,\n",
       " 10,\n",
       " 70,\n",
       " 20,\n",
       " 260,\n",
       " 40,\n",
       " 110,\n",
       " 150,\n",
       " 160,\n",
       " 210,\n",
       " 20,\n",
       " 110,\n",
       " 130,\n",
       " 170,\n",
       " 210,\n",
       " 240,\n",
       " 280,\n",
       " 10,\n",
       " 150,\n",
       " 240,\n",
       " 240,\n",
       " 30,\n",
       " 90,\n",
       " 100,\n",
       " 140,\n",
       " 190,\n",
       " 260,\n",
       " 300,\n",
       " 90,\n",
       " 10,\n",
       " 180,\n",
       " 220,\n",
       " 230,\n",
       " 260,\n",
       " 40,\n",
       " 80,\n",
       " 90,\n",
       " 90,\n",
       " 140,\n",
       " 170,\n",
       " 200,\n",
       " 270,\n",
       " 120,\n",
       " 120,\n",
       " 300,\n",
       " 40,\n",
       " 80,\n",
       " 170,\n",
       " 30,\n",
       " 300,\n",
       " 50,\n",
       " 80,\n",
       " 110,\n",
       " 250,\n",
       " 270,\n",
       " 280,\n",
       " 50,\n",
       " 80,\n",
       " 120,\n",
       " 170,\n",
       " 270,\n",
       " 280,\n",
       " 100,\n",
       " 120,\n",
       " 140,\n",
       " 180,\n",
       " 190,\n",
       " 220,\n",
       " 220,\n",
       " 240,\n",
       " 280,\n",
       " 100,\n",
       " 20,\n",
       " 290,\n",
       " 300,\n",
       " 40,\n",
       " 130,\n",
       " 130,\n",
       " 300,\n",
       " 70,\n",
       " 130,\n",
       " 140,\n",
       " 160,\n",
       " 240,\n",
       " 290,\n",
       " 40,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 140,\n",
       " 230,\n",
       " 240,\n",
       " 70,\n",
       " 80,\n",
       " 110,\n",
       " 120,\n",
       " 180,\n",
       " 20,\n",
       " 290,\n",
       " 60,\n",
       " 110,\n",
       " ...]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 150]\n",
    "\n",
    "d_binned = [round(dd/10)*10 for dd in d]\n",
    "\n",
    "d_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3c8cc2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2398, 15000)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_obs = []\n",
    "min_time = 18000\n",
    "for k in range(len(x)):\n",
    "    if mode != 'mono':\n",
    "        obs, sr = librosa.load(folder+x[k], mono=False)#, mono=False\n",
    "        x_obs.append(obs[:, 1000:16000])# :,\n",
    "    else:\n",
    "        obs, sr = librosa.load(folder+x[k])#, mono=False\n",
    "        x_obs.append(obs[1000:16000])# :,\n",
    "\n",
    "x_obs = np.array(x_obs)\n",
    "x_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "52428a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,  20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240,\n",
       "        260, 280, 300, 320, 340]),\n",
       " array([129, 132, 143, 114, 146, 101, 136, 128, 145, 128, 142, 124, 142,\n",
       "        139, 167, 121, 137, 124], dtype=int64))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_vals, z_counts = np.unique(y_binned, return_counts=True)                   # There is a serious imbalance in the class samplesnp.unique(y[:, 0], return_counts=True)                   # There is a serious imbalance in the class samples\n",
    "np.unique(y_binned, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0d497a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'mono': x = np.reshape(x_obs, [-1, 15000, 2]) #2\n",
    "else: x = np.reshape(x_obs, [-1, 15000, 1]) #2\n",
    "y = np.array(y_binned)\n",
    "d = np.array(d_binned)\n",
    "z = np.copy(y)\n",
    "y = np.reshape(y, [-1, 1])\n",
    "d = np.reshape(d, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d4adb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.copy(x)\n",
    "y0 = np.copy(y)\n",
    "d0 = np.copy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3fa0345c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_vals, z_counts = np.unique(y_binned, return_counts=True) \n",
    "max_ct = max(z_counts)\n",
    "x0 = np.copy(x)\n",
    "y0 = np.copy(y)\n",
    "d0 = np.copy(d)\n",
    "for val, ct in zip(z_vals, z_counts):\n",
    "    \n",
    "    mult = max_ct // ct - 1\n",
    "    mult += (max_ct - ct*(1+mult))/ct\n",
    "    rem = mult % 1\n",
    "    mult = int(mult // 1)\n",
    "\n",
    "    new_entries = np.tile(x[np.where(z==val)],(mult,1))\n",
    "    new_targets = np.tile(y[np.where(z==val)],(mult,1))\n",
    "    new_ds      = np.tile(d[np.where(z==val)],(mult,1))\n",
    " \n",
    "\n",
    "    if mult > 0:\n",
    "        x0 = np.append(x0, new_entries, axis = 0)\n",
    "        y0 = np.append(y0, new_targets, axis = 0)\n",
    "        d0 = np.append(d0, new_ds, axis = 0)\n",
    "    \n",
    "\n",
    "    new_entries = x[np.where(z==val)][0:int(rem*ct)]\n",
    "    new_targets = y[np.where(z==val)][0:int(rem*ct)]\n",
    "    new_ds      = d[np.where(z==val)][0:int(rem*ct)]\n",
    "    \n",
    "    x0 = np.append(x0, new_entries, axis = 0)\n",
    "    y0 = np.append(y0, new_targets, axis = 0)\n",
    "    d0 = np.append(d0, new_ds, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a48667a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,  20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240,\n",
       "        260, 280, 300, 320, 340]),\n",
       " array([129, 132, 143, 114, 146, 101, 136, 128, 145, 128, 142, 124, 142,\n",
       "        139, 167, 121, 137, 124], dtype=int64))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y0, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7fe2e589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,  20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240,\n",
       "        260, 280, 300, 320, 340]),\n",
       " array([129, 132, 143, 114, 146, 101, 136, 128, 145, 128, 142, 124, 142,\n",
       "        139, 167, 121, 137, 124], dtype=int64))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_vals, z_counts = np.unique(d0, return_counts=True)                   # There is a serious imbalance in the class samplesnp.unique(y[:, 0], return_counts=True)                   # There is a serious imbalance in the class samples\n",
    "np.unique(y_binned, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eaff73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'mono': x = np.reshape(x0, [-1, 15000, 2]) #2\n",
    "else: x = np.reshape(x0, [-1, 15000, 1]) #2\n",
    "y = y0\n",
    "d = d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "19d37964",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "925e1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_train, dx_test, d_train, d_test = train_test_split(x, d, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d6f4254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encoder(y_train, y_test):\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "    \n",
    "    vals = np.unique(y_train.astype(int))\n",
    "    keys = np.array(range(len(vals)))\n",
    "    valdict = {val:key for key, val in zip(keys, vals)}\n",
    "    ic(valdict)\n",
    "    y_test = np.array([valdict[v[0]] for v in y_test.astype(int)])\n",
    "    y_train = np.array([valdict[v[0]] for v in y_train.astype(int)])\n",
    "    \n",
    "    n_classes = len(np.unique(y_train))\n",
    "    y_train = np.array(y_train.astype(int))\n",
    "    y_train = np.eye(n_classes)[y_train]                  # One-hot encoding of target variable\n",
    "    y_test = np.array(y_test.astype(int))\n",
    "    y_test = np.eye(n_classes)[y_test]\n",
    "    return y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "58ef8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| valdict: {0: 0,\n",
      "              20: 1,\n",
      "              40: 2,\n",
      "              60: 3,\n",
      "              80: 4,\n",
      "              100: 5,\n",
      "              120: 6,\n",
      "              140: 7,\n",
      "              160: 8,\n",
      "              180: 9,\n",
      "              200: 10,\n",
      "              220: 11,\n",
      "              240: 12,\n",
      "              260: 13,\n",
      "              280: 14,\n",
      "              300: 15,\n",
      "              320: 16,\n",
      "              340: 17}\n",
      "ic| valdict: {10: 0,\n",
      "              20: 1,\n",
      "              30: 2,\n",
      "              40: 3,\n",
      "              50: 4,\n",
      "              60: 5,\n",
      "              70: 6,\n",
      "              80: 7,\n",
      "              90: 8,\n",
      "              100: 9,\n",
      "              110: 10,\n",
      "              120: 11,\n",
      "              130: 12,\n",
      "              140: 13,\n",
      "              150: 14,\n",
      "              160: 15,\n",
      "              170: 16,\n",
      "              180: 17,\n",
      "              190: 18,\n",
      "              200: 19,\n",
      "              210: 20,\n",
      "              220: 21,\n",
      "              230: 22,\n",
      "              240: 23,\n",
      "              250: 24,\n",
      "              260: 25,\n",
      "              270: 26,\n",
      "              280: 27,\n",
      "              290: 28,\n",
      "              300: 29}\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = hot_encoder(y_train, y_test)\n",
    "d_train, d_test = hot_encoder(d_train, d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5a2a5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.random.set_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "50f998d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_train.shape: (2158, 15000, 1), y_train.shape: (2158, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2158, 15000, 1), (2158, 18))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "58f2f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_test.shape: (240, 15000, 1), y_test.shape: (240, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((240, 15000, 1), (240, 18))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f7136cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_train.shape: (2158, 15000, 1), d_train.shape: (2158, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2158, 15000, 1), (2158, 30))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(x_train.shape, d_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8807e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_test.shape: (240, 15000, 1), d_test.shape: (240, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((240, 15000, 1), (240, 30))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(x_test.shape, d_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "25eecaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'mono': feat = 2\n",
    "else: feat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d2777ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelizer(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    verbose, epochs, batch_size = 1, 800, 32\n",
    "    n_timesteps, n_outputs = 15000, y_test.shape[1]\n",
    "    n_features = feat\n",
    "    kr =0\n",
    "    activation = 'relu'\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Network Backbone\n",
    "    model.add(Conv1D(filters=32, kernel_size=128, strides=8, activation=activation, kernel_initializer=initializer, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=16, strides=4, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(Conv1D(filters=64, kernel_size=8, strides=2, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(Conv1D(filters=128, kernel_size=4, strides=2, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=256, kernel_size=8, strides=4, activation=activation, kernel_initializer=initializer))\n",
    "\n",
    "    # Network Head\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=activation,kernel_initializer=initializer))\n",
    "    model.add(Dropout(0.33))\n",
    "    model.add(Dense(64, activation=activation,kernel_initializer=initializer))\n",
    "    model.add(Dropout(0.33))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    #model.add(Conv1D(filters=128, kernel_size=1, activation=activation))\n",
    "    #model.add(Conv1D(filters=64, kernel_size=1, activation=activation))\n",
    "    #model.add(Conv1D(filters=n_outputs, kernel_size=1))\n",
    "    #model.add(layers.GlobalMaxPooling1D())\n",
    "    #model.add(layers.Softmax())\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    checkpoint_filepath = 'weights.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "\n",
    "    initial_learning_rate = 0.01\n",
    "\n",
    "    def lr_step_decay(epoch, lr):\n",
    "        drop_rate = 0.5\n",
    "        epochs_drop = 10.0\n",
    "        return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "\n",
    "    callbacks = [model_checkpoint_callback] #callbacks = callbacks,\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,  validation_data = (x_test, y_test), verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d3dd7db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1860, 32)          4128      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 462, 32)           16416     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 228, 64)           16448     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 226, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 112, 128)          32896     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 110, 256)          98560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 26, 256)           524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6656)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               852096    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 1,566,866\n",
      "Trainable params: 1,566,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/800\n",
      "68/68 [==============================] - 2s 21ms/step - loss: 2.8941 - accuracy: 0.0561 - val_loss: 2.8761 - val_accuracy: 0.0917\n",
      "Epoch 2/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8823 - accuracy: 0.0649 - val_loss: 2.8680 - val_accuracy: 0.1000\n",
      "Epoch 3/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8799 - accuracy: 0.0728 - val_loss: 2.8592 - val_accuracy: 0.1250\n",
      "Epoch 4/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8751 - accuracy: 0.0755 - val_loss: 2.8498 - val_accuracy: 0.1250\n",
      "Epoch 5/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8702 - accuracy: 0.0802 - val_loss: 2.8434 - val_accuracy: 0.1042\n",
      "Epoch 6/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8640 - accuracy: 0.0811 - val_loss: 2.8415 - val_accuracy: 0.1167\n",
      "Epoch 7/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8616 - accuracy: 0.0941 - val_loss: 2.8311 - val_accuracy: 0.1208\n",
      "Epoch 8/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.8458 - accuracy: 0.0941 - val_loss: 2.8161 - val_accuracy: 0.1250\n",
      "Epoch 9/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.8378 - accuracy: 0.1015 - val_loss: 2.8145 - val_accuracy: 0.1208\n",
      "Epoch 10/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.8304 - accuracy: 0.1033 - val_loss: 2.7868 - val_accuracy: 0.1333\n",
      "Epoch 11/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.8048 - accuracy: 0.1200 - val_loss: 2.7703 - val_accuracy: 0.1458\n",
      "Epoch 12/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.7960 - accuracy: 0.1251 - val_loss: 2.7454 - val_accuracy: 0.1417\n",
      "Epoch 13/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.7695 - accuracy: 0.1311 - val_loss: 2.7375 - val_accuracy: 0.1625\n",
      "Epoch 14/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.7379 - accuracy: 0.1367 - val_loss: 2.6841 - val_accuracy: 0.1583\n",
      "Epoch 15/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.7036 - accuracy: 0.1548 - val_loss: 2.6416 - val_accuracy: 0.1667\n",
      "Epoch 16/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.6516 - accuracy: 0.1562 - val_loss: 2.5952 - val_accuracy: 0.1625\n",
      "Epoch 17/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.6374 - accuracy: 0.1501 - val_loss: 2.5604 - val_accuracy: 0.1833\n",
      "Epoch 18/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.5732 - accuracy: 0.1784 - val_loss: 2.4917 - val_accuracy: 0.2000\n",
      "Epoch 19/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.5204 - accuracy: 0.1849 - val_loss: 2.4181 - val_accuracy: 0.2333\n",
      "Epoch 20/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.4684 - accuracy: 0.1965 - val_loss: 2.3468 - val_accuracy: 0.2250\n",
      "Epoch 21/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.3962 - accuracy: 0.2187 - val_loss: 2.4844 - val_accuracy: 0.1583\n",
      "Epoch 22/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.3208 - accuracy: 0.2354 - val_loss: 2.2891 - val_accuracy: 0.2250\n",
      "Epoch 23/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.2305 - accuracy: 0.2386 - val_loss: 2.5929 - val_accuracy: 0.1583\n",
      "Epoch 24/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.1864 - accuracy: 0.2456 - val_loss: 2.0439 - val_accuracy: 0.3167\n",
      "Epoch 25/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.0844 - accuracy: 0.2776 - val_loss: 2.0565 - val_accuracy: 0.2625\n",
      "Epoch 26/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.9977 - accuracy: 0.2831 - val_loss: 1.8669 - val_accuracy: 0.3167\n",
      "Epoch 27/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.9474 - accuracy: 0.2938 - val_loss: 1.9154 - val_accuracy: 0.2833\n",
      "Epoch 28/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.8723 - accuracy: 0.3040 - val_loss: 1.9667 - val_accuracy: 0.2833\n",
      "Epoch 29/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.8253 - accuracy: 0.3184 - val_loss: 1.8335 - val_accuracy: 0.2875\n",
      "Epoch 30/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.7777 - accuracy: 0.3230 - val_loss: 1.6867 - val_accuracy: 0.3792\n",
      "Epoch 31/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.6828 - accuracy: 0.3656 - val_loss: 1.6658 - val_accuracy: 0.3875\n",
      "Epoch 32/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.6853 - accuracy: 0.3424 - val_loss: 1.6513 - val_accuracy: 0.3292\n",
      "Epoch 33/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.6112 - accuracy: 0.3568 - val_loss: 1.6374 - val_accuracy: 0.3708\n",
      "Epoch 34/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5459 - accuracy: 0.3786 - val_loss: 1.5443 - val_accuracy: 0.3750\n",
      "Epoch 35/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5261 - accuracy: 0.3786 - val_loss: 1.5422 - val_accuracy: 0.4000\n",
      "Epoch 36/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5130 - accuracy: 0.3865 - val_loss: 1.5067 - val_accuracy: 0.4375\n",
      "Epoch 37/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4333 - accuracy: 0.4180 - val_loss: 1.5123 - val_accuracy: 0.4250\n",
      "Epoch 38/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4194 - accuracy: 0.4217 - val_loss: 1.5460 - val_accuracy: 0.4333\n",
      "Epoch 39/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3657 - accuracy: 0.4296 - val_loss: 1.6184 - val_accuracy: 0.3500\n",
      "Epoch 40/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3378 - accuracy: 0.4486 - val_loss: 1.4104 - val_accuracy: 0.4667\n",
      "Epoch 41/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3171 - accuracy: 0.4347 - val_loss: 1.3922 - val_accuracy: 0.4375\n",
      "Epoch 42/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2475 - accuracy: 0.4708 - val_loss: 1.3545 - val_accuracy: 0.4750\n",
      "Epoch 43/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2354 - accuracy: 0.4634 - val_loss: 1.3976 - val_accuracy: 0.4833\n",
      "Epoch 44/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2161 - accuracy: 0.4722 - val_loss: 1.4795 - val_accuracy: 0.4083\n",
      "Epoch 45/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1905 - accuracy: 0.4917 - val_loss: 1.3064 - val_accuracy: 0.4667\n",
      "Epoch 46/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1444 - accuracy: 0.5042 - val_loss: 1.4159 - val_accuracy: 0.4750\n",
      "Epoch 47/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1604 - accuracy: 0.4889 - val_loss: 1.3074 - val_accuracy: 0.5542\n",
      "Epoch 48/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0969 - accuracy: 0.5259 - val_loss: 1.3471 - val_accuracy: 0.4375\n",
      "Epoch 49/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0637 - accuracy: 0.5204 - val_loss: 1.2913 - val_accuracy: 0.5500\n",
      "Epoch 50/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0588 - accuracy: 0.5301 - val_loss: 1.2881 - val_accuracy: 0.5750\n",
      "Epoch 51/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.0412 - accuracy: 0.5468 - val_loss: 1.3476 - val_accuracy: 0.5167\n",
      "Epoch 52/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0111 - accuracy: 0.5468 - val_loss: 1.3495 - val_accuracy: 0.4625\n",
      "Epoch 53/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9942 - accuracy: 0.5454 - val_loss: 1.3533 - val_accuracy: 0.4833\n",
      "Epoch 54/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9576 - accuracy: 0.5802 - val_loss: 1.2895 - val_accuracy: 0.5750\n",
      "Epoch 55/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9786 - accuracy: 0.5714 - val_loss: 1.3702 - val_accuracy: 0.5583\n",
      "Epoch 56/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9445 - accuracy: 0.5621 - val_loss: 1.2867 - val_accuracy: 0.6208\n",
      "Epoch 57/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9373 - accuracy: 0.5918 - val_loss: 1.2549 - val_accuracy: 0.5875\n",
      "Epoch 58/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9127 - accuracy: 0.5890 - val_loss: 1.2204 - val_accuracy: 0.5250\n",
      "Epoch 59/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9031 - accuracy: 0.5741 - val_loss: 1.2905 - val_accuracy: 0.5833\n",
      "Epoch 60/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.8622 - accuracy: 0.6135 - val_loss: 1.3672 - val_accuracy: 0.4792\n",
      "Epoch 61/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8684 - accuracy: 0.6117 - val_loss: 1.2881 - val_accuracy: 0.5500\n",
      "Epoch 62/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.8568 - accuracy: 0.6209 - val_loss: 1.2417 - val_accuracy: 0.6125\n",
      "Epoch 63/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.8458 - accuracy: 0.6260 - val_loss: 1.2189 - val_accuracy: 0.5667\n",
      "Epoch 64/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.8485 - accuracy: 0.6247 - val_loss: 1.3977 - val_accuracy: 0.5417\n",
      "Epoch 65/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.8043 - accuracy: 0.6339 - val_loss: 1.3321 - val_accuracy: 0.5250\n",
      "Epoch 66/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.8068 - accuracy: 0.6348 - val_loss: 1.2269 - val_accuracy: 0.6583\n",
      "Epoch 67/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7767 - accuracy: 0.6548 - val_loss: 1.2806 - val_accuracy: 0.6083\n",
      "Epoch 68/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7773 - accuracy: 0.6677 - val_loss: 1.2878 - val_accuracy: 0.6292\n",
      "Epoch 69/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7446 - accuracy: 0.6738 - val_loss: 1.4058 - val_accuracy: 0.5833\n",
      "Epoch 70/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7255 - accuracy: 0.6724 - val_loss: 1.3271 - val_accuracy: 0.5917\n",
      "Epoch 71/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6992 - accuracy: 0.6988 - val_loss: 1.3598 - val_accuracy: 0.5917\n",
      "Epoch 72/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6973 - accuracy: 0.6937 - val_loss: 1.2713 - val_accuracy: 0.6250\n",
      "Epoch 73/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7041 - accuracy: 0.7030 - val_loss: 1.4653 - val_accuracy: 0.5000\n",
      "Epoch 74/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6917 - accuracy: 0.6997 - val_loss: 1.5442 - val_accuracy: 0.5750\n",
      "Epoch 75/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6710 - accuracy: 0.7141 - val_loss: 1.2751 - val_accuracy: 0.6667\n",
      "Epoch 76/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6521 - accuracy: 0.7331 - val_loss: 1.3446 - val_accuracy: 0.5875\n",
      "Epoch 77/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6587 - accuracy: 0.7030 - val_loss: 1.4352 - val_accuracy: 0.6042\n",
      "Epoch 78/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.6407 - accuracy: 0.7285 - val_loss: 1.3080 - val_accuracy: 0.6125\n",
      "Epoch 79/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6281 - accuracy: 0.7428 - val_loss: 1.3384 - val_accuracy: 0.6458\n",
      "Epoch 80/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6011 - accuracy: 0.7298 - val_loss: 1.2640 - val_accuracy: 0.6292\n",
      "Epoch 81/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6157 - accuracy: 0.7317 - val_loss: 1.4873 - val_accuracy: 0.5750\n",
      "Epoch 82/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5899 - accuracy: 0.7576 - val_loss: 1.4552 - val_accuracy: 0.5958\n",
      "Epoch 83/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5882 - accuracy: 0.7424 - val_loss: 1.3516 - val_accuracy: 0.6417\n",
      "Epoch 84/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5944 - accuracy: 0.7516 - val_loss: 1.6010 - val_accuracy: 0.5917\n",
      "Epoch 85/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5932 - accuracy: 0.7428 - val_loss: 1.4102 - val_accuracy: 0.6542\n",
      "Epoch 86/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5711 - accuracy: 0.7539 - val_loss: 1.4056 - val_accuracy: 0.6417\n",
      "Epoch 87/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5795 - accuracy: 0.7586 - val_loss: 1.2988 - val_accuracy: 0.6500\n",
      "Epoch 88/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5409 - accuracy: 0.7762 - val_loss: 1.3450 - val_accuracy: 0.6792\n",
      "Epoch 89/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5566 - accuracy: 0.7600 - val_loss: 1.3950 - val_accuracy: 0.6500\n",
      "Epoch 90/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5445 - accuracy: 0.7780 - val_loss: 1.4999 - val_accuracy: 0.6292\n",
      "Epoch 91/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5165 - accuracy: 0.7966 - val_loss: 1.3996 - val_accuracy: 0.6833\n",
      "Epoch 92/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5060 - accuracy: 0.7905 - val_loss: 1.3180 - val_accuracy: 0.6958\n",
      "Epoch 93/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5166 - accuracy: 0.7850 - val_loss: 1.5215 - val_accuracy: 0.6292\n",
      "Epoch 94/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.5153 - accuracy: 0.7966 - val_loss: 1.3792 - val_accuracy: 0.6375\n",
      "Epoch 95/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.5145 - accuracy: 0.7933 - val_loss: 1.3423 - val_accuracy: 0.6917\n",
      "Epoch 96/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4925 - accuracy: 0.8040 - val_loss: 1.3125 - val_accuracy: 0.6667\n",
      "Epoch 97/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4677 - accuracy: 0.8095 - val_loss: 1.3890 - val_accuracy: 0.6333\n",
      "Epoch 98/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4706 - accuracy: 0.8026 - val_loss: 1.4515 - val_accuracy: 0.6500\n",
      "Epoch 99/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4610 - accuracy: 0.8095 - val_loss: 1.5339 - val_accuracy: 0.6333\n",
      "Epoch 100/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4631 - accuracy: 0.8123 - val_loss: 1.4690 - val_accuracy: 0.6750\n",
      "Epoch 101/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4647 - accuracy: 0.8100 - val_loss: 1.6542 - val_accuracy: 0.6167\n",
      "Epoch 102/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4512 - accuracy: 0.8146 - val_loss: 1.4279 - val_accuracy: 0.6792\n",
      "Epoch 103/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4717 - accuracy: 0.8123 - val_loss: 1.3171 - val_accuracy: 0.7000\n",
      "Epoch 104/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4330 - accuracy: 0.8234 - val_loss: 1.3921 - val_accuracy: 0.7042\n",
      "Epoch 105/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4265 - accuracy: 0.8276 - val_loss: 1.5109 - val_accuracy: 0.6708\n",
      "Epoch 106/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4100 - accuracy: 0.8299 - val_loss: 1.5734 - val_accuracy: 0.6458\n",
      "Epoch 107/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4024 - accuracy: 0.8299 - val_loss: 1.4292 - val_accuracy: 0.7000\n",
      "Epoch 108/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4199 - accuracy: 0.8290 - val_loss: 1.3963 - val_accuracy: 0.6958\n",
      "Epoch 109/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4027 - accuracy: 0.8350 - val_loss: 1.4865 - val_accuracy: 0.6667\n",
      "Epoch 110/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3898 - accuracy: 0.8448 - val_loss: 1.6422 - val_accuracy: 0.6333\n",
      "Epoch 111/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3965 - accuracy: 0.8406 - val_loss: 1.6436 - val_accuracy: 0.7000\n",
      "Epoch 112/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4215 - accuracy: 0.8309 - val_loss: 1.5104 - val_accuracy: 0.6625\n",
      "Epoch 113/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3961 - accuracy: 0.8424 - val_loss: 1.4659 - val_accuracy: 0.7042\n",
      "Epoch 114/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3696 - accuracy: 0.8628 - val_loss: 1.4401 - val_accuracy: 0.7250\n",
      "Epoch 115/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3820 - accuracy: 0.8471 - val_loss: 1.4167 - val_accuracy: 0.6792\n",
      "Epoch 116/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3790 - accuracy: 0.8573 - val_loss: 1.6788 - val_accuracy: 0.6542\n",
      "Epoch 117/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3622 - accuracy: 0.8503 - val_loss: 1.4590 - val_accuracy: 0.7042\n",
      "Epoch 118/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3357 - accuracy: 0.8642 - val_loss: 1.4585 - val_accuracy: 0.7000\n",
      "Epoch 119/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3269 - accuracy: 0.8652 - val_loss: 1.5156 - val_accuracy: 0.7042\n",
      "Epoch 120/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3497 - accuracy: 0.8614 - val_loss: 1.4932 - val_accuracy: 0.7208\n",
      "Epoch 121/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3215 - accuracy: 0.8767 - val_loss: 1.8347 - val_accuracy: 0.6667\n",
      "Epoch 122/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3676 - accuracy: 0.8587 - val_loss: 1.7479 - val_accuracy: 0.6458\n",
      "Epoch 123/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3453 - accuracy: 0.8698 - val_loss: 1.6784 - val_accuracy: 0.7125\n",
      "Epoch 124/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3076 - accuracy: 0.8809 - val_loss: 2.0052 - val_accuracy: 0.6500\n",
      "Epoch 125/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3325 - accuracy: 0.8749 - val_loss: 1.5550 - val_accuracy: 0.7250\n",
      "Epoch 126/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3150 - accuracy: 0.8712 - val_loss: 1.6324 - val_accuracy: 0.7042\n",
      "Epoch 127/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3065 - accuracy: 0.8814 - val_loss: 1.8048 - val_accuracy: 0.6833\n",
      "Epoch 128/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3655 - accuracy: 0.8661 - val_loss: 1.5472 - val_accuracy: 0.7042\n",
      "Epoch 129/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3369 - accuracy: 0.8707 - val_loss: 1.6685 - val_accuracy: 0.7250\n",
      "Epoch 130/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3089 - accuracy: 0.8786 - val_loss: 1.6800 - val_accuracy: 0.6833\n",
      "Epoch 131/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3303 - accuracy: 0.8749 - val_loss: 1.6514 - val_accuracy: 0.7167\n",
      "Epoch 132/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2871 - accuracy: 0.8892 - val_loss: 1.7159 - val_accuracy: 0.7042\n",
      "Epoch 133/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3256 - accuracy: 0.8777 - val_loss: 1.7163 - val_accuracy: 0.6833\n",
      "Epoch 134/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3054 - accuracy: 0.8781 - val_loss: 1.6349 - val_accuracy: 0.7208\n",
      "Epoch 135/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2823 - accuracy: 0.8967 - val_loss: 1.6804 - val_accuracy: 0.7083\n",
      "Epoch 136/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2876 - accuracy: 0.8883 - val_loss: 1.7021 - val_accuracy: 0.7208\n",
      "Epoch 137/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3161 - accuracy: 0.8879 - val_loss: 1.6679 - val_accuracy: 0.6792\n",
      "Epoch 138/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2708 - accuracy: 0.8934 - val_loss: 1.6300 - val_accuracy: 0.7167\n",
      "Epoch 139/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2736 - accuracy: 0.8916 - val_loss: 1.4601 - val_accuracy: 0.7625\n",
      "Epoch 140/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2700 - accuracy: 0.9036 - val_loss: 1.6199 - val_accuracy: 0.7417\n",
      "Epoch 141/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2731 - accuracy: 0.8962 - val_loss: 1.7231 - val_accuracy: 0.7542\n",
      "Epoch 142/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2992 - accuracy: 0.8897 - val_loss: 1.5865 - val_accuracy: 0.7542\n",
      "Epoch 143/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2732 - accuracy: 0.8943 - val_loss: 1.7067 - val_accuracy: 0.7208\n",
      "Epoch 144/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2781 - accuracy: 0.8990 - val_loss: 1.6869 - val_accuracy: 0.7375\n",
      "Epoch 145/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2804 - accuracy: 0.8930 - val_loss: 1.6154 - val_accuracy: 0.7250\n",
      "Epoch 146/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2748 - accuracy: 0.8902 - val_loss: 1.7919 - val_accuracy: 0.7083\n",
      "Epoch 147/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2629 - accuracy: 0.8957 - val_loss: 1.6822 - val_accuracy: 0.7250\n",
      "Epoch 148/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2569 - accuracy: 0.9082 - val_loss: 1.7890 - val_accuracy: 0.7125\n",
      "Epoch 149/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2731 - accuracy: 0.9055 - val_loss: 1.6405 - val_accuracy: 0.7167\n",
      "Epoch 150/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2844 - accuracy: 0.8962 - val_loss: 1.7057 - val_accuracy: 0.6917\n",
      "Epoch 151/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2541 - accuracy: 0.9055 - val_loss: 1.7080 - val_accuracy: 0.6833\n",
      "Epoch 152/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2797 - accuracy: 0.8920 - val_loss: 1.6298 - val_accuracy: 0.7208\n",
      "Epoch 153/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2116 - accuracy: 0.9240 - val_loss: 1.7411 - val_accuracy: 0.7375\n",
      "Epoch 154/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2453 - accuracy: 0.9110 - val_loss: 1.7333 - val_accuracy: 0.7083\n",
      "Epoch 155/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2848 - accuracy: 0.8948 - val_loss: 1.7940 - val_accuracy: 0.7042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2506 - accuracy: 0.9027 - val_loss: 1.7106 - val_accuracy: 0.7542\n",
      "Epoch 157/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2216 - accuracy: 0.9222 - val_loss: 1.9024 - val_accuracy: 0.7167\n",
      "Epoch 158/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2535 - accuracy: 0.8994 - val_loss: 1.8283 - val_accuracy: 0.6958\n",
      "Epoch 159/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2458 - accuracy: 0.9078 - val_loss: 1.9455 - val_accuracy: 0.6792\n",
      "Epoch 160/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2483 - accuracy: 0.9106 - val_loss: 1.7379 - val_accuracy: 0.7083\n",
      "Epoch 161/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2452 - accuracy: 0.9027 - val_loss: 1.7499 - val_accuracy: 0.7125\n",
      "Epoch 162/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2300 - accuracy: 0.9189 - val_loss: 1.8884 - val_accuracy: 0.6833\n",
      "Epoch 163/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2358 - accuracy: 0.9069 - val_loss: 2.3937 - val_accuracy: 0.5875\n",
      "Epoch 164/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2254 - accuracy: 0.9157 - val_loss: 1.8358 - val_accuracy: 0.7167\n",
      "Epoch 165/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2163 - accuracy: 0.9171 - val_loss: 1.8953 - val_accuracy: 0.7042\n",
      "Epoch 166/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2154 - accuracy: 0.9203 - val_loss: 1.8924 - val_accuracy: 0.7125\n",
      "Epoch 167/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2330 - accuracy: 0.9087 - val_loss: 1.7670 - val_accuracy: 0.7000\n",
      "Epoch 168/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2343 - accuracy: 0.9143 - val_loss: 1.7198 - val_accuracy: 0.7333\n",
      "Epoch 169/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2105 - accuracy: 0.9226 - val_loss: 1.7381 - val_accuracy: 0.7167\n",
      "Epoch 170/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2260 - accuracy: 0.9138 - val_loss: 1.9693 - val_accuracy: 0.6917\n",
      "Epoch 171/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1996 - accuracy: 0.9268 - val_loss: 1.8943 - val_accuracy: 0.7083\n",
      "Epoch 172/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2175 - accuracy: 0.9189 - val_loss: 1.9183 - val_accuracy: 0.7083\n",
      "Epoch 173/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2591 - accuracy: 0.9036 - val_loss: 1.8111 - val_accuracy: 0.6500\n",
      "Epoch 174/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2645 - accuracy: 0.9032 - val_loss: 1.7842 - val_accuracy: 0.7417\n",
      "Epoch 175/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2077 - accuracy: 0.9226 - val_loss: 1.7779 - val_accuracy: 0.7125\n",
      "Epoch 176/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1871 - accuracy: 0.9272 - val_loss: 2.0569 - val_accuracy: 0.7208\n",
      "Epoch 177/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1937 - accuracy: 0.9277 - val_loss: 2.0652 - val_accuracy: 0.6958\n",
      "Epoch 178/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2275 - accuracy: 0.9152 - val_loss: 1.8312 - val_accuracy: 0.7333\n",
      "Epoch 179/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2143 - accuracy: 0.9198 - val_loss: 1.9144 - val_accuracy: 0.7375\n",
      "Epoch 180/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2057 - accuracy: 0.9212 - val_loss: 1.7880 - val_accuracy: 0.7250\n",
      "Epoch 181/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1970 - accuracy: 0.9310 - val_loss: 1.7832 - val_accuracy: 0.7375\n",
      "Epoch 182/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2112 - accuracy: 0.9277 - val_loss: 1.7718 - val_accuracy: 0.7417\n",
      "Epoch 183/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1819 - accuracy: 0.9305 - val_loss: 2.0632 - val_accuracy: 0.7208\n",
      "Epoch 184/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1777 - accuracy: 0.9361 - val_loss: 2.0260 - val_accuracy: 0.7042\n",
      "Epoch 185/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1986 - accuracy: 0.9268 - val_loss: 1.9320 - val_accuracy: 0.7333\n",
      "Epoch 186/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2030 - accuracy: 0.9296 - val_loss: 1.9244 - val_accuracy: 0.7292\n",
      "Epoch 187/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2081 - accuracy: 0.9212 - val_loss: 2.0241 - val_accuracy: 0.6625\n",
      "Epoch 188/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1688 - accuracy: 0.9342 - val_loss: 1.7641 - val_accuracy: 0.7083\n",
      "Epoch 189/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1658 - accuracy: 0.9296 - val_loss: 1.9052 - val_accuracy: 0.7375\n",
      "Epoch 190/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1868 - accuracy: 0.9337 - val_loss: 1.8942 - val_accuracy: 0.7417\n",
      "Epoch 191/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1902 - accuracy: 0.9342 - val_loss: 2.0549 - val_accuracy: 0.7292\n",
      "Epoch 192/800\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2049 - accuracy: 0.9272 - val_loss: 1.8570 - val_accuracy: 0.7167\n",
      "Epoch 193/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2100 - accuracy: 0.9212 - val_loss: 2.1430 - val_accuracy: 0.7167\n",
      "Epoch 194/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2002 - accuracy: 0.9272 - val_loss: 2.1216 - val_accuracy: 0.7208\n",
      "Epoch 195/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1966 - accuracy: 0.9286 - val_loss: 2.0612 - val_accuracy: 0.7125\n",
      "Epoch 196/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1672 - accuracy: 0.9398 - val_loss: 2.2170 - val_accuracy: 0.7000\n",
      "Epoch 197/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1650 - accuracy: 0.9402 - val_loss: 2.0860 - val_accuracy: 0.7542\n",
      "Epoch 198/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1770 - accuracy: 0.9356 - val_loss: 2.0690 - val_accuracy: 0.7125\n",
      "Epoch 199/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1523 - accuracy: 0.9439 - val_loss: 1.9136 - val_accuracy: 0.7375\n",
      "Epoch 200/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1752 - accuracy: 0.9361 - val_loss: 1.9612 - val_accuracy: 0.7708\n",
      "Epoch 201/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2106 - accuracy: 0.9286 - val_loss: 1.9502 - val_accuracy: 0.7208\n",
      "Epoch 202/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1926 - accuracy: 0.9323 - val_loss: 1.9031 - val_accuracy: 0.7500\n",
      "Epoch 203/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1695 - accuracy: 0.9365 - val_loss: 2.0134 - val_accuracy: 0.7333\n",
      "Epoch 204/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1619 - accuracy: 0.9388 - val_loss: 1.8774 - val_accuracy: 0.7458\n",
      "Epoch 205/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1714 - accuracy: 0.9337 - val_loss: 2.0962 - val_accuracy: 0.7167\n",
      "Epoch 206/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1630 - accuracy: 0.9458 - val_loss: 1.9616 - val_accuracy: 0.7333\n",
      "Epoch 207/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1569 - accuracy: 0.9384 - val_loss: 2.0502 - val_accuracy: 0.7417\n",
      "Epoch 208/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1772 - accuracy: 0.9370 - val_loss: 1.9268 - val_accuracy: 0.7542\n",
      "Epoch 209/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1631 - accuracy: 0.9370 - val_loss: 1.9250 - val_accuracy: 0.7458\n",
      "Epoch 210/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1624 - accuracy: 0.9379 - val_loss: 2.2177 - val_accuracy: 0.7042\n",
      "Epoch 211/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1676 - accuracy: 0.9411 - val_loss: 2.3044 - val_accuracy: 0.7042\n",
      "Epoch 212/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1753 - accuracy: 0.9379 - val_loss: 2.0699 - val_accuracy: 0.7208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1679 - accuracy: 0.9384 - val_loss: 1.9861 - val_accuracy: 0.7167\n",
      "Epoch 214/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1672 - accuracy: 0.9435 - val_loss: 2.0520 - val_accuracy: 0.7208\n",
      "Epoch 215/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1726 - accuracy: 0.9361 - val_loss: 1.9401 - val_accuracy: 0.7542\n",
      "Epoch 216/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1692 - accuracy: 0.9458 - val_loss: 2.1085 - val_accuracy: 0.7333\n",
      "Epoch 217/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1661 - accuracy: 0.9453 - val_loss: 2.0435 - val_accuracy: 0.7292\n",
      "Epoch 218/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1914 - accuracy: 0.9328 - val_loss: 2.0523 - val_accuracy: 0.7167\n",
      "Epoch 219/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1842 - accuracy: 0.9356 - val_loss: 2.0461 - val_accuracy: 0.7375\n",
      "Epoch 220/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1626 - accuracy: 0.9398 - val_loss: 2.2019 - val_accuracy: 0.6750\n",
      "Epoch 221/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1392 - accuracy: 0.9495 - val_loss: 2.0323 - val_accuracy: 0.7625\n",
      "Epoch 222/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1378 - accuracy: 0.9430 - val_loss: 2.3148 - val_accuracy: 0.7125\n",
      "Epoch 223/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1518 - accuracy: 0.9444 - val_loss: 2.4206 - val_accuracy: 0.6875\n",
      "Epoch 224/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1826 - accuracy: 0.9365 - val_loss: 2.0736 - val_accuracy: 0.7125\n",
      "Epoch 225/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1363 - accuracy: 0.9513 - val_loss: 2.6544 - val_accuracy: 0.6333\n",
      "Epoch 226/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1537 - accuracy: 0.9518 - val_loss: 2.2110 - val_accuracy: 0.7542\n",
      "Epoch 227/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1471 - accuracy: 0.9467 - val_loss: 2.0605 - val_accuracy: 0.7583\n",
      "Epoch 228/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1447 - accuracy: 0.9467 - val_loss: 1.9179 - val_accuracy: 0.7542\n",
      "Epoch 229/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1754 - accuracy: 0.9421 - val_loss: 2.2758 - val_accuracy: 0.6917\n",
      "Epoch 230/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1704 - accuracy: 0.9384 - val_loss: 2.0750 - val_accuracy: 0.7500\n",
      "Epoch 231/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1503 - accuracy: 0.9476 - val_loss: 2.0080 - val_accuracy: 0.7333\n",
      "Epoch 232/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1473 - accuracy: 0.9458 - val_loss: 2.0908 - val_accuracy: 0.7417\n",
      "Epoch 233/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1394 - accuracy: 0.9527 - val_loss: 2.0770 - val_accuracy: 0.7333\n",
      "Epoch 234/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1431 - accuracy: 0.9462 - val_loss: 1.9910 - val_accuracy: 0.7458\n",
      "Epoch 235/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1340 - accuracy: 0.9518 - val_loss: 2.0617 - val_accuracy: 0.7583\n",
      "Epoch 236/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1557 - accuracy: 0.9430 - val_loss: 1.9704 - val_accuracy: 0.7500\n",
      "Epoch 237/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1465 - accuracy: 0.9462 - val_loss: 2.1239 - val_accuracy: 0.7542\n",
      "Epoch 238/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1364 - accuracy: 0.9504 - val_loss: 2.0818 - val_accuracy: 0.7375\n",
      "Epoch 239/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1315 - accuracy: 0.9546 - val_loss: 2.0797 - val_accuracy: 0.7542\n",
      "Epoch 240/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1319 - accuracy: 0.9537 - val_loss: 2.2021 - val_accuracy: 0.7583\n",
      "Epoch 241/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1358 - accuracy: 0.9486 - val_loss: 2.3048 - val_accuracy: 0.6833\n",
      "Epoch 242/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1463 - accuracy: 0.9486 - val_loss: 2.1752 - val_accuracy: 0.7500\n",
      "Epoch 243/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1417 - accuracy: 0.9462 - val_loss: 2.2533 - val_accuracy: 0.7500\n",
      "Epoch 244/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1325 - accuracy: 0.9532 - val_loss: 2.1571 - val_accuracy: 0.7333\n",
      "Epoch 245/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1594 - accuracy: 0.9444 - val_loss: 2.0739 - val_accuracy: 0.7417\n",
      "Epoch 246/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1240 - accuracy: 0.9555 - val_loss: 1.8939 - val_accuracy: 0.7833\n",
      "Epoch 247/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1369 - accuracy: 0.9486 - val_loss: 1.9776 - val_accuracy: 0.7750\n",
      "Epoch 248/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1286 - accuracy: 0.9518 - val_loss: 2.0679 - val_accuracy: 0.7500\n",
      "Epoch 249/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1431 - accuracy: 0.9449 - val_loss: 2.2450 - val_accuracy: 0.7292\n",
      "Epoch 250/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1468 - accuracy: 0.9523 - val_loss: 1.9714 - val_accuracy: 0.7458\n",
      "Epoch 251/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1506 - accuracy: 0.9462 - val_loss: 1.9743 - val_accuracy: 0.7292\n",
      "Epoch 252/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1455 - accuracy: 0.9462 - val_loss: 1.9245 - val_accuracy: 0.7458\n",
      "Epoch 253/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1480 - accuracy: 0.9458 - val_loss: 2.0145 - val_accuracy: 0.7250\n",
      "Epoch 254/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1354 - accuracy: 0.9504 - val_loss: 1.9680 - val_accuracy: 0.7458\n",
      "Epoch 255/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1163 - accuracy: 0.9513 - val_loss: 2.0019 - val_accuracy: 0.7167\n",
      "Epoch 256/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1490 - accuracy: 0.9472 - val_loss: 2.0076 - val_accuracy: 0.7208\n",
      "Epoch 257/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1351 - accuracy: 0.9513 - val_loss: 2.0991 - val_accuracy: 0.7417\n",
      "Epoch 258/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1296 - accuracy: 0.9527 - val_loss: 2.0310 - val_accuracy: 0.7500\n",
      "Epoch 259/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1314 - accuracy: 0.9518 - val_loss: 1.9736 - val_accuracy: 0.7375\n",
      "Epoch 260/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1329 - accuracy: 0.9537 - val_loss: 1.9563 - val_accuracy: 0.7792\n",
      "Epoch 261/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1175 - accuracy: 0.9583 - val_loss: 2.0785 - val_accuracy: 0.7417\n",
      "Epoch 262/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1244 - accuracy: 0.9560 - val_loss: 1.9600 - val_accuracy: 0.7667\n",
      "Epoch 263/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1144 - accuracy: 0.9597 - val_loss: 2.0170 - val_accuracy: 0.7583\n",
      "Epoch 264/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1069 - accuracy: 0.9588 - val_loss: 2.1129 - val_accuracy: 0.7458\n",
      "Epoch 265/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1147 - accuracy: 0.9592 - val_loss: 2.1818 - val_accuracy: 0.7542\n",
      "Epoch 266/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0987 - accuracy: 0.9648 - val_loss: 2.3385 - val_accuracy: 0.7292\n",
      "Epoch 267/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1767 - accuracy: 0.9361 - val_loss: 1.9560 - val_accuracy: 0.7292\n",
      "Epoch 268/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1426 - accuracy: 0.9504 - val_loss: 1.8228 - val_accuracy: 0.7458\n",
      "Epoch 269/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1105 - accuracy: 0.9588 - val_loss: 1.9405 - val_accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1188 - accuracy: 0.9555 - val_loss: 1.9362 - val_accuracy: 0.7833\n",
      "Epoch 271/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1394 - accuracy: 0.9527 - val_loss: 1.7965 - val_accuracy: 0.7750\n",
      "Epoch 272/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1307 - accuracy: 0.9583 - val_loss: 1.9425 - val_accuracy: 0.7792\n",
      "Epoch 273/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1107 - accuracy: 0.9615 - val_loss: 1.9856 - val_accuracy: 0.7750\n",
      "Epoch 274/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1341 - accuracy: 0.9504 - val_loss: 1.9504 - val_accuracy: 0.7375\n",
      "Epoch 275/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1293 - accuracy: 0.9564 - val_loss: 2.0493 - val_accuracy: 0.7583\n",
      "Epoch 276/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1350 - accuracy: 0.9523 - val_loss: 1.9769 - val_accuracy: 0.7625\n",
      "Epoch 277/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1276 - accuracy: 0.9527 - val_loss: 1.9779 - val_accuracy: 0.7583\n",
      "Epoch 278/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1155 - accuracy: 0.9564 - val_loss: 2.1269 - val_accuracy: 0.7625\n",
      "Epoch 279/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1589 - accuracy: 0.9462 - val_loss: 1.9653 - val_accuracy: 0.7500\n",
      "Epoch 280/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1147 - accuracy: 0.9639 - val_loss: 2.0444 - val_accuracy: 0.7750\n",
      "Epoch 281/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1819 - accuracy: 0.9449 - val_loss: 1.7504 - val_accuracy: 0.7625\n",
      "Epoch 282/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1245 - accuracy: 0.9643 - val_loss: 1.9474 - val_accuracy: 0.7458\n",
      "Epoch 283/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1322 - accuracy: 0.9537 - val_loss: 2.0665 - val_accuracy: 0.7583\n",
      "Epoch 284/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1136 - accuracy: 0.9601 - val_loss: 2.0214 - val_accuracy: 0.7042\n",
      "Epoch 285/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1074 - accuracy: 0.9662 - val_loss: 1.9327 - val_accuracy: 0.7458\n",
      "Epoch 286/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1072 - accuracy: 0.9639 - val_loss: 1.9337 - val_accuracy: 0.7583\n",
      "Epoch 287/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1077 - accuracy: 0.9657 - val_loss: 2.0957 - val_accuracy: 0.7708\n",
      "Epoch 288/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1125 - accuracy: 0.9643 - val_loss: 2.2689 - val_accuracy: 0.7500\n",
      "Epoch 289/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1272 - accuracy: 0.9601 - val_loss: 2.1144 - val_accuracy: 0.7583\n",
      "Epoch 290/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1223 - accuracy: 0.9606 - val_loss: 1.8185 - val_accuracy: 0.7583\n",
      "Epoch 291/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0966 - accuracy: 0.9652 - val_loss: 2.0071 - val_accuracy: 0.7708\n",
      "Epoch 292/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1308 - accuracy: 0.9555 - val_loss: 2.0066 - val_accuracy: 0.7583\n",
      "Epoch 293/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1321 - accuracy: 0.9518 - val_loss: 2.0564 - val_accuracy: 0.7292\n",
      "Epoch 294/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1076 - accuracy: 0.9652 - val_loss: 2.0570 - val_accuracy: 0.7583\n",
      "Epoch 295/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1125 - accuracy: 0.9597 - val_loss: 2.0462 - val_accuracy: 0.7625\n",
      "Epoch 296/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1016 - accuracy: 0.9601 - val_loss: 2.1934 - val_accuracy: 0.7750\n",
      "Epoch 297/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1057 - accuracy: 0.9639 - val_loss: 2.1853 - val_accuracy: 0.7667\n",
      "Epoch 298/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0954 - accuracy: 0.9731 - val_loss: 2.4879 - val_accuracy: 0.7250\n",
      "Epoch 299/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1485 - accuracy: 0.9537 - val_loss: 2.1646 - val_accuracy: 0.7500\n",
      "Epoch 300/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1268 - accuracy: 0.9546 - val_loss: 2.4504 - val_accuracy: 0.7417\n",
      "Epoch 301/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1112 - accuracy: 0.9625 - val_loss: 2.0297 - val_accuracy: 0.7500\n",
      "Epoch 302/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0889 - accuracy: 0.9690 - val_loss: 2.2186 - val_accuracy: 0.7625\n",
      "Epoch 303/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1064 - accuracy: 0.9657 - val_loss: 2.0011 - val_accuracy: 0.7667\n",
      "Epoch 304/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1163 - accuracy: 0.9606 - val_loss: 2.0594 - val_accuracy: 0.7542\n",
      "Epoch 305/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1256 - accuracy: 0.9560 - val_loss: 2.0074 - val_accuracy: 0.7708\n",
      "Epoch 306/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0987 - accuracy: 0.9648 - val_loss: 2.1296 - val_accuracy: 0.7583\n",
      "Epoch 307/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1028 - accuracy: 0.9648 - val_loss: 2.2456 - val_accuracy: 0.7500\n",
      "Epoch 308/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1279 - accuracy: 0.9564 - val_loss: 1.9895 - val_accuracy: 0.7792\n",
      "Epoch 309/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1059 - accuracy: 0.9611 - val_loss: 1.9992 - val_accuracy: 0.7542\n",
      "Epoch 310/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1012 - accuracy: 0.9657 - val_loss: 1.9510 - val_accuracy: 0.7750\n",
      "Epoch 311/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1077 - accuracy: 0.9592 - val_loss: 2.1141 - val_accuracy: 0.7542\n",
      "Epoch 312/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0865 - accuracy: 0.9750 - val_loss: 2.1608 - val_accuracy: 0.7708\n",
      "Epoch 313/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1015 - accuracy: 0.9648 - val_loss: 2.2902 - val_accuracy: 0.7625\n",
      "Epoch 314/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0889 - accuracy: 0.9685 - val_loss: 2.2614 - val_accuracy: 0.7708\n",
      "Epoch 315/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0970 - accuracy: 0.9643 - val_loss: 2.1436 - val_accuracy: 0.7500\n",
      "Epoch 316/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1034 - accuracy: 0.9662 - val_loss: 2.0702 - val_accuracy: 0.7625\n",
      "Epoch 317/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0959 - accuracy: 0.9680 - val_loss: 2.1774 - val_accuracy: 0.7583\n",
      "Epoch 318/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1083 - accuracy: 0.9615 - val_loss: 2.1322 - val_accuracy: 0.7542\n",
      "Epoch 319/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1304 - accuracy: 0.9578 - val_loss: 2.0989 - val_accuracy: 0.7292\n",
      "Epoch 320/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1540 - accuracy: 0.9560 - val_loss: 2.0125 - val_accuracy: 0.7417\n",
      "Epoch 321/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1318 - accuracy: 0.9504 - val_loss: 2.0961 - val_accuracy: 0.7542\n",
      "Epoch 322/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1056 - accuracy: 0.9578 - val_loss: 2.1466 - val_accuracy: 0.7583\n",
      "Epoch 323/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0914 - accuracy: 0.9699 - val_loss: 2.1434 - val_accuracy: 0.7583\n",
      "Epoch 324/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0966 - accuracy: 0.9666 - val_loss: 2.1527 - val_accuracy: 0.7458\n",
      "Epoch 325/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1006 - accuracy: 0.9611 - val_loss: 2.0828 - val_accuracy: 0.7458\n",
      "Epoch 326/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0944 - accuracy: 0.9648 - val_loss: 2.2185 - val_accuracy: 0.7417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 327/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1207 - accuracy: 0.9588 - val_loss: 2.1120 - val_accuracy: 0.7667\n",
      "Epoch 328/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1079 - accuracy: 0.9629 - val_loss: 2.2203 - val_accuracy: 0.7292\n",
      "Epoch 329/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0952 - accuracy: 0.9629 - val_loss: 2.3024 - val_accuracy: 0.7292\n",
      "Epoch 330/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1297 - accuracy: 0.9551 - val_loss: 1.9348 - val_accuracy: 0.7375\n",
      "Epoch 331/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0823 - accuracy: 0.9694 - val_loss: 2.0839 - val_accuracy: 0.7542\n",
      "Epoch 332/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1138 - accuracy: 0.9643 - val_loss: 2.1093 - val_accuracy: 0.7250\n",
      "Epoch 333/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0973 - accuracy: 0.9662 - val_loss: 2.2777 - val_accuracy: 0.7667\n",
      "Epoch 334/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0910 - accuracy: 0.9708 - val_loss: 2.3447 - val_accuracy: 0.7542\n",
      "Epoch 335/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0962 - accuracy: 0.9717 - val_loss: 2.0249 - val_accuracy: 0.7583\n",
      "Epoch 336/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0960 - accuracy: 0.9694 - val_loss: 2.2055 - val_accuracy: 0.7667\n",
      "Epoch 337/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0870 - accuracy: 0.9722 - val_loss: 2.2896 - val_accuracy: 0.7500\n",
      "Epoch 338/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1098 - accuracy: 0.9643 - val_loss: 2.3853 - val_accuracy: 0.7333\n",
      "Epoch 339/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0811 - accuracy: 0.9754 - val_loss: 2.4356 - val_accuracy: 0.7542\n",
      "Epoch 340/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1310 - accuracy: 0.9588 - val_loss: 2.1301 - val_accuracy: 0.7750\n",
      "Epoch 341/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0971 - accuracy: 0.9652 - val_loss: 2.0051 - val_accuracy: 0.7667\n",
      "Epoch 342/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0839 - accuracy: 0.9731 - val_loss: 2.1317 - val_accuracy: 0.7625\n",
      "Epoch 343/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0741 - accuracy: 0.9741 - val_loss: 2.3948 - val_accuracy: 0.7583\n",
      "Epoch 344/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0969 - accuracy: 0.9662 - val_loss: 2.3181 - val_accuracy: 0.7458\n",
      "Epoch 345/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0836 - accuracy: 0.9727 - val_loss: 2.4044 - val_accuracy: 0.7417\n",
      "Epoch 346/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1370 - accuracy: 0.9606 - val_loss: 2.0006 - val_accuracy: 0.7542\n",
      "Epoch 347/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0857 - accuracy: 0.9699 - val_loss: 2.0365 - val_accuracy: 0.7917\n",
      "Epoch 348/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0734 - accuracy: 0.9764 - val_loss: 2.1372 - val_accuracy: 0.7833\n",
      "Epoch 349/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0880 - accuracy: 0.9713 - val_loss: 2.2906 - val_accuracy: 0.7625\n",
      "Epoch 350/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0787 - accuracy: 0.9754 - val_loss: 2.3695 - val_accuracy: 0.7250\n",
      "Epoch 351/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0760 - accuracy: 0.9745 - val_loss: 3.0321 - val_accuracy: 0.6917\n",
      "Epoch 352/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1381 - accuracy: 0.9611 - val_loss: 2.1655 - val_accuracy: 0.7583\n",
      "Epoch 353/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1024 - accuracy: 0.9643 - val_loss: 2.1834 - val_accuracy: 0.7625\n",
      "Epoch 354/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1099 - accuracy: 0.9634 - val_loss: 2.4029 - val_accuracy: 0.7250\n",
      "Epoch 355/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1016 - accuracy: 0.9699 - val_loss: 2.2289 - val_accuracy: 0.7417\n",
      "Epoch 356/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0943 - accuracy: 0.9676 - val_loss: 2.0760 - val_accuracy: 0.7750\n",
      "Epoch 357/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1105 - accuracy: 0.9643 - val_loss: 2.2749 - val_accuracy: 0.7625\n",
      "Epoch 358/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0765 - accuracy: 0.9764 - val_loss: 2.3394 - val_accuracy: 0.7208\n",
      "Epoch 359/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0924 - accuracy: 0.9690 - val_loss: 2.2188 - val_accuracy: 0.7583\n",
      "Epoch 360/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0832 - accuracy: 0.9736 - val_loss: 2.2858 - val_accuracy: 0.7542\n",
      "Epoch 361/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0870 - accuracy: 0.9713 - val_loss: 2.3305 - val_accuracy: 0.7625\n",
      "Epoch 362/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0902 - accuracy: 0.9759 - val_loss: 1.9350 - val_accuracy: 0.7917\n",
      "Epoch 363/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0943 - accuracy: 0.9676 - val_loss: 2.2961 - val_accuracy: 0.7333\n",
      "Epoch 364/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0892 - accuracy: 0.9676 - val_loss: 2.1771 - val_accuracy: 0.7500\n",
      "Epoch 365/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0824 - accuracy: 0.9713 - val_loss: 2.6025 - val_accuracy: 0.7333\n",
      "Epoch 366/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0907 - accuracy: 0.9703 - val_loss: 2.2262 - val_accuracy: 0.7708\n",
      "Epoch 367/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0809 - accuracy: 0.9722 - val_loss: 2.3865 - val_accuracy: 0.7458\n",
      "Epoch 368/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0830 - accuracy: 0.9717 - val_loss: 2.3136 - val_accuracy: 0.7667\n",
      "Epoch 369/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1006 - accuracy: 0.9652 - val_loss: 2.2334 - val_accuracy: 0.7708\n",
      "Epoch 370/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0848 - accuracy: 0.9685 - val_loss: 2.1807 - val_accuracy: 0.7750\n",
      "Epoch 371/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0899 - accuracy: 0.9680 - val_loss: 2.4764 - val_accuracy: 0.7667\n",
      "Epoch 372/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0773 - accuracy: 0.9731 - val_loss: 2.3184 - val_accuracy: 0.7750\n",
      "Epoch 373/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0872 - accuracy: 0.9713 - val_loss: 2.2414 - val_accuracy: 0.7458\n",
      "Epoch 374/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0812 - accuracy: 0.9727 - val_loss: 2.3487 - val_accuracy: 0.7833\n",
      "Epoch 375/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0737 - accuracy: 0.9741 - val_loss: 2.3880 - val_accuracy: 0.7750\n",
      "Epoch 376/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1148 - accuracy: 0.9657 - val_loss: 2.2052 - val_accuracy: 0.7625\n",
      "Epoch 377/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0895 - accuracy: 0.9750 - val_loss: 2.3760 - val_accuracy: 0.7292\n",
      "Epoch 378/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1119 - accuracy: 0.9671 - val_loss: 2.1369 - val_accuracy: 0.7583\n",
      "Epoch 379/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0785 - accuracy: 0.9703 - val_loss: 2.2136 - val_accuracy: 0.7792\n",
      "Epoch 380/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0819 - accuracy: 0.9731 - val_loss: 2.3351 - val_accuracy: 0.7667\n",
      "Epoch 381/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1017 - accuracy: 0.9662 - val_loss: 2.5617 - val_accuracy: 0.7458\n",
      "Epoch 382/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0780 - accuracy: 0.9727 - val_loss: 2.3999 - val_accuracy: 0.7625\n",
      "Epoch 383/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0655 - accuracy: 0.9778 - val_loss: 2.5486 - val_accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1154 - accuracy: 0.9676 - val_loss: 2.1074 - val_accuracy: 0.7708\n",
      "Epoch 385/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0879 - accuracy: 0.9671 - val_loss: 2.3295 - val_accuracy: 0.7292\n",
      "Epoch 386/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1187 - accuracy: 0.9574 - val_loss: 2.1890 - val_accuracy: 0.7625\n",
      "Epoch 387/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0702 - accuracy: 0.9782 - val_loss: 2.2865 - val_accuracy: 0.7667\n",
      "Epoch 388/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0999 - accuracy: 0.9690 - val_loss: 2.4381 - val_accuracy: 0.7375\n",
      "Epoch 389/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1093 - accuracy: 0.9676 - val_loss: 2.2763 - val_accuracy: 0.7292\n",
      "Epoch 390/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0793 - accuracy: 0.9722 - val_loss: 2.2285 - val_accuracy: 0.7417\n",
      "Epoch 391/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0787 - accuracy: 0.9741 - val_loss: 2.6509 - val_accuracy: 0.6958\n",
      "Epoch 392/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0850 - accuracy: 0.9676 - val_loss: 2.2638 - val_accuracy: 0.7500\n",
      "Epoch 393/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1146 - accuracy: 0.9625 - val_loss: 2.2143 - val_accuracy: 0.7542\n",
      "Epoch 394/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0887 - accuracy: 0.9736 - val_loss: 2.3159 - val_accuracy: 0.7458\n",
      "Epoch 395/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0769 - accuracy: 0.9741 - val_loss: 2.3998 - val_accuracy: 0.7875\n",
      "Epoch 396/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0802 - accuracy: 0.9727 - val_loss: 2.1918 - val_accuracy: 0.7500\n",
      "Epoch 397/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0935 - accuracy: 0.9708 - val_loss: 2.3634 - val_accuracy: 0.7625\n",
      "Epoch 398/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0993 - accuracy: 0.9708 - val_loss: 2.3589 - val_accuracy: 0.7417\n",
      "Epoch 399/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1270 - accuracy: 0.9629 - val_loss: 2.3550 - val_accuracy: 0.7542\n",
      "Epoch 400/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1100 - accuracy: 0.9680 - val_loss: 2.3333 - val_accuracy: 0.7333\n",
      "Epoch 401/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0841 - accuracy: 0.9745 - val_loss: 2.3272 - val_accuracy: 0.7625\n",
      "Epoch 402/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0819 - accuracy: 0.9727 - val_loss: 2.2357 - val_accuracy: 0.7500\n",
      "Epoch 403/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0796 - accuracy: 0.9750 - val_loss: 2.4537 - val_accuracy: 0.7417\n",
      "Epoch 404/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1008 - accuracy: 0.9694 - val_loss: 2.0838 - val_accuracy: 0.7625\n",
      "Epoch 405/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0714 - accuracy: 0.9727 - val_loss: 2.1449 - val_accuracy: 0.7667\n",
      "Epoch 406/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0715 - accuracy: 0.9764 - val_loss: 2.3002 - val_accuracy: 0.7625\n",
      "Epoch 407/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0789 - accuracy: 0.9759 - val_loss: 2.1602 - val_accuracy: 0.7542\n",
      "Epoch 408/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0638 - accuracy: 0.9768 - val_loss: 2.3683 - val_accuracy: 0.7583\n",
      "Epoch 409/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0885 - accuracy: 0.9727 - val_loss: 2.2914 - val_accuracy: 0.7708\n",
      "Epoch 410/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0593 - accuracy: 0.9829 - val_loss: 2.3168 - val_accuracy: 0.7250\n",
      "Epoch 411/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0554 - accuracy: 0.9801 - val_loss: 2.3563 - val_accuracy: 0.7458\n",
      "Epoch 412/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0735 - accuracy: 0.9750 - val_loss: 2.4942 - val_accuracy: 0.7625\n",
      "Epoch 413/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0891 - accuracy: 0.9671 - val_loss: 2.5877 - val_accuracy: 0.7250\n",
      "Epoch 414/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0742 - accuracy: 0.9801 - val_loss: 2.3395 - val_accuracy: 0.7750\n",
      "Epoch 415/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0662 - accuracy: 0.9801 - val_loss: 2.3078 - val_accuracy: 0.7667\n",
      "Epoch 416/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0739 - accuracy: 0.9764 - val_loss: 2.4154 - val_accuracy: 0.7417\n",
      "Epoch 417/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0692 - accuracy: 0.9745 - val_loss: 2.2689 - val_accuracy: 0.7917\n",
      "Epoch 418/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0815 - accuracy: 0.9778 - val_loss: 2.2352 - val_accuracy: 0.7667\n",
      "Epoch 419/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0754 - accuracy: 0.9782 - val_loss: 2.4017 - val_accuracy: 0.7542\n",
      "Epoch 420/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1006 - accuracy: 0.9671 - val_loss: 2.1883 - val_accuracy: 0.7667\n",
      "Epoch 421/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0815 - accuracy: 0.9731 - val_loss: 2.0292 - val_accuracy: 0.7750\n",
      "Epoch 422/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0863 - accuracy: 0.9703 - val_loss: 2.1981 - val_accuracy: 0.7458\n",
      "Epoch 423/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0651 - accuracy: 0.9773 - val_loss: 2.9909 - val_accuracy: 0.6708\n",
      "Epoch 424/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1232 - accuracy: 0.9597 - val_loss: 2.0451 - val_accuracy: 0.7542\n",
      "Epoch 425/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1037 - accuracy: 0.9648 - val_loss: 2.2486 - val_accuracy: 0.7458\n",
      "Epoch 426/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0946 - accuracy: 0.9676 - val_loss: 2.1061 - val_accuracy: 0.7667\n",
      "Epoch 427/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1167 - accuracy: 0.9680 - val_loss: 1.8889 - val_accuracy: 0.7750\n",
      "Epoch 428/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0976 - accuracy: 0.9666 - val_loss: 2.1454 - val_accuracy: 0.7542\n",
      "Epoch 429/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0859 - accuracy: 0.9741 - val_loss: 2.2245 - val_accuracy: 0.7667\n",
      "Epoch 430/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0773 - accuracy: 0.9717 - val_loss: 2.2537 - val_accuracy: 0.7875\n",
      "Epoch 431/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0542 - accuracy: 0.9838 - val_loss: 2.2022 - val_accuracy: 0.7875\n",
      "Epoch 432/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0589 - accuracy: 0.9801 - val_loss: 2.2437 - val_accuracy: 0.7875\n",
      "Epoch 433/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0683 - accuracy: 0.9754 - val_loss: 2.3587 - val_accuracy: 0.7875\n",
      "Epoch 434/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0636 - accuracy: 0.9805 - val_loss: 2.4421 - val_accuracy: 0.7833\n",
      "Epoch 435/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0738 - accuracy: 0.9731 - val_loss: 2.3981 - val_accuracy: 0.7583\n",
      "Epoch 436/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0722 - accuracy: 0.9759 - val_loss: 2.3904 - val_accuracy: 0.7542\n",
      "Epoch 437/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0679 - accuracy: 0.9778 - val_loss: 2.4779 - val_accuracy: 0.7542\n",
      "Epoch 438/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0663 - accuracy: 0.9805 - val_loss: 2.3767 - val_accuracy: 0.7625\n",
      "Epoch 439/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0663 - accuracy: 0.9778 - val_loss: 2.3823 - val_accuracy: 0.7583\n",
      "Epoch 440/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0758 - accuracy: 0.9745 - val_loss: 2.3544 - val_accuracy: 0.7458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1144 - accuracy: 0.9611 - val_loss: 2.3524 - val_accuracy: 0.7292\n",
      "Epoch 442/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1127 - accuracy: 0.9657 - val_loss: 1.9017 - val_accuracy: 0.7542\n",
      "Epoch 443/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0953 - accuracy: 0.9680 - val_loss: 2.1662 - val_accuracy: 0.7333\n",
      "Epoch 444/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0791 - accuracy: 0.9754 - val_loss: 2.0161 - val_accuracy: 0.7833\n",
      "Epoch 445/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0962 - accuracy: 0.9690 - val_loss: 2.2338 - val_accuracy: 0.7667\n",
      "Epoch 446/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1005 - accuracy: 0.9731 - val_loss: 2.0716 - val_accuracy: 0.7750\n",
      "Epoch 447/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0861 - accuracy: 0.9768 - val_loss: 2.2433 - val_accuracy: 0.7542\n",
      "Epoch 448/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0583 - accuracy: 0.9810 - val_loss: 2.2289 - val_accuracy: 0.7625\n",
      "Epoch 449/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0631 - accuracy: 0.9796 - val_loss: 2.3939 - val_accuracy: 0.7583\n",
      "Epoch 450/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0685 - accuracy: 0.9745 - val_loss: 2.5674 - val_accuracy: 0.7333\n",
      "Epoch 451/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1086 - accuracy: 0.9671 - val_loss: 2.3284 - val_accuracy: 0.7458\n",
      "Epoch 452/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0834 - accuracy: 0.9727 - val_loss: 2.2651 - val_accuracy: 0.7583\n",
      "Epoch 453/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0821 - accuracy: 0.9745 - val_loss: 2.0621 - val_accuracy: 0.7708\n",
      "Epoch 454/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0895 - accuracy: 0.9741 - val_loss: 2.1547 - val_accuracy: 0.7583\n",
      "Epoch 455/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0707 - accuracy: 0.9787 - val_loss: 2.3292 - val_accuracy: 0.7542\n",
      "Epoch 456/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0676 - accuracy: 0.9824 - val_loss: 2.2900 - val_accuracy: 0.7667\n",
      "Epoch 457/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0525 - accuracy: 0.9815 - val_loss: 2.3609 - val_accuracy: 0.7667\n",
      "Epoch 458/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0884 - accuracy: 0.9703 - val_loss: 2.3426 - val_accuracy: 0.7625\n",
      "Epoch 459/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0672 - accuracy: 0.9782 - val_loss: 2.4590 - val_accuracy: 0.7625\n",
      "Epoch 460/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0787 - accuracy: 0.9736 - val_loss: 2.3319 - val_accuracy: 0.7583\n",
      "Epoch 461/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0679 - accuracy: 0.9787 - val_loss: 2.3943 - val_accuracy: 0.7500\n",
      "Epoch 462/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0694 - accuracy: 0.9796 - val_loss: 2.4052 - val_accuracy: 0.7667\n",
      "Epoch 463/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0657 - accuracy: 0.9810 - val_loss: 2.2133 - val_accuracy: 0.7583\n",
      "Epoch 464/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0663 - accuracy: 0.9768 - val_loss: 2.1249 - val_accuracy: 0.7667\n",
      "Epoch 465/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0633 - accuracy: 0.9819 - val_loss: 2.2067 - val_accuracy: 0.7667\n",
      "Epoch 466/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0496 - accuracy: 0.9838 - val_loss: 2.2323 - val_accuracy: 0.7417\n",
      "Epoch 467/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0815 - accuracy: 0.9731 - val_loss: 2.2675 - val_accuracy: 0.7792\n",
      "Epoch 468/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0712 - accuracy: 0.9778 - val_loss: 2.2834 - val_accuracy: 0.7625\n",
      "Epoch 469/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0564 - accuracy: 0.9829 - val_loss: 2.2462 - val_accuracy: 0.7583\n",
      "Epoch 470/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0773 - accuracy: 0.9731 - val_loss: 2.3226 - val_accuracy: 0.7167\n",
      "Epoch 471/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0622 - accuracy: 0.9787 - val_loss: 2.3046 - val_accuracy: 0.7625\n",
      "Epoch 472/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0589 - accuracy: 0.9787 - val_loss: 2.1951 - val_accuracy: 0.7750\n",
      "Epoch 473/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0632 - accuracy: 0.9773 - val_loss: 2.3742 - val_accuracy: 0.7583\n",
      "Epoch 474/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0633 - accuracy: 0.9778 - val_loss: 2.4282 - val_accuracy: 0.7792\n",
      "Epoch 475/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0646 - accuracy: 0.9782 - val_loss: 2.4668 - val_accuracy: 0.7667\n",
      "Epoch 476/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0650 - accuracy: 0.9731 - val_loss: 2.2609 - val_accuracy: 0.7750\n",
      "Epoch 477/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0855 - accuracy: 0.9759 - val_loss: 2.2957 - val_accuracy: 0.7792\n",
      "Epoch 478/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0557 - accuracy: 0.9805 - val_loss: 2.4415 - val_accuracy: 0.7708\n",
      "Epoch 479/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0446 - accuracy: 0.9856 - val_loss: 2.3694 - val_accuracy: 0.7833\n",
      "Epoch 480/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0774 - accuracy: 0.9731 - val_loss: 2.5830 - val_accuracy: 0.7333\n",
      "Epoch 481/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0745 - accuracy: 0.9759 - val_loss: 2.4468 - val_accuracy: 0.7542\n",
      "Epoch 482/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0762 - accuracy: 0.9754 - val_loss: 2.3099 - val_accuracy: 0.7208\n",
      "Epoch 483/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0607 - accuracy: 0.9805 - val_loss: 2.3281 - val_accuracy: 0.7750\n",
      "Epoch 484/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0689 - accuracy: 0.9805 - val_loss: 2.7101 - val_accuracy: 0.7500\n",
      "Epoch 485/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1210 - accuracy: 0.9657 - val_loss: 2.3359 - val_accuracy: 0.7542\n",
      "Epoch 486/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0660 - accuracy: 0.9773 - val_loss: 2.1502 - val_accuracy: 0.7708\n",
      "Epoch 487/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0523 - accuracy: 0.9833 - val_loss: 2.2772 - val_accuracy: 0.7792\n",
      "Epoch 488/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0991 - accuracy: 0.9731 - val_loss: 2.0266 - val_accuracy: 0.7542\n",
      "Epoch 489/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 2.3560 - val_accuracy: 0.7542\n",
      "Epoch 490/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0723 - accuracy: 0.9741 - val_loss: 2.1855 - val_accuracy: 0.7625\n",
      "Epoch 491/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0726 - accuracy: 0.9796 - val_loss: 2.1291 - val_accuracy: 0.7708\n",
      "Epoch 492/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0692 - accuracy: 0.9773 - val_loss: 2.0497 - val_accuracy: 0.7625\n",
      "Epoch 493/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0499 - accuracy: 0.9842 - val_loss: 2.2961 - val_accuracy: 0.7542\n",
      "Epoch 494/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0773 - accuracy: 0.9759 - val_loss: 2.1653 - val_accuracy: 0.7500\n",
      "Epoch 495/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0843 - accuracy: 0.9750 - val_loss: 2.3355 - val_accuracy: 0.7500\n",
      "Epoch 496/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0566 - accuracy: 0.9824 - val_loss: 2.2561 - val_accuracy: 0.7833\n",
      "Epoch 497/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0672 - accuracy: 0.9791 - val_loss: 2.3151 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0650 - accuracy: 0.9787 - val_loss: 2.3843 - val_accuracy: 0.7833\n",
      "Epoch 499/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0653 - accuracy: 0.9801 - val_loss: 2.4448 - val_accuracy: 0.7500\n",
      "Epoch 500/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0583 - accuracy: 0.9856 - val_loss: 2.6326 - val_accuracy: 0.7458\n",
      "Epoch 501/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0612 - accuracy: 0.9796 - val_loss: 2.4565 - val_accuracy: 0.7417\n",
      "Epoch 502/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0695 - accuracy: 0.9759 - val_loss: 2.2697 - val_accuracy: 0.7667\n",
      "Epoch 503/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 2.3382 - val_accuracy: 0.7750\n",
      "Epoch 504/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0658 - accuracy: 0.9782 - val_loss: 2.2636 - val_accuracy: 0.7583\n",
      "Epoch 505/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0802 - accuracy: 0.9745 - val_loss: 2.3882 - val_accuracy: 0.7583\n",
      "Epoch 506/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0615 - accuracy: 0.9810 - val_loss: 2.2567 - val_accuracy: 0.7792\n",
      "Epoch 507/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0677 - accuracy: 0.9782 - val_loss: 2.2432 - val_accuracy: 0.7667\n",
      "Epoch 508/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0498 - accuracy: 0.9852 - val_loss: 2.3698 - val_accuracy: 0.7708\n",
      "Epoch 509/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0509 - accuracy: 0.9819 - val_loss: 2.4018 - val_accuracy: 0.7708\n",
      "Epoch 510/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0405 - accuracy: 0.9884 - val_loss: 2.5293 - val_accuracy: 0.7667\n",
      "Epoch 511/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 2.5443 - val_accuracy: 0.7833\n",
      "Epoch 512/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0577 - accuracy: 0.9810 - val_loss: 2.5411 - val_accuracy: 0.7833\n",
      "Epoch 513/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0643 - accuracy: 0.9782 - val_loss: 2.4509 - val_accuracy: 0.7708\n",
      "Epoch 514/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0522 - accuracy: 0.9838 - val_loss: 2.4789 - val_accuracy: 0.7875\n",
      "Epoch 515/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0532 - accuracy: 0.9819 - val_loss: 2.5880 - val_accuracy: 0.7750\n",
      "Epoch 516/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1169 - accuracy: 0.9657 - val_loss: 2.4870 - val_accuracy: 0.7417\n",
      "Epoch 517/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0914 - accuracy: 0.9791 - val_loss: 2.3856 - val_accuracy: 0.7625\n",
      "Epoch 518/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0654 - accuracy: 0.9736 - val_loss: 2.3959 - val_accuracy: 0.7625\n",
      "Epoch 519/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0844 - accuracy: 0.9773 - val_loss: 2.3387 - val_accuracy: 0.7750\n",
      "Epoch 520/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0591 - accuracy: 0.9819 - val_loss: 2.4408 - val_accuracy: 0.7667\n",
      "Epoch 521/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0631 - accuracy: 0.9810 - val_loss: 2.3401 - val_accuracy: 0.7792\n",
      "Epoch 522/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0638 - accuracy: 0.9805 - val_loss: 2.4885 - val_accuracy: 0.7500\n",
      "Epoch 523/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0690 - accuracy: 0.9759 - val_loss: 2.4265 - val_accuracy: 0.7542\n",
      "Epoch 524/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0544 - accuracy: 0.9824 - val_loss: 2.3830 - val_accuracy: 0.7583\n",
      "Epoch 525/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0997 - accuracy: 0.9768 - val_loss: 2.0677 - val_accuracy: 0.7708\n",
      "Epoch 526/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0516 - accuracy: 0.9838 - val_loss: 2.3143 - val_accuracy: 0.7708\n",
      "Epoch 527/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 2.5300 - val_accuracy: 0.7458\n",
      "Epoch 528/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 2.4407 - val_accuracy: 0.7583\n",
      "Epoch 529/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0583 - accuracy: 0.9801 - val_loss: 2.4269 - val_accuracy: 0.7542\n",
      "Epoch 530/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0528 - accuracy: 0.9838 - val_loss: 2.4146 - val_accuracy: 0.7542\n",
      "Epoch 531/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0685 - accuracy: 0.9815 - val_loss: 2.3637 - val_accuracy: 0.7792\n",
      "Epoch 532/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0949 - accuracy: 0.9699 - val_loss: 2.1737 - val_accuracy: 0.7625\n",
      "Epoch 533/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0686 - accuracy: 0.9759 - val_loss: 2.2245 - val_accuracy: 0.7833\n",
      "Epoch 534/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0583 - accuracy: 0.9810 - val_loss: 2.4016 - val_accuracy: 0.7667\n",
      "Epoch 535/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0614 - accuracy: 0.9801 - val_loss: 2.1178 - val_accuracy: 0.7542\n",
      "Epoch 536/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0652 - accuracy: 0.9782 - val_loss: 2.1914 - val_accuracy: 0.7625\n",
      "Epoch 537/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 2.3677 - val_accuracy: 0.7708\n",
      "Epoch 538/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0579 - accuracy: 0.9852 - val_loss: 2.2271 - val_accuracy: 0.7667\n",
      "Epoch 539/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0438 - accuracy: 0.9856 - val_loss: 2.2564 - val_accuracy: 0.7750\n",
      "Epoch 540/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0465 - accuracy: 0.9861 - val_loss: 2.2684 - val_accuracy: 0.7833\n",
      "Epoch 541/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0513 - accuracy: 0.9833 - val_loss: 2.4641 - val_accuracy: 0.7625\n",
      "Epoch 542/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0662 - accuracy: 0.9801 - val_loss: 2.4472 - val_accuracy: 0.7500\n",
      "Epoch 543/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0382 - accuracy: 0.9893 - val_loss: 2.4680 - val_accuracy: 0.7667\n",
      "Epoch 544/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0556 - accuracy: 0.9815 - val_loss: 2.3640 - val_accuracy: 0.8042\n",
      "Epoch 545/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0466 - accuracy: 0.9847 - val_loss: 2.4283 - val_accuracy: 0.7708\n",
      "Epoch 546/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0512 - accuracy: 0.9842 - val_loss: 2.5033 - val_accuracy: 0.7542\n",
      "Epoch 547/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0521 - accuracy: 0.9847 - val_loss: 2.2731 - val_accuracy: 0.7458\n",
      "Epoch 548/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0445 - accuracy: 0.9852 - val_loss: 2.3291 - val_accuracy: 0.7917\n",
      "Epoch 549/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0445 - accuracy: 0.9829 - val_loss: 2.6372 - val_accuracy: 0.7667\n",
      "Epoch 550/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0606 - accuracy: 0.9791 - val_loss: 2.6893 - val_accuracy: 0.7583\n",
      "Epoch 551/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0759 - accuracy: 0.9731 - val_loss: 2.4517 - val_accuracy: 0.7583\n",
      "Epoch 552/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0383 - accuracy: 0.9898 - val_loss: 2.5717 - val_accuracy: 0.7625\n",
      "Epoch 553/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 2.3896 - val_accuracy: 0.7833\n",
      "Epoch 554/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0629 - accuracy: 0.9768 - val_loss: 2.5889 - val_accuracy: 0.7458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 555/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0819 - accuracy: 0.9699 - val_loss: 2.7475 - val_accuracy: 0.7250\n",
      "Epoch 556/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1325 - accuracy: 0.9703 - val_loss: 2.1641 - val_accuracy: 0.7667\n",
      "Epoch 557/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0634 - accuracy: 0.9778 - val_loss: 2.4310 - val_accuracy: 0.7458\n",
      "Epoch 558/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0463 - accuracy: 0.9842 - val_loss: 2.4717 - val_accuracy: 0.7500\n",
      "Epoch 559/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0724 - accuracy: 0.9815 - val_loss: 2.4706 - val_accuracy: 0.7333\n",
      "Epoch 560/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0562 - accuracy: 0.9815 - val_loss: 2.4597 - val_accuracy: 0.7500\n",
      "Epoch 561/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0468 - accuracy: 0.9852 - val_loss: 2.5355 - val_accuracy: 0.7417\n",
      "Epoch 562/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0474 - accuracy: 0.9838 - val_loss: 2.4900 - val_accuracy: 0.7458\n",
      "Epoch 563/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0553 - accuracy: 0.9810 - val_loss: 2.5738 - val_accuracy: 0.7500\n",
      "Epoch 564/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0509 - accuracy: 0.9838 - val_loss: 2.4838 - val_accuracy: 0.7542\n",
      "Epoch 565/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0484 - accuracy: 0.9842 - val_loss: 2.5118 - val_accuracy: 0.7583\n",
      "Epoch 566/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0379 - accuracy: 0.9898 - val_loss: 2.5828 - val_accuracy: 0.7625\n",
      "Epoch 567/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0490 - accuracy: 0.9856 - val_loss: 2.5299 - val_accuracy: 0.7542\n",
      "Epoch 568/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9838 - val_loss: 2.5172 - val_accuracy: 0.7875\n",
      "Epoch 569/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0495 - accuracy: 0.9829 - val_loss: 2.4390 - val_accuracy: 0.7625\n",
      "Epoch 570/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0439 - accuracy: 0.9856 - val_loss: 2.5182 - val_accuracy: 0.7708\n",
      "Epoch 571/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0521 - accuracy: 0.9833 - val_loss: 2.5001 - val_accuracy: 0.7708\n",
      "Epoch 572/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0446 - accuracy: 0.9870 - val_loss: 2.4874 - val_accuracy: 0.7542\n",
      "Epoch 573/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0422 - accuracy: 0.9875 - val_loss: 2.5465 - val_accuracy: 0.7583\n",
      "Epoch 574/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0467 - accuracy: 0.9833 - val_loss: 2.6784 - val_accuracy: 0.7542\n",
      "Epoch 575/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0509 - accuracy: 0.9852 - val_loss: 2.7249 - val_accuracy: 0.7667\n",
      "Epoch 576/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0681 - accuracy: 0.9805 - val_loss: 2.4988 - val_accuracy: 0.7667\n",
      "Epoch 577/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0691 - accuracy: 0.9764 - val_loss: 2.4736 - val_accuracy: 0.7750\n",
      "Epoch 578/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0784 - accuracy: 0.9773 - val_loss: 2.5780 - val_accuracy: 0.7667\n",
      "Epoch 579/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0434 - accuracy: 0.9880 - val_loss: 2.6063 - val_accuracy: 0.7500\n",
      "Epoch 580/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0866 - accuracy: 0.9773 - val_loss: 2.5966 - val_accuracy: 0.7542\n",
      "Epoch 581/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0542 - accuracy: 0.9842 - val_loss: 2.4639 - val_accuracy: 0.7750\n",
      "Epoch 582/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0532 - accuracy: 0.9829 - val_loss: 2.7368 - val_accuracy: 0.7375\n",
      "Epoch 583/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0546 - accuracy: 0.9852 - val_loss: 2.6450 - val_accuracy: 0.7542\n",
      "Epoch 584/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 2.7194 - val_accuracy: 0.7583\n",
      "Epoch 585/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0427 - accuracy: 0.9898 - val_loss: 2.5829 - val_accuracy: 0.7500\n",
      "Epoch 586/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0494 - accuracy: 0.9842 - val_loss: 2.3583 - val_accuracy: 0.7625\n",
      "Epoch 587/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9829 - val_loss: 2.5397 - val_accuracy: 0.7750\n",
      "Epoch 588/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0719 - accuracy: 0.9796 - val_loss: 2.4261 - val_accuracy: 0.7792\n",
      "Epoch 589/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0545 - accuracy: 0.9866 - val_loss: 2.2671 - val_accuracy: 0.7917\n",
      "Epoch 590/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0452 - accuracy: 0.9875 - val_loss: 2.7377 - val_accuracy: 0.7500\n",
      "Epoch 591/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0603 - accuracy: 0.9796 - val_loss: 2.7115 - val_accuracy: 0.7792\n",
      "Epoch 592/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0799 - accuracy: 0.9768 - val_loss: 2.5001 - val_accuracy: 0.7875\n",
      "Epoch 593/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0655 - accuracy: 0.9787 - val_loss: 2.4306 - val_accuracy: 0.7833\n",
      "Epoch 594/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0453 - accuracy: 0.9875 - val_loss: 2.4495 - val_accuracy: 0.7875\n",
      "Epoch 595/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0559 - accuracy: 0.9819 - val_loss: 2.4547 - val_accuracy: 0.7750\n",
      "Epoch 596/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0496 - accuracy: 0.9842 - val_loss: 2.4636 - val_accuracy: 0.7792\n",
      "Epoch 597/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0583 - accuracy: 0.9815 - val_loss: 2.3685 - val_accuracy: 0.7458\n",
      "Epoch 598/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 2.5591 - val_accuracy: 0.7708\n",
      "Epoch 599/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0341 - accuracy: 0.9893 - val_loss: 2.8784 - val_accuracy: 0.7208\n",
      "Epoch 600/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0712 - accuracy: 0.9778 - val_loss: 2.5959 - val_accuracy: 0.7708\n",
      "Epoch 601/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 2.3251 - val_accuracy: 0.7583\n",
      "Epoch 602/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0575 - accuracy: 0.9787 - val_loss: 2.7055 - val_accuracy: 0.7875\n",
      "Epoch 603/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0807 - accuracy: 0.9778 - val_loss: 2.7054 - val_accuracy: 0.7458\n",
      "Epoch 604/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0657 - accuracy: 0.9796 - val_loss: 2.6208 - val_accuracy: 0.7417\n",
      "Epoch 605/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0715 - accuracy: 0.9768 - val_loss: 2.7376 - val_accuracy: 0.7750\n",
      "Epoch 606/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0555 - accuracy: 0.9852 - val_loss: 2.6415 - val_accuracy: 0.7833\n",
      "Epoch 607/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0464 - accuracy: 0.9824 - val_loss: 2.6192 - val_accuracy: 0.7750\n",
      "Epoch 608/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0517 - accuracy: 0.9824 - val_loss: 2.6506 - val_accuracy: 0.7750\n",
      "Epoch 609/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0377 - accuracy: 0.9866 - val_loss: 2.8524 - val_accuracy: 0.7708\n",
      "Epoch 610/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0480 - accuracy: 0.9842 - val_loss: 2.4703 - val_accuracy: 0.7750\n",
      "Epoch 611/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0628 - accuracy: 0.9805 - val_loss: 2.5718 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 612/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0661 - accuracy: 0.9824 - val_loss: 2.4763 - val_accuracy: 0.7583\n",
      "Epoch 613/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 2.5661 - val_accuracy: 0.7708\n",
      "Epoch 614/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0640 - accuracy: 0.9787 - val_loss: 2.4432 - val_accuracy: 0.7625\n",
      "Epoch 615/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0446 - accuracy: 0.9903 - val_loss: 2.3439 - val_accuracy: 0.7917\n",
      "Epoch 616/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0393 - accuracy: 0.9893 - val_loss: 2.5861 - val_accuracy: 0.7750\n",
      "Epoch 617/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0891 - accuracy: 0.9722 - val_loss: 2.1825 - val_accuracy: 0.7917\n",
      "Epoch 618/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0549 - accuracy: 0.9889 - val_loss: 2.1924 - val_accuracy: 0.7917\n",
      "Epoch 619/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0682 - accuracy: 0.9791 - val_loss: 2.1679 - val_accuracy: 0.7625\n",
      "Epoch 620/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0777 - accuracy: 0.9787 - val_loss: 2.1350 - val_accuracy: 0.8083\n",
      "Epoch 621/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0446 - accuracy: 0.9875 - val_loss: 2.3223 - val_accuracy: 0.7792\n",
      "Epoch 622/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0532 - accuracy: 0.9847 - val_loss: 2.1165 - val_accuracy: 0.7958\n",
      "Epoch 623/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0360 - accuracy: 0.9903 - val_loss: 2.2630 - val_accuracy: 0.7958\n",
      "Epoch 624/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0788 - accuracy: 0.9810 - val_loss: 2.0334 - val_accuracy: 0.7875\n",
      "Epoch 625/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0622 - accuracy: 0.9801 - val_loss: 2.2232 - val_accuracy: 0.7792\n",
      "Epoch 626/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 2.4154 - val_accuracy: 0.7667\n",
      "Epoch 627/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 2.2067 - val_accuracy: 0.7875\n",
      "Epoch 628/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0462 - accuracy: 0.9870 - val_loss: 2.3201 - val_accuracy: 0.7833\n",
      "Epoch 629/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0399 - accuracy: 0.9861 - val_loss: 2.5279 - val_accuracy: 0.7792\n",
      "Epoch 630/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0534 - accuracy: 0.9810 - val_loss: 2.2772 - val_accuracy: 0.8000\n",
      "Epoch 631/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0388 - accuracy: 0.9880 - val_loss: 2.3241 - val_accuracy: 0.7750\n",
      "Epoch 632/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0754 - accuracy: 0.9787 - val_loss: 2.3238 - val_accuracy: 0.7500\n",
      "Epoch 633/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0444 - accuracy: 0.9856 - val_loss: 2.3344 - val_accuracy: 0.7792\n",
      "Epoch 634/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0438 - accuracy: 0.9842 - val_loss: 2.4390 - val_accuracy: 0.7750\n",
      "Epoch 635/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0380 - accuracy: 0.9875 - val_loss: 2.5540 - val_accuracy: 0.7542\n",
      "Epoch 636/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0469 - accuracy: 0.9824 - val_loss: 2.6168 - val_accuracy: 0.7750\n",
      "Epoch 637/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0557 - accuracy: 0.9829 - val_loss: 2.2856 - val_accuracy: 0.7833\n",
      "Epoch 638/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0454 - accuracy: 0.9842 - val_loss: 2.6768 - val_accuracy: 0.7542\n",
      "Epoch 639/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0518 - accuracy: 0.9847 - val_loss: 2.3112 - val_accuracy: 0.7917\n",
      "Epoch 640/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0663 - accuracy: 0.9810 - val_loss: 2.3746 - val_accuracy: 0.7583\n",
      "Epoch 641/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0528 - accuracy: 0.9829 - val_loss: 2.2493 - val_accuracy: 0.7833\n",
      "Epoch 642/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0960 - accuracy: 0.9778 - val_loss: 2.2126 - val_accuracy: 0.7667\n",
      "Epoch 643/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0517 - accuracy: 0.9847 - val_loss: 2.2726 - val_accuracy: 0.7792\n",
      "Epoch 644/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0643 - accuracy: 0.9824 - val_loss: 2.3723 - val_accuracy: 0.7750\n",
      "Epoch 645/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0387 - accuracy: 0.9861 - val_loss: 2.4728 - val_accuracy: 0.7792\n",
      "Epoch 646/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0364 - accuracy: 0.9889 - val_loss: 2.5948 - val_accuracy: 0.7583\n",
      "Epoch 647/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0461 - accuracy: 0.9880 - val_loss: 2.7303 - val_accuracy: 0.7708\n",
      "Epoch 648/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 2.6999 - val_accuracy: 0.7625\n",
      "Epoch 649/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0437 - accuracy: 0.9880 - val_loss: 2.4825 - val_accuracy: 0.7583\n",
      "Epoch 650/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0317 - accuracy: 0.9893 - val_loss: 2.6291 - val_accuracy: 0.7708\n",
      "Epoch 651/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0516 - accuracy: 0.9847 - val_loss: 2.6697 - val_accuracy: 0.7625\n",
      "Epoch 652/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0490 - accuracy: 0.9842 - val_loss: 2.6224 - val_accuracy: 0.7792\n",
      "Epoch 653/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 2.6281 - val_accuracy: 0.7792\n",
      "Epoch 654/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0592 - accuracy: 0.9824 - val_loss: 2.4886 - val_accuracy: 0.7667\n",
      "Epoch 655/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0381 - accuracy: 0.9884 - val_loss: 2.6796 - val_accuracy: 0.7833\n",
      "Epoch 656/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0412 - accuracy: 0.9833 - val_loss: 2.7097 - val_accuracy: 0.7708\n",
      "Epoch 657/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0434 - accuracy: 0.9861 - val_loss: 2.7503 - val_accuracy: 0.7708\n",
      "Epoch 658/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0346 - accuracy: 0.9884 - val_loss: 2.8039 - val_accuracy: 0.7750\n",
      "Epoch 659/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 3.8922 - val_accuracy: 0.6958\n",
      "Epoch 660/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1370 - accuracy: 0.9685 - val_loss: 2.1825 - val_accuracy: 0.7792\n",
      "Epoch 661/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0720 - accuracy: 0.9791 - val_loss: 2.1822 - val_accuracy: 0.7667\n",
      "Epoch 662/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0449 - accuracy: 0.9866 - val_loss: 2.2269 - val_accuracy: 0.7750\n",
      "Epoch 663/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0564 - accuracy: 0.9815 - val_loss: 2.2703 - val_accuracy: 0.7833\n",
      "Epoch 664/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0389 - accuracy: 0.9875 - val_loss: 2.3049 - val_accuracy: 0.7917\n",
      "Epoch 665/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0370 - accuracy: 0.9875 - val_loss: 2.2700 - val_accuracy: 0.7958\n",
      "Epoch 666/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0403 - accuracy: 0.9847 - val_loss: 2.4957 - val_accuracy: 0.7667\n",
      "Epoch 667/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0519 - accuracy: 0.9838 - val_loss: 2.4238 - val_accuracy: 0.7833\n",
      "Epoch 668/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0450 - accuracy: 0.9852 - val_loss: 2.4633 - val_accuracy: 0.7875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 669/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0409 - accuracy: 0.9847 - val_loss: 2.4309 - val_accuracy: 0.7833\n",
      "Epoch 670/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0375 - accuracy: 0.9875 - val_loss: 2.4222 - val_accuracy: 0.7833\n",
      "Epoch 671/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0418 - accuracy: 0.9861 - val_loss: 2.3476 - val_accuracy: 0.7917\n",
      "Epoch 672/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0365 - accuracy: 0.9870 - val_loss: 2.4861 - val_accuracy: 0.7833\n",
      "Epoch 673/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0497 - accuracy: 0.9833 - val_loss: 2.6359 - val_accuracy: 0.7708\n",
      "Epoch 674/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0367 - accuracy: 0.9884 - val_loss: 2.5363 - val_accuracy: 0.7875\n",
      "Epoch 675/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0430 - accuracy: 0.9861 - val_loss: 2.5306 - val_accuracy: 0.7875\n",
      "Epoch 676/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9833 - val_loss: 2.4103 - val_accuracy: 0.7708\n",
      "Epoch 677/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0445 - accuracy: 0.9829 - val_loss: 2.3920 - val_accuracy: 0.7875\n",
      "Epoch 678/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0307 - accuracy: 0.9921 - val_loss: 2.4771 - val_accuracy: 0.8000\n",
      "Epoch 679/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0446 - accuracy: 0.9884 - val_loss: 2.4699 - val_accuracy: 0.7667\n",
      "Epoch 680/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0689 - accuracy: 0.9787 - val_loss: 2.3478 - val_accuracy: 0.7917\n",
      "Epoch 681/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1067 - accuracy: 0.9750 - val_loss: 2.2983 - val_accuracy: 0.7875\n",
      "Epoch 682/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0447 - accuracy: 0.9847 - val_loss: 2.3566 - val_accuracy: 0.7708\n",
      "Epoch 683/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0581 - accuracy: 0.9819 - val_loss: 2.3095 - val_accuracy: 0.7625\n",
      "Epoch 684/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0466 - accuracy: 0.9866 - val_loss: 2.4041 - val_accuracy: 0.7708\n",
      "Epoch 685/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0495 - accuracy: 0.9866 - val_loss: 2.3973 - val_accuracy: 0.7750\n",
      "Epoch 686/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0438 - accuracy: 0.9875 - val_loss: 2.3531 - val_accuracy: 0.7667\n",
      "Epoch 687/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0413 - accuracy: 0.9847 - val_loss: 2.3885 - val_accuracy: 0.7958\n",
      "Epoch 688/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0317 - accuracy: 0.9912 - val_loss: 2.4830 - val_accuracy: 0.7792\n",
      "Epoch 689/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0418 - accuracy: 0.9861 - val_loss: 2.3744 - val_accuracy: 0.7875\n",
      "Epoch 690/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0419 - accuracy: 0.9893 - val_loss: 2.6328 - val_accuracy: 0.7708\n",
      "Epoch 691/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0444 - accuracy: 0.9861 - val_loss: 2.5603 - val_accuracy: 0.7833\n",
      "Epoch 692/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0311 - accuracy: 0.9898 - val_loss: 2.6986 - val_accuracy: 0.7750\n",
      "Epoch 693/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0401 - accuracy: 0.9861 - val_loss: 2.6087 - val_accuracy: 0.7792\n",
      "Epoch 694/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0496 - accuracy: 0.9856 - val_loss: 2.5510 - val_accuracy: 0.7375\n",
      "Epoch 695/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0802 - accuracy: 0.9764 - val_loss: 2.4657 - val_accuracy: 0.7667\n",
      "Epoch 696/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0469 - accuracy: 0.9861 - val_loss: 2.4374 - val_accuracy: 0.7875\n",
      "Epoch 697/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0470 - accuracy: 0.9856 - val_loss: 2.7824 - val_accuracy: 0.7625\n",
      "Epoch 698/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0665 - accuracy: 0.9842 - val_loss: 2.4241 - val_accuracy: 0.7708\n",
      "Epoch 699/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0407 - accuracy: 0.9880 - val_loss: 2.4212 - val_accuracy: 0.7708\n",
      "Epoch 700/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 2.4644 - val_accuracy: 0.7958\n",
      "Epoch 701/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0342 - accuracy: 0.9917 - val_loss: 2.5696 - val_accuracy: 0.7583\n",
      "Epoch 702/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0519 - accuracy: 0.9815 - val_loss: 2.4118 - val_accuracy: 0.7750\n",
      "Epoch 703/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 2.4480 - val_accuracy: 0.7792\n",
      "Epoch 704/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0378 - accuracy: 0.9903 - val_loss: 2.6231 - val_accuracy: 0.7708\n",
      "Epoch 705/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0302 - accuracy: 0.9912 - val_loss: 2.6480 - val_accuracy: 0.7875\n",
      "Epoch 706/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0386 - accuracy: 0.9861 - val_loss: 2.5760 - val_accuracy: 0.7917\n",
      "Epoch 707/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0298 - accuracy: 0.9889 - val_loss: 2.6718 - val_accuracy: 0.7708\n",
      "Epoch 708/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0224 - accuracy: 0.9944 - val_loss: 2.7166 - val_accuracy: 0.7833\n",
      "Epoch 709/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0440 - accuracy: 0.9852 - val_loss: 2.6503 - val_accuracy: 0.7792\n",
      "Epoch 710/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0471 - accuracy: 0.9815 - val_loss: 2.5860 - val_accuracy: 0.7958\n",
      "Epoch 711/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 2.6961 - val_accuracy: 0.7792\n",
      "Epoch 712/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0314 - accuracy: 0.9889 - val_loss: 2.6893 - val_accuracy: 0.7833\n",
      "Epoch 713/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0298 - accuracy: 0.9903 - val_loss: 2.7491 - val_accuracy: 0.7750\n",
      "Epoch 714/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0402 - accuracy: 0.9880 - val_loss: 2.6911 - val_accuracy: 0.8000\n",
      "Epoch 715/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0307 - accuracy: 0.9907 - val_loss: 2.6209 - val_accuracy: 0.8000\n",
      "Epoch 716/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9930 - val_loss: 2.9450 - val_accuracy: 0.7625\n",
      "Epoch 717/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0369 - accuracy: 0.9898 - val_loss: 2.7150 - val_accuracy: 0.7750\n",
      "Epoch 718/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0453 - accuracy: 0.9866 - val_loss: 2.5888 - val_accuracy: 0.7792\n",
      "Epoch 719/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0770 - accuracy: 0.9805 - val_loss: 2.5318 - val_accuracy: 0.7917\n",
      "Epoch 720/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0505 - accuracy: 0.9870 - val_loss: 2.3510 - val_accuracy: 0.7708\n",
      "Epoch 721/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0524 - accuracy: 0.9838 - val_loss: 2.5036 - val_accuracy: 0.7625\n",
      "Epoch 722/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0503 - accuracy: 0.9805 - val_loss: 2.5432 - val_accuracy: 0.7833\n",
      "Epoch 723/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0435 - accuracy: 0.9842 - val_loss: 2.5338 - val_accuracy: 0.7875\n",
      "Epoch 724/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0355 - accuracy: 0.9893 - val_loss: 2.4879 - val_accuracy: 0.8042\n",
      "Epoch 725/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0364 - accuracy: 0.9880 - val_loss: 2.5416 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 726/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0347 - accuracy: 0.9889 - val_loss: 2.6127 - val_accuracy: 0.7833\n",
      "Epoch 727/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0517 - accuracy: 0.9870 - val_loss: 2.4828 - val_accuracy: 0.7875\n",
      "Epoch 728/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0501 - accuracy: 0.9861 - val_loss: 2.4806 - val_accuracy: 0.7833\n",
      "Epoch 729/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0290 - accuracy: 0.9898 - val_loss: 2.6039 - val_accuracy: 0.7833\n",
      "Epoch 730/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0250 - accuracy: 0.9930 - val_loss: 2.6336 - val_accuracy: 0.7833\n",
      "Epoch 731/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0391 - accuracy: 0.9852 - val_loss: 2.7115 - val_accuracy: 0.7792\n",
      "Epoch 732/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0506 - accuracy: 0.9870 - val_loss: 2.5110 - val_accuracy: 0.7708\n",
      "Epoch 733/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0907 - accuracy: 0.9731 - val_loss: 2.2128 - val_accuracy: 0.7750\n",
      "Epoch 734/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0440 - accuracy: 0.9880 - val_loss: 2.2556 - val_accuracy: 0.7625\n",
      "Epoch 735/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0410 - accuracy: 0.9870 - val_loss: 2.3579 - val_accuracy: 0.7625\n",
      "Epoch 736/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0288 - accuracy: 0.9898 - val_loss: 2.4018 - val_accuracy: 0.7583\n",
      "Epoch 737/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0568 - accuracy: 0.9852 - val_loss: 2.2812 - val_accuracy: 0.7667\n",
      "Epoch 738/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0531 - accuracy: 0.9852 - val_loss: 2.3047 - val_accuracy: 0.7875\n",
      "Epoch 739/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0342 - accuracy: 0.9907 - val_loss: 2.4340 - val_accuracy: 0.7625\n",
      "Epoch 740/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0578 - accuracy: 0.9838 - val_loss: 2.3137 - val_accuracy: 0.7708\n",
      "Epoch 741/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0583 - accuracy: 0.9852 - val_loss: 2.3796 - val_accuracy: 0.7917\n",
      "Epoch 742/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0423 - accuracy: 0.9875 - val_loss: 2.1599 - val_accuracy: 0.7875\n",
      "Epoch 743/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 2.2740 - val_accuracy: 0.7875\n",
      "Epoch 744/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0506 - accuracy: 0.9866 - val_loss: 2.3621 - val_accuracy: 0.7833\n",
      "Epoch 745/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0320 - accuracy: 0.9889 - val_loss: 2.3394 - val_accuracy: 0.7708\n",
      "Epoch 746/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0354 - accuracy: 0.9856 - val_loss: 2.4858 - val_accuracy: 0.7708\n",
      "Epoch 747/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0525 - accuracy: 0.9810 - val_loss: 2.4357 - val_accuracy: 0.7792\n",
      "Epoch 748/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0602 - accuracy: 0.9856 - val_loss: 2.3475 - val_accuracy: 0.7625\n",
      "Epoch 749/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0395 - accuracy: 0.9870 - val_loss: 2.3273 - val_accuracy: 0.7875\n",
      "Epoch 750/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9829 - val_loss: 2.4769 - val_accuracy: 0.7625\n",
      "Epoch 751/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0653 - accuracy: 0.9852 - val_loss: 2.5419 - val_accuracy: 0.7375\n",
      "Epoch 752/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0611 - accuracy: 0.9782 - val_loss: 2.2440 - val_accuracy: 0.7750\n",
      "Epoch 753/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0695 - accuracy: 0.9824 - val_loss: 2.0958 - val_accuracy: 0.7833\n",
      "Epoch 754/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1034 - accuracy: 0.9801 - val_loss: 2.3042 - val_accuracy: 0.7667\n",
      "Epoch 755/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0863 - accuracy: 0.9759 - val_loss: 2.0895 - val_accuracy: 0.7833\n",
      "Epoch 756/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0479 - accuracy: 0.9866 - val_loss: 2.3309 - val_accuracy: 0.7667\n",
      "Epoch 757/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0527 - accuracy: 0.9856 - val_loss: 2.1869 - val_accuracy: 0.8083\n",
      "Epoch 758/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0500 - accuracy: 0.9833 - val_loss: 2.1864 - val_accuracy: 0.7917\n",
      "Epoch 759/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0447 - accuracy: 0.9880 - val_loss: 1.9984 - val_accuracy: 0.8000\n",
      "Epoch 760/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0429 - accuracy: 0.9875 - val_loss: 2.1891 - val_accuracy: 0.8000\n",
      "Epoch 761/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0383 - accuracy: 0.9866 - val_loss: 2.3386 - val_accuracy: 0.7917\n",
      "Epoch 762/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0348 - accuracy: 0.9903 - val_loss: 2.3229 - val_accuracy: 0.7875\n",
      "Epoch 763/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0641 - accuracy: 0.9847 - val_loss: 2.4652 - val_accuracy: 0.7708\n",
      "Epoch 764/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0563 - accuracy: 0.9838 - val_loss: 2.3374 - val_accuracy: 0.7750\n",
      "Epoch 765/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0458 - accuracy: 0.9852 - val_loss: 2.4750 - val_accuracy: 0.7833\n",
      "Epoch 766/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0386 - accuracy: 0.9861 - val_loss: 2.3266 - val_accuracy: 0.7875\n",
      "Epoch 767/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0365 - accuracy: 0.9889 - val_loss: 2.4163 - val_accuracy: 0.7708\n",
      "Epoch 768/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9819 - val_loss: 2.1981 - val_accuracy: 0.7833\n",
      "Epoch 769/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0603 - accuracy: 0.9852 - val_loss: 2.3103 - val_accuracy: 0.7875\n",
      "Epoch 770/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0457 - accuracy: 0.9884 - val_loss: 2.1601 - val_accuracy: 0.8000\n",
      "Epoch 771/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0573 - accuracy: 0.9838 - val_loss: 2.3974 - val_accuracy: 0.7583\n",
      "Epoch 772/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0656 - accuracy: 0.9847 - val_loss: 2.2848 - val_accuracy: 0.7750\n",
      "Epoch 773/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0363 - accuracy: 0.9898 - val_loss: 2.2777 - val_accuracy: 0.7875\n",
      "Epoch 774/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0374 - accuracy: 0.9861 - val_loss: 2.3829 - val_accuracy: 0.7792\n",
      "Epoch 775/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0358 - accuracy: 0.9889 - val_loss: 2.4807 - val_accuracy: 0.7875\n",
      "Epoch 776/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0556 - accuracy: 0.9810 - val_loss: 2.4140 - val_accuracy: 0.7875\n",
      "Epoch 777/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0478 - accuracy: 0.9866 - val_loss: 2.3274 - val_accuracy: 0.7875\n",
      "Epoch 778/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0560 - accuracy: 0.9829 - val_loss: 2.2725 - val_accuracy: 0.7958\n",
      "Epoch 779/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0387 - accuracy: 0.9870 - val_loss: 2.5951 - val_accuracy: 0.7625\n",
      "Epoch 780/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0701 - accuracy: 0.9810 - val_loss: 2.3887 - val_accuracy: 0.7708\n",
      "Epoch 781/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0568 - accuracy: 0.9856 - val_loss: 2.3100 - val_accuracy: 0.7750\n",
      "Epoch 782/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0338 - accuracy: 0.9893 - val_loss: 2.3317 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 783/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0444 - accuracy: 0.9866 - val_loss: 2.2999 - val_accuracy: 0.7792\n",
      "Epoch 784/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0482 - accuracy: 0.9884 - val_loss: 2.2922 - val_accuracy: 0.7667\n",
      "Epoch 785/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0511 - accuracy: 0.9829 - val_loss: 2.4080 - val_accuracy: 0.7875\n",
      "Epoch 786/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0462 - accuracy: 0.9870 - val_loss: 2.4219 - val_accuracy: 0.7667\n",
      "Epoch 787/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0454 - accuracy: 0.9852 - val_loss: 2.4317 - val_accuracy: 0.7625\n",
      "Epoch 788/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0418 - accuracy: 0.9833 - val_loss: 2.3716 - val_accuracy: 0.7708\n",
      "Epoch 789/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0344 - accuracy: 0.9884 - val_loss: 2.7423 - val_accuracy: 0.7458\n",
      "Epoch 790/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0470 - accuracy: 0.9847 - val_loss: 2.6347 - val_accuracy: 0.7500\n",
      "Epoch 791/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0382 - accuracy: 0.9889 - val_loss: 2.4346 - val_accuracy: 0.7750\n",
      "Epoch 792/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0504 - accuracy: 0.9889 - val_loss: 2.4715 - val_accuracy: 0.7750\n",
      "Epoch 793/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0754 - accuracy: 0.9778 - val_loss: 2.2584 - val_accuracy: 0.7625\n",
      "Epoch 794/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0614 - accuracy: 0.9829 - val_loss: 2.1155 - val_accuracy: 0.7792\n",
      "Epoch 795/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0386 - accuracy: 0.9893 - val_loss: 2.4445 - val_accuracy: 0.7625\n",
      "Epoch 796/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0374 - accuracy: 0.9866 - val_loss: 2.5062 - val_accuracy: 0.7750\n",
      "Epoch 797/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 2.5701 - val_accuracy: 0.7708\n",
      "Epoch 798/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0305 - accuracy: 0.9903 - val_loss: 2.5948 - val_accuracy: 0.7667\n",
      "Epoch 799/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.0402 - accuracy: 0.9893 - val_loss: 2.5898 - val_accuracy: 0.7833\n",
      "Epoch 800/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.0283 - accuracy: 0.9930 - val_loss: 2.5960 - val_accuracy: 0.7833\n"
     ]
    }
   ],
   "source": [
    "model_a = modelizer(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "107e4023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 1860, 32)          4128      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 462, 32)           16416     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 228, 64)           16448     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 226, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 112, 128)          32896     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 110, 256)          98560     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 26, 256)           524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6656)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               852096    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                1950      \n",
      "=================================================================\n",
      "Total params: 1,567,646\n",
      "Trainable params: 1,567,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/800\n",
      "68/68 [==============================] - 2s 13ms/step - loss: 3.4048 - accuracy: 0.0306 - val_loss: 3.3932 - val_accuracy: 0.0417\n",
      "Epoch 2/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 3.3874 - accuracy: 0.0361 - val_loss: 3.3788 - val_accuracy: 0.0458\n",
      "Epoch 3/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.3769 - accuracy: 0.0449 - val_loss: 3.3662 - val_accuracy: 0.0583\n",
      "Epoch 4/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.3540 - accuracy: 0.0477 - val_loss: 3.3392 - val_accuracy: 0.0750\n",
      "Epoch 5/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.3295 - accuracy: 0.0663 - val_loss: 3.3019 - val_accuracy: 0.0833\n",
      "Epoch 6/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.2905 - accuracy: 0.0695 - val_loss: 3.2558 - val_accuracy: 0.0750\n",
      "Epoch 7/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.2529 - accuracy: 0.0751 - val_loss: 3.1916 - val_accuracy: 0.0792\n",
      "Epoch 8/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.1903 - accuracy: 0.0820 - val_loss: 3.1038 - val_accuracy: 0.0542\n",
      "Epoch 9/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.1028 - accuracy: 0.0941 - val_loss: 3.0267 - val_accuracy: 0.1042\n",
      "Epoch 10/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.0653 - accuracy: 0.0978 - val_loss: 2.9307 - val_accuracy: 0.1250\n",
      "Epoch 11/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 3.0047 - accuracy: 0.0936 - val_loss: 2.8336 - val_accuracy: 0.1333\n",
      "Epoch 12/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.9339 - accuracy: 0.1043 - val_loss: 2.7647 - val_accuracy: 0.1125\n",
      "Epoch 13/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.8923 - accuracy: 0.1149 - val_loss: 2.7288 - val_accuracy: 0.1542\n",
      "Epoch 14/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.8333 - accuracy: 0.1186 - val_loss: 2.6756 - val_accuracy: 0.1542\n",
      "Epoch 15/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.7789 - accuracy: 0.1353 - val_loss: 2.6060 - val_accuracy: 0.1417\n",
      "Epoch 16/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.7188 - accuracy: 0.1316 - val_loss: 2.5922 - val_accuracy: 0.1417\n",
      "Epoch 17/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.6611 - accuracy: 0.1497 - val_loss: 2.5527 - val_accuracy: 0.1542\n",
      "Epoch 18/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.6190 - accuracy: 0.1571 - val_loss: 2.3901 - val_accuracy: 0.1917\n",
      "Epoch 19/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.5515 - accuracy: 0.1645 - val_loss: 2.5157 - val_accuracy: 0.1458\n",
      "Epoch 20/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.4992 - accuracy: 0.1552 - val_loss: 2.2960 - val_accuracy: 0.1917\n",
      "Epoch 21/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.4693 - accuracy: 0.1724 - val_loss: 2.4183 - val_accuracy: 0.1542\n",
      "Epoch 22/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.4275 - accuracy: 0.1738 - val_loss: 2.2281 - val_accuracy: 0.1917\n",
      "Epoch 23/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.4112 - accuracy: 0.1756 - val_loss: 2.2515 - val_accuracy: 0.1958\n",
      "Epoch 24/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.3205 - accuracy: 0.2002 - val_loss: 2.3859 - val_accuracy: 0.1750\n",
      "Epoch 25/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.3179 - accuracy: 0.1946 - val_loss: 2.1875 - val_accuracy: 0.2250\n",
      "Epoch 26/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.2721 - accuracy: 0.1937 - val_loss: 2.1230 - val_accuracy: 0.1750\n",
      "Epoch 27/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.2904 - accuracy: 0.1807 - val_loss: 2.1261 - val_accuracy: 0.1792\n",
      "Epoch 28/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.1882 - accuracy: 0.2067 - val_loss: 2.0525 - val_accuracy: 0.2375\n",
      "Epoch 29/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.2035 - accuracy: 0.2150 - val_loss: 2.0076 - val_accuracy: 0.2500\n",
      "Epoch 30/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.1234 - accuracy: 0.2257 - val_loss: 2.0328 - val_accuracy: 0.2250\n",
      "Epoch 31/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.1336 - accuracy: 0.2127 - val_loss: 1.9703 - val_accuracy: 0.2500\n",
      "Epoch 32/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.1003 - accuracy: 0.2289 - val_loss: 1.9406 - val_accuracy: 0.2375\n",
      "Epoch 33/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 2.0586 - accuracy: 0.2493 - val_loss: 1.8545 - val_accuracy: 0.2542\n",
      "Epoch 34/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.0544 - accuracy: 0.2377 - val_loss: 1.8445 - val_accuracy: 0.2750\n",
      "Epoch 35/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 2.0270 - accuracy: 0.2512 - val_loss: 1.9360 - val_accuracy: 0.2042\n",
      "Epoch 36/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.9924 - accuracy: 0.2442 - val_loss: 1.8990 - val_accuracy: 0.2333\n",
      "Epoch 37/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.9753 - accuracy: 0.2475 - val_loss: 1.9657 - val_accuracy: 0.2500\n",
      "Epoch 38/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.9427 - accuracy: 0.2614 - val_loss: 1.8274 - val_accuracy: 0.2458\n",
      "Epoch 39/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.9550 - accuracy: 0.2576 - val_loss: 1.7650 - val_accuracy: 0.3000\n",
      "Epoch 40/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.9085 - accuracy: 0.2734 - val_loss: 1.8800 - val_accuracy: 0.2417\n",
      "Epoch 41/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.8898 - accuracy: 0.2586 - val_loss: 1.7250 - val_accuracy: 0.3125\n",
      "Epoch 42/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.8350 - accuracy: 0.2943 - val_loss: 1.7231 - val_accuracy: 0.2792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.8541 - accuracy: 0.2887 - val_loss: 1.7454 - val_accuracy: 0.3042\n",
      "Epoch 44/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.8476 - accuracy: 0.2901 - val_loss: 1.9548 - val_accuracy: 0.2083\n",
      "Epoch 45/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.8024 - accuracy: 0.2915 - val_loss: 1.7162 - val_accuracy: 0.3292\n",
      "Epoch 46/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.7895 - accuracy: 0.3017 - val_loss: 1.9509 - val_accuracy: 0.2000\n",
      "Epoch 47/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.7865 - accuracy: 0.2984 - val_loss: 1.5754 - val_accuracy: 0.3500\n",
      "Epoch 48/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.7596 - accuracy: 0.3100 - val_loss: 1.6626 - val_accuracy: 0.3000\n",
      "Epoch 49/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.7271 - accuracy: 0.3230 - val_loss: 1.5817 - val_accuracy: 0.3625\n",
      "Epoch 50/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.7011 - accuracy: 0.3336 - val_loss: 1.6124 - val_accuracy: 0.3292\n",
      "Epoch 51/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.6817 - accuracy: 0.3105 - val_loss: 1.5617 - val_accuracy: 0.3333\n",
      "Epoch 52/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.6886 - accuracy: 0.3123 - val_loss: 1.5404 - val_accuracy: 0.3625\n",
      "Epoch 53/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.6863 - accuracy: 0.3244 - val_loss: 1.6432 - val_accuracy: 0.2958\n",
      "Epoch 54/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.6515 - accuracy: 0.3503 - val_loss: 1.6022 - val_accuracy: 0.3208\n",
      "Epoch 55/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.6462 - accuracy: 0.3457 - val_loss: 1.5257 - val_accuracy: 0.3875\n",
      "Epoch 56/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.6070 - accuracy: 0.3545 - val_loss: 1.6003 - val_accuracy: 0.3208\n",
      "Epoch 57/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 1.5968 - accuracy: 0.3610 - val_loss: 1.5334 - val_accuracy: 0.3542\n",
      "Epoch 58/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.6000 - accuracy: 0.3614 - val_loss: 1.4817 - val_accuracy: 0.4167\n",
      "Epoch 59/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5544 - accuracy: 0.3726 - val_loss: 1.7700 - val_accuracy: 0.2542\n",
      "Epoch 60/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.6099 - accuracy: 0.3508 - val_loss: 1.6328 - val_accuracy: 0.3458\n",
      "Epoch 61/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5414 - accuracy: 0.3647 - val_loss: 1.8107 - val_accuracy: 0.2250\n",
      "Epoch 62/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5268 - accuracy: 0.3781 - val_loss: 1.4414 - val_accuracy: 0.3583\n",
      "Epoch 63/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5231 - accuracy: 0.3661 - val_loss: 1.7693 - val_accuracy: 0.2917\n",
      "Epoch 64/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5319 - accuracy: 0.3633 - val_loss: 1.4237 - val_accuracy: 0.4208\n",
      "Epoch 65/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4884 - accuracy: 0.3865 - val_loss: 1.6563 - val_accuracy: 0.2542\n",
      "Epoch 66/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5105 - accuracy: 0.3823 - val_loss: 1.6076 - val_accuracy: 0.3292\n",
      "Epoch 67/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5042 - accuracy: 0.3735 - val_loss: 1.3845 - val_accuracy: 0.4000\n",
      "Epoch 68/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.5106 - accuracy: 0.3758 - val_loss: 1.4528 - val_accuracy: 0.3583\n",
      "Epoch 69/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4809 - accuracy: 0.4073 - val_loss: 1.5312 - val_accuracy: 0.3583\n",
      "Epoch 70/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4498 - accuracy: 0.3976 - val_loss: 1.4755 - val_accuracy: 0.3667\n",
      "Epoch 71/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4379 - accuracy: 0.4087 - val_loss: 1.3511 - val_accuracy: 0.4375\n",
      "Epoch 72/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4540 - accuracy: 0.4073 - val_loss: 1.4141 - val_accuracy: 0.4125\n",
      "Epoch 73/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3939 - accuracy: 0.4217 - val_loss: 1.4776 - val_accuracy: 0.3500\n",
      "Epoch 74/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4056 - accuracy: 0.4087 - val_loss: 1.5612 - val_accuracy: 0.3083\n",
      "Epoch 75/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4249 - accuracy: 0.4032 - val_loss: 1.4414 - val_accuracy: 0.3500\n",
      "Epoch 76/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.4065 - accuracy: 0.4106 - val_loss: 1.5023 - val_accuracy: 0.3958\n",
      "Epoch 77/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3989 - accuracy: 0.4022 - val_loss: 1.6435 - val_accuracy: 0.2833\n",
      "Epoch 78/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3684 - accuracy: 0.4208 - val_loss: 1.3114 - val_accuracy: 0.4458\n",
      "Epoch 79/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3820 - accuracy: 0.4323 - val_loss: 1.3721 - val_accuracy: 0.4333\n",
      "Epoch 80/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3574 - accuracy: 0.4305 - val_loss: 1.3204 - val_accuracy: 0.4750\n",
      "Epoch 81/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3592 - accuracy: 0.4286 - val_loss: 1.3665 - val_accuracy: 0.4417\n",
      "Epoch 82/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3628 - accuracy: 0.4259 - val_loss: 1.3820 - val_accuracy: 0.4167\n",
      "Epoch 83/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3424 - accuracy: 0.4305 - val_loss: 1.3579 - val_accuracy: 0.4625\n",
      "Epoch 84/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3020 - accuracy: 0.4495 - val_loss: 1.3927 - val_accuracy: 0.3750\n",
      "Epoch 85/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3098 - accuracy: 0.4652 - val_loss: 1.3509 - val_accuracy: 0.4375\n",
      "Epoch 86/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.3277 - accuracy: 0.4569 - val_loss: 1.4849 - val_accuracy: 0.4125\n",
      "Epoch 87/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2720 - accuracy: 0.4634 - val_loss: 1.4987 - val_accuracy: 0.3333\n",
      "Epoch 88/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2765 - accuracy: 0.4592 - val_loss: 1.3505 - val_accuracy: 0.4292\n",
      "Epoch 89/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2858 - accuracy: 0.4601 - val_loss: 1.8200 - val_accuracy: 0.3167\n",
      "Epoch 90/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2720 - accuracy: 0.4639 - val_loss: 1.4340 - val_accuracy: 0.4292\n",
      "Epoch 91/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2682 - accuracy: 0.4620 - val_loss: 1.2239 - val_accuracy: 0.4875\n",
      "Epoch 92/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2414 - accuracy: 0.4652 - val_loss: 1.2861 - val_accuracy: 0.4208\n",
      "Epoch 93/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2668 - accuracy: 0.4778 - val_loss: 1.2648 - val_accuracy: 0.4292\n",
      "Epoch 94/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2326 - accuracy: 0.4666 - val_loss: 1.2441 - val_accuracy: 0.4458\n",
      "Epoch 95/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2465 - accuracy: 0.4750 - val_loss: 1.3229 - val_accuracy: 0.4042\n",
      "Epoch 96/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2170 - accuracy: 0.4745 - val_loss: 1.8707 - val_accuracy: 0.2750\n",
      "Epoch 97/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1919 - accuracy: 0.4986 - val_loss: 1.2909 - val_accuracy: 0.4500\n",
      "Epoch 98/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1950 - accuracy: 0.4986 - val_loss: 1.3351 - val_accuracy: 0.4167\n",
      "Epoch 99/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1741 - accuracy: 0.4986 - val_loss: 1.2306 - val_accuracy: 0.5042\n",
      "Epoch 100/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1644 - accuracy: 0.5153 - val_loss: 1.2127 - val_accuracy: 0.5000\n",
      "Epoch 101/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.2184 - accuracy: 0.4963 - val_loss: 1.2490 - val_accuracy: 0.4458\n",
      "Epoch 102/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1820 - accuracy: 0.4935 - val_loss: 1.2855 - val_accuracy: 0.4500\n",
      "Epoch 103/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1730 - accuracy: 0.4972 - val_loss: 1.3154 - val_accuracy: 0.4500\n",
      "Epoch 104/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1466 - accuracy: 0.5185 - val_loss: 1.2870 - val_accuracy: 0.4792\n",
      "Epoch 105/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1346 - accuracy: 0.5195 - val_loss: 1.3420 - val_accuracy: 0.4583\n",
      "Epoch 106/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1171 - accuracy: 0.5241 - val_loss: 1.3101 - val_accuracy: 0.3750\n",
      "Epoch 107/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1336 - accuracy: 0.5181 - val_loss: 1.2201 - val_accuracy: 0.4750\n",
      "Epoch 108/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1237 - accuracy: 0.5310 - val_loss: 1.2648 - val_accuracy: 0.4708\n",
      "Epoch 109/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1266 - accuracy: 0.5079 - val_loss: 1.2324 - val_accuracy: 0.4375\n",
      "Epoch 110/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1499 - accuracy: 0.5185 - val_loss: 1.4145 - val_accuracy: 0.4083\n",
      "Epoch 111/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.1075 - accuracy: 0.5287 - val_loss: 1.1883 - val_accuracy: 0.5083\n",
      "Epoch 112/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0972 - accuracy: 0.5269 - val_loss: 1.2367 - val_accuracy: 0.5500\n",
      "Epoch 113/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0838 - accuracy: 0.5329 - val_loss: 1.4886 - val_accuracy: 0.3583\n",
      "Epoch 114/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0708 - accuracy: 0.5436 - val_loss: 1.2257 - val_accuracy: 0.4667\n",
      "Epoch 115/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0628 - accuracy: 0.5324 - val_loss: 1.1422 - val_accuracy: 0.5625\n",
      "Epoch 116/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0527 - accuracy: 0.5602 - val_loss: 1.2899 - val_accuracy: 0.4667\n",
      "Epoch 117/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0695 - accuracy: 0.5463 - val_loss: 1.3007 - val_accuracy: 0.5000\n",
      "Epoch 118/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0515 - accuracy: 0.5473 - val_loss: 1.2062 - val_accuracy: 0.5875\n",
      "Epoch 119/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0773 - accuracy: 0.5463 - val_loss: 1.3811 - val_accuracy: 0.4458\n",
      "Epoch 120/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0660 - accuracy: 0.5334 - val_loss: 1.2025 - val_accuracy: 0.4792\n",
      "Epoch 121/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0779 - accuracy: 0.5422 - val_loss: 1.3064 - val_accuracy: 0.4958\n",
      "Epoch 122/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0654 - accuracy: 0.5630 - val_loss: 1.1794 - val_accuracy: 0.5292\n",
      "Epoch 123/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0099 - accuracy: 0.5589 - val_loss: 1.5344 - val_accuracy: 0.3250\n",
      "Epoch 124/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0284 - accuracy: 0.5690 - val_loss: 1.5223 - val_accuracy: 0.3833\n",
      "Epoch 125/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0390 - accuracy: 0.5575 - val_loss: 1.2962 - val_accuracy: 0.4333\n",
      "Epoch 126/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9892 - accuracy: 0.5913 - val_loss: 1.2747 - val_accuracy: 0.4583\n",
      "Epoch 127/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0043 - accuracy: 0.5686 - val_loss: 1.2542 - val_accuracy: 0.4583\n",
      "Epoch 128/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 1.0053 - accuracy: 0.5760 - val_loss: 1.3189 - val_accuracy: 0.4958\n",
      "Epoch 129/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9564 - accuracy: 0.5890 - val_loss: 1.6064 - val_accuracy: 0.3667\n",
      "Epoch 130/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9853 - accuracy: 0.5737 - val_loss: 1.3101 - val_accuracy: 0.5042\n",
      "Epoch 131/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9916 - accuracy: 0.5857 - val_loss: 1.2797 - val_accuracy: 0.5083\n",
      "Epoch 132/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9720 - accuracy: 0.5816 - val_loss: 1.6194 - val_accuracy: 0.4833\n",
      "Epoch 133/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9772 - accuracy: 0.5839 - val_loss: 1.1078 - val_accuracy: 0.5458\n",
      "Epoch 134/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9547 - accuracy: 0.5876 - val_loss: 1.5681 - val_accuracy: 0.3417\n",
      "Epoch 135/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9798 - accuracy: 0.5834 - val_loss: 1.2504 - val_accuracy: 0.5250\n",
      "Epoch 136/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9455 - accuracy: 0.5968 - val_loss: 1.1749 - val_accuracy: 0.5292\n",
      "Epoch 137/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9418 - accuracy: 0.5913 - val_loss: 1.2904 - val_accuracy: 0.4833\n",
      "Epoch 138/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9341 - accuracy: 0.5968 - val_loss: 1.3536 - val_accuracy: 0.5250\n",
      "Epoch 139/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.9032 - accuracy: 0.6149 - val_loss: 1.4653 - val_accuracy: 0.3417\n",
      "Epoch 140/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8960 - accuracy: 0.6145 - val_loss: 1.4535 - val_accuracy: 0.5375\n",
      "Epoch 141/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.9471 - accuracy: 0.5922 - val_loss: 1.3981 - val_accuracy: 0.5250\n",
      "Epoch 142/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.9251 - accuracy: 0.6168 - val_loss: 1.2733 - val_accuracy: 0.5042\n",
      "Epoch 143/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8986 - accuracy: 0.6196 - val_loss: 1.2513 - val_accuracy: 0.5333\n",
      "Epoch 144/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.9176 - accuracy: 0.6205 - val_loss: 1.6268 - val_accuracy: 0.4333\n",
      "Epoch 145/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.9119 - accuracy: 0.6117 - val_loss: 1.2647 - val_accuracy: 0.4875\n",
      "Epoch 146/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8939 - accuracy: 0.6270 - val_loss: 2.1134 - val_accuracy: 0.3292\n",
      "Epoch 147/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8784 - accuracy: 0.6316 - val_loss: 1.1825 - val_accuracy: 0.5708\n",
      "Epoch 148/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8506 - accuracy: 0.6297 - val_loss: 1.3533 - val_accuracy: 0.5167\n",
      "Epoch 149/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8927 - accuracy: 0.6367 - val_loss: 1.2561 - val_accuracy: 0.5542\n",
      "Epoch 150/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8604 - accuracy: 0.6372 - val_loss: 1.3175 - val_accuracy: 0.5208\n",
      "Epoch 151/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8686 - accuracy: 0.6348 - val_loss: 1.2372 - val_accuracy: 0.4958\n",
      "Epoch 152/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8460 - accuracy: 0.6469 - val_loss: 1.4204 - val_accuracy: 0.4958\n",
      "Epoch 153/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8319 - accuracy: 0.6423 - val_loss: 1.2929 - val_accuracy: 0.4792\n",
      "Epoch 154/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8845 - accuracy: 0.6237 - val_loss: 1.3214 - val_accuracy: 0.5333\n",
      "Epoch 155/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8499 - accuracy: 0.6409 - val_loss: 1.2962 - val_accuracy: 0.5500\n",
      "Epoch 156/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8332 - accuracy: 0.6418 - val_loss: 1.3306 - val_accuracy: 0.5583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8755 - accuracy: 0.6242 - val_loss: 1.2969 - val_accuracy: 0.4958\n",
      "Epoch 158/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8481 - accuracy: 0.6413 - val_loss: 1.2807 - val_accuracy: 0.5833\n",
      "Epoch 159/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8322 - accuracy: 0.6497 - val_loss: 1.3815 - val_accuracy: 0.5875\n",
      "Epoch 160/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8168 - accuracy: 0.6548 - val_loss: 1.0947 - val_accuracy: 0.6000\n",
      "Epoch 161/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7638 - accuracy: 0.6705 - val_loss: 1.3172 - val_accuracy: 0.5625\n",
      "Epoch 162/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7894 - accuracy: 0.6719 - val_loss: 1.1246 - val_accuracy: 0.5583\n",
      "Epoch 163/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7787 - accuracy: 0.6789 - val_loss: 1.2041 - val_accuracy: 0.6000\n",
      "Epoch 164/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.8206 - accuracy: 0.6636 - val_loss: 1.2454 - val_accuracy: 0.5792\n",
      "Epoch 165/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7857 - accuracy: 0.6728 - val_loss: 1.3269 - val_accuracy: 0.5208\n",
      "Epoch 166/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7869 - accuracy: 0.6710 - val_loss: 1.1974 - val_accuracy: 0.5208\n",
      "Epoch 167/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7670 - accuracy: 0.6617 - val_loss: 1.6017 - val_accuracy: 0.5083\n",
      "Epoch 168/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7675 - accuracy: 0.6793 - val_loss: 1.5556 - val_accuracy: 0.3958\n",
      "Epoch 169/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7893 - accuracy: 0.6747 - val_loss: 1.2522 - val_accuracy: 0.5792\n",
      "Epoch 170/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7465 - accuracy: 0.6691 - val_loss: 1.2350 - val_accuracy: 0.6167\n",
      "Epoch 171/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7936 - accuracy: 0.6650 - val_loss: 1.2045 - val_accuracy: 0.5750\n",
      "Epoch 172/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7549 - accuracy: 0.6807 - val_loss: 1.2955 - val_accuracy: 0.6208\n",
      "Epoch 173/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7727 - accuracy: 0.6775 - val_loss: 1.6664 - val_accuracy: 0.4792\n",
      "Epoch 174/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7419 - accuracy: 0.6988 - val_loss: 1.5614 - val_accuracy: 0.5125\n",
      "Epoch 175/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7968 - accuracy: 0.6742 - val_loss: 1.2216 - val_accuracy: 0.6208\n",
      "Epoch 176/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7428 - accuracy: 0.6969 - val_loss: 1.3828 - val_accuracy: 0.6083\n",
      "Epoch 177/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.7382 - accuracy: 0.6891 - val_loss: 1.3005 - val_accuracy: 0.5833\n",
      "Epoch 178/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7432 - accuracy: 0.6835 - val_loss: 1.3522 - val_accuracy: 0.5250\n",
      "Epoch 179/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7442 - accuracy: 0.6923 - val_loss: 1.6103 - val_accuracy: 0.4708\n",
      "Epoch 180/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7247 - accuracy: 0.6956 - val_loss: 1.2596 - val_accuracy: 0.6375\n",
      "Epoch 181/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7172 - accuracy: 0.6979 - val_loss: 1.3196 - val_accuracy: 0.5708\n",
      "Epoch 182/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7028 - accuracy: 0.7132 - val_loss: 1.3779 - val_accuracy: 0.5583\n",
      "Epoch 183/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7152 - accuracy: 0.6942 - val_loss: 1.3080 - val_accuracy: 0.5333\n",
      "Epoch 184/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7064 - accuracy: 0.7020 - val_loss: 1.3389 - val_accuracy: 0.6000\n",
      "Epoch 185/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6853 - accuracy: 0.7136 - val_loss: 1.3916 - val_accuracy: 0.5417\n",
      "Epoch 186/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7240 - accuracy: 0.6997 - val_loss: 1.4865 - val_accuracy: 0.5750\n",
      "Epoch 187/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6956 - accuracy: 0.6946 - val_loss: 1.2677 - val_accuracy: 0.6250\n",
      "Epoch 188/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6940 - accuracy: 0.7159 - val_loss: 1.3836 - val_accuracy: 0.5542\n",
      "Epoch 189/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6691 - accuracy: 0.7127 - val_loss: 1.3971 - val_accuracy: 0.5375\n",
      "Epoch 190/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.6835 - accuracy: 0.7030 - val_loss: 1.6057 - val_accuracy: 0.4833\n",
      "Epoch 191/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6772 - accuracy: 0.7234 - val_loss: 1.4021 - val_accuracy: 0.5917\n",
      "Epoch 192/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6711 - accuracy: 0.7238 - val_loss: 1.2894 - val_accuracy: 0.6292\n",
      "Epoch 193/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6469 - accuracy: 0.7266 - val_loss: 1.4832 - val_accuracy: 0.6125\n",
      "Epoch 194/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6733 - accuracy: 0.7164 - val_loss: 1.3704 - val_accuracy: 0.5375\n",
      "Epoch 195/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6276 - accuracy: 0.7419 - val_loss: 1.4128 - val_accuracy: 0.5417\n",
      "Epoch 196/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6646 - accuracy: 0.7303 - val_loss: 1.3580 - val_accuracy: 0.6000\n",
      "Epoch 197/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6443 - accuracy: 0.7326 - val_loss: 1.3988 - val_accuracy: 0.5917\n",
      "Epoch 198/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6371 - accuracy: 0.7247 - val_loss: 1.3560 - val_accuracy: 0.6000\n",
      "Epoch 199/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6282 - accuracy: 0.7354 - val_loss: 1.9657 - val_accuracy: 0.4625\n",
      "Epoch 200/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6469 - accuracy: 0.7271 - val_loss: 1.5969 - val_accuracy: 0.4750\n",
      "Epoch 201/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6476 - accuracy: 0.7247 - val_loss: 1.4949 - val_accuracy: 0.5542\n",
      "Epoch 202/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6282 - accuracy: 0.7359 - val_loss: 1.5095 - val_accuracy: 0.6125\n",
      "Epoch 203/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6412 - accuracy: 0.7414 - val_loss: 1.4084 - val_accuracy: 0.5542\n",
      "Epoch 204/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.7057 - accuracy: 0.7187 - val_loss: 1.3372 - val_accuracy: 0.5958\n",
      "Epoch 205/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6154 - accuracy: 0.7456 - val_loss: 1.4276 - val_accuracy: 0.6167\n",
      "Epoch 206/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6252 - accuracy: 0.7507 - val_loss: 1.4757 - val_accuracy: 0.6333\n",
      "Epoch 207/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5974 - accuracy: 0.7553 - val_loss: 1.6967 - val_accuracy: 0.5625\n",
      "Epoch 208/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6182 - accuracy: 0.7447 - val_loss: 1.4829 - val_accuracy: 0.4958\n",
      "Epoch 209/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6453 - accuracy: 0.7572 - val_loss: 1.4634 - val_accuracy: 0.5708\n",
      "Epoch 210/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5996 - accuracy: 0.7470 - val_loss: 1.6739 - val_accuracy: 0.5083\n",
      "Epoch 211/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6031 - accuracy: 0.7549 - val_loss: 1.3526 - val_accuracy: 0.6375\n",
      "Epoch 212/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6035 - accuracy: 0.7553 - val_loss: 1.3438 - val_accuracy: 0.6458\n",
      "Epoch 213/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6062 - accuracy: 0.7553 - val_loss: 1.5803 - val_accuracy: 0.5417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5977 - accuracy: 0.7488 - val_loss: 1.4502 - val_accuracy: 0.6375\n",
      "Epoch 215/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5795 - accuracy: 0.7563 - val_loss: 1.3542 - val_accuracy: 0.6375\n",
      "Epoch 216/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5893 - accuracy: 0.7498 - val_loss: 1.5206 - val_accuracy: 0.6417\n",
      "Epoch 217/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6027 - accuracy: 0.7516 - val_loss: 1.8196 - val_accuracy: 0.6083\n",
      "Epoch 218/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6089 - accuracy: 0.7419 - val_loss: 1.4362 - val_accuracy: 0.6583\n",
      "Epoch 219/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5784 - accuracy: 0.7516 - val_loss: 1.4808 - val_accuracy: 0.6167\n",
      "Epoch 220/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5702 - accuracy: 0.7614 - val_loss: 1.3153 - val_accuracy: 0.6208\n",
      "Epoch 221/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5837 - accuracy: 0.7697 - val_loss: 1.3754 - val_accuracy: 0.6458\n",
      "Epoch 222/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5291 - accuracy: 0.7734 - val_loss: 1.6610 - val_accuracy: 0.6042\n",
      "Epoch 223/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5545 - accuracy: 0.7766 - val_loss: 1.5143 - val_accuracy: 0.5625\n",
      "Epoch 224/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5643 - accuracy: 0.7530 - val_loss: 1.4812 - val_accuracy: 0.6167\n",
      "Epoch 225/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5563 - accuracy: 0.7665 - val_loss: 1.6552 - val_accuracy: 0.5958\n",
      "Epoch 226/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5355 - accuracy: 0.7905 - val_loss: 1.6950 - val_accuracy: 0.5875\n",
      "Epoch 227/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5226 - accuracy: 0.7794 - val_loss: 1.5722 - val_accuracy: 0.5833\n",
      "Epoch 228/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5855 - accuracy: 0.7632 - val_loss: 1.5486 - val_accuracy: 0.6208\n",
      "Epoch 229/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5318 - accuracy: 0.7804 - val_loss: 1.6283 - val_accuracy: 0.6000\n",
      "Epoch 230/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5404 - accuracy: 0.7706 - val_loss: 1.4824 - val_accuracy: 0.6458\n",
      "Epoch 231/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5394 - accuracy: 0.7831 - val_loss: 1.5287 - val_accuracy: 0.6292\n",
      "Epoch 232/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5150 - accuracy: 0.7882 - val_loss: 2.3477 - val_accuracy: 0.4625\n",
      "Epoch 233/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5485 - accuracy: 0.7748 - val_loss: 1.5766 - val_accuracy: 0.5833\n",
      "Epoch 234/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5394 - accuracy: 0.7790 - val_loss: 1.6435 - val_accuracy: 0.6083\n",
      "Epoch 235/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5288 - accuracy: 0.7808 - val_loss: 1.5925 - val_accuracy: 0.6375\n",
      "Epoch 236/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5243 - accuracy: 0.7766 - val_loss: 1.6731 - val_accuracy: 0.5625\n",
      "Epoch 237/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5318 - accuracy: 0.7757 - val_loss: 1.4634 - val_accuracy: 0.5917\n",
      "Epoch 238/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5276 - accuracy: 0.7808 - val_loss: 1.8091 - val_accuracy: 0.6125\n",
      "Epoch 239/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5263 - accuracy: 0.7915 - val_loss: 1.4414 - val_accuracy: 0.6125\n",
      "Epoch 240/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5380 - accuracy: 0.7743 - val_loss: 1.4577 - val_accuracy: 0.6667\n",
      "Epoch 241/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.5118 - accuracy: 0.7961 - val_loss: 1.5831 - val_accuracy: 0.6417\n",
      "Epoch 242/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5213 - accuracy: 0.7845 - val_loss: 1.8367 - val_accuracy: 0.5458\n",
      "Epoch 243/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5420 - accuracy: 0.7734 - val_loss: 1.4721 - val_accuracy: 0.6458\n",
      "Epoch 244/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5192 - accuracy: 0.7952 - val_loss: 1.7767 - val_accuracy: 0.6167\n",
      "Epoch 245/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5210 - accuracy: 0.7878 - val_loss: 1.9237 - val_accuracy: 0.5500\n",
      "Epoch 246/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4996 - accuracy: 0.7938 - val_loss: 1.5267 - val_accuracy: 0.6375\n",
      "Epoch 247/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5215 - accuracy: 0.7841 - val_loss: 1.8531 - val_accuracy: 0.5417\n",
      "Epoch 248/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5277 - accuracy: 0.7896 - val_loss: 1.6954 - val_accuracy: 0.6125\n",
      "Epoch 249/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5094 - accuracy: 0.7799 - val_loss: 1.4688 - val_accuracy: 0.6292\n",
      "Epoch 250/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4929 - accuracy: 0.7910 - val_loss: 1.5058 - val_accuracy: 0.6167\n",
      "Epoch 251/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4943 - accuracy: 0.7841 - val_loss: 1.7768 - val_accuracy: 0.5958\n",
      "Epoch 252/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4898 - accuracy: 0.8054 - val_loss: 1.6435 - val_accuracy: 0.6458\n",
      "Epoch 253/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5247 - accuracy: 0.7766 - val_loss: 1.5733 - val_accuracy: 0.6458\n",
      "Epoch 254/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4861 - accuracy: 0.8068 - val_loss: 1.6803 - val_accuracy: 0.6208\n",
      "Epoch 255/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4814 - accuracy: 0.7994 - val_loss: 1.6723 - val_accuracy: 0.5958\n",
      "Epoch 256/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4641 - accuracy: 0.8109 - val_loss: 1.6983 - val_accuracy: 0.6250\n",
      "Epoch 257/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5050 - accuracy: 0.7947 - val_loss: 1.6668 - val_accuracy: 0.5333\n",
      "Epoch 258/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4546 - accuracy: 0.8137 - val_loss: 1.4856 - val_accuracy: 0.6333\n",
      "Epoch 259/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4995 - accuracy: 0.8012 - val_loss: 1.8181 - val_accuracy: 0.5583\n",
      "Epoch 260/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4784 - accuracy: 0.8142 - val_loss: 1.4368 - val_accuracy: 0.6583\n",
      "Epoch 261/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4871 - accuracy: 0.7947 - val_loss: 1.4980 - val_accuracy: 0.6125\n",
      "Epoch 262/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4641 - accuracy: 0.8077 - val_loss: 1.6284 - val_accuracy: 0.6583\n",
      "Epoch 263/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4586 - accuracy: 0.8068 - val_loss: 1.9393 - val_accuracy: 0.5500\n",
      "Epoch 264/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4729 - accuracy: 0.8012 - val_loss: 1.6944 - val_accuracy: 0.6375\n",
      "Epoch 265/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4572 - accuracy: 0.8128 - val_loss: 1.4243 - val_accuracy: 0.6667\n",
      "Epoch 266/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4866 - accuracy: 0.8007 - val_loss: 1.5496 - val_accuracy: 0.6417\n",
      "Epoch 267/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4798 - accuracy: 0.7998 - val_loss: 1.6867 - val_accuracy: 0.6125\n",
      "Epoch 268/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4519 - accuracy: 0.8156 - val_loss: 1.7170 - val_accuracy: 0.6625\n",
      "Epoch 269/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4586 - accuracy: 0.8170 - val_loss: 2.1309 - val_accuracy: 0.5875\n",
      "Epoch 270/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4558 - accuracy: 0.8239 - val_loss: 1.5044 - val_accuracy: 0.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4731 - accuracy: 0.8068 - val_loss: 1.7730 - val_accuracy: 0.6417\n",
      "Epoch 272/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4592 - accuracy: 0.8128 - val_loss: 1.6825 - val_accuracy: 0.6125\n",
      "Epoch 273/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4346 - accuracy: 0.8295 - val_loss: 1.8757 - val_accuracy: 0.6000\n",
      "Epoch 274/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4686 - accuracy: 0.7984 - val_loss: 1.7530 - val_accuracy: 0.5500\n",
      "Epoch 275/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4528 - accuracy: 0.8188 - val_loss: 1.7196 - val_accuracy: 0.5958\n",
      "Epoch 276/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4365 - accuracy: 0.8197 - val_loss: 2.4996 - val_accuracy: 0.4458\n",
      "Epoch 277/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4774 - accuracy: 0.8021 - val_loss: 1.5750 - val_accuracy: 0.6458\n",
      "Epoch 278/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4302 - accuracy: 0.8165 - val_loss: 1.8382 - val_accuracy: 0.6708\n",
      "Epoch 279/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4648 - accuracy: 0.8151 - val_loss: 1.7460 - val_accuracy: 0.6583\n",
      "Epoch 280/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4566 - accuracy: 0.8151 - val_loss: 1.7153 - val_accuracy: 0.6375\n",
      "Epoch 281/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4406 - accuracy: 0.8225 - val_loss: 1.7003 - val_accuracy: 0.5875\n",
      "Epoch 282/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4214 - accuracy: 0.8239 - val_loss: 1.7768 - val_accuracy: 0.6542\n",
      "Epoch 283/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4545 - accuracy: 0.8248 - val_loss: 1.7819 - val_accuracy: 0.6292\n",
      "Epoch 284/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4340 - accuracy: 0.8160 - val_loss: 1.8332 - val_accuracy: 0.6083\n",
      "Epoch 285/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4345 - accuracy: 0.8234 - val_loss: 1.6144 - val_accuracy: 0.6333\n",
      "Epoch 286/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4129 - accuracy: 0.8313 - val_loss: 1.6091 - val_accuracy: 0.6458\n",
      "Epoch 287/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4461 - accuracy: 0.8160 - val_loss: 1.6217 - val_accuracy: 0.7000\n",
      "Epoch 288/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4174 - accuracy: 0.8285 - val_loss: 1.7238 - val_accuracy: 0.6542\n",
      "Epoch 289/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4839 - accuracy: 0.8142 - val_loss: 1.6128 - val_accuracy: 0.6125\n",
      "Epoch 290/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4389 - accuracy: 0.8197 - val_loss: 1.6771 - val_accuracy: 0.6500\n",
      "Epoch 291/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4023 - accuracy: 0.8313 - val_loss: 1.6297 - val_accuracy: 0.6750\n",
      "Epoch 292/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4304 - accuracy: 0.8239 - val_loss: 1.7830 - val_accuracy: 0.6667\n",
      "Epoch 293/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4188 - accuracy: 0.8332 - val_loss: 1.6550 - val_accuracy: 0.6708\n",
      "Epoch 294/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4251 - accuracy: 0.8285 - val_loss: 1.9523 - val_accuracy: 0.6375\n",
      "Epoch 295/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4446 - accuracy: 0.8100 - val_loss: 1.8527 - val_accuracy: 0.6375\n",
      "Epoch 296/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3949 - accuracy: 0.8489 - val_loss: 1.9310 - val_accuracy: 0.6083\n",
      "Epoch 297/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4169 - accuracy: 0.8336 - val_loss: 1.6820 - val_accuracy: 0.6667\n",
      "Epoch 298/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4445 - accuracy: 0.8128 - val_loss: 1.6197 - val_accuracy: 0.6167\n",
      "Epoch 299/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4280 - accuracy: 0.8211 - val_loss: 1.8642 - val_accuracy: 0.6125\n",
      "Epoch 300/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4048 - accuracy: 0.8383 - val_loss: 1.9266 - val_accuracy: 0.6583\n",
      "Epoch 301/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4024 - accuracy: 0.8406 - val_loss: 1.8285 - val_accuracy: 0.6542\n",
      "Epoch 302/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4390 - accuracy: 0.8244 - val_loss: 1.8004 - val_accuracy: 0.6375\n",
      "Epoch 303/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4317 - accuracy: 0.8341 - val_loss: 1.8113 - val_accuracy: 0.6625\n",
      "Epoch 304/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3809 - accuracy: 0.8378 - val_loss: 2.0076 - val_accuracy: 0.5667\n",
      "Epoch 305/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3974 - accuracy: 0.8420 - val_loss: 1.9079 - val_accuracy: 0.6333\n",
      "Epoch 306/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4088 - accuracy: 0.8420 - val_loss: 1.7867 - val_accuracy: 0.6292\n",
      "Epoch 307/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4033 - accuracy: 0.8378 - val_loss: 1.7007 - val_accuracy: 0.6542\n",
      "Epoch 308/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4249 - accuracy: 0.8267 - val_loss: 1.8777 - val_accuracy: 0.6500\n",
      "Epoch 309/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4100 - accuracy: 0.8346 - val_loss: 1.6767 - val_accuracy: 0.6458\n",
      "Epoch 310/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3850 - accuracy: 0.8336 - val_loss: 2.1579 - val_accuracy: 0.5333\n",
      "Epoch 311/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4007 - accuracy: 0.8392 - val_loss: 1.8699 - val_accuracy: 0.7000\n",
      "Epoch 312/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3953 - accuracy: 0.8397 - val_loss: 3.5690 - val_accuracy: 0.5833\n",
      "Epoch 313/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5131 - accuracy: 0.8091 - val_loss: 1.7207 - val_accuracy: 0.6292\n",
      "Epoch 314/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4238 - accuracy: 0.8262 - val_loss: 1.5765 - val_accuracy: 0.6833\n",
      "Epoch 315/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4352 - accuracy: 0.8244 - val_loss: 1.5727 - val_accuracy: 0.6625\n",
      "Epoch 316/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4424 - accuracy: 0.8207 - val_loss: 1.5307 - val_accuracy: 0.6167\n",
      "Epoch 317/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3813 - accuracy: 0.8438 - val_loss: 1.6138 - val_accuracy: 0.6750\n",
      "Epoch 318/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3874 - accuracy: 0.8462 - val_loss: 1.7215 - val_accuracy: 0.6542\n",
      "Epoch 319/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3724 - accuracy: 0.8554 - val_loss: 1.6151 - val_accuracy: 0.6375\n",
      "Epoch 320/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3925 - accuracy: 0.8397 - val_loss: 1.7139 - val_accuracy: 0.6333\n",
      "Epoch 321/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3784 - accuracy: 0.8485 - val_loss: 1.9268 - val_accuracy: 0.6000\n",
      "Epoch 322/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3837 - accuracy: 0.8438 - val_loss: 1.6856 - val_accuracy: 0.6500\n",
      "Epoch 323/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3771 - accuracy: 0.8462 - val_loss: 1.9474 - val_accuracy: 0.6542\n",
      "Epoch 324/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3663 - accuracy: 0.8522 - val_loss: 1.7053 - val_accuracy: 0.6250\n",
      "Epoch 325/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3793 - accuracy: 0.8448 - val_loss: 1.9388 - val_accuracy: 0.5917\n",
      "Epoch 326/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3596 - accuracy: 0.8554 - val_loss: 1.9567 - val_accuracy: 0.6667\n",
      "Epoch 327/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3631 - accuracy: 0.8517 - val_loss: 2.1373 - val_accuracy: 0.5958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4005 - accuracy: 0.8485 - val_loss: 2.0171 - val_accuracy: 0.6292\n",
      "Epoch 329/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3826 - accuracy: 0.8443 - val_loss: 1.8096 - val_accuracy: 0.6542\n",
      "Epoch 330/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3875 - accuracy: 0.8406 - val_loss: 1.8464 - val_accuracy: 0.6458\n",
      "Epoch 331/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3834 - accuracy: 0.8411 - val_loss: 2.0369 - val_accuracy: 0.6125\n",
      "Epoch 332/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3783 - accuracy: 0.8438 - val_loss: 1.8724 - val_accuracy: 0.6542\n",
      "Epoch 333/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4041 - accuracy: 0.8424 - val_loss: 1.8354 - val_accuracy: 0.6417\n",
      "Epoch 334/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3406 - accuracy: 0.8661 - val_loss: 1.9005 - val_accuracy: 0.6542\n",
      "Epoch 335/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3605 - accuracy: 0.8466 - val_loss: 1.9282 - val_accuracy: 0.6375\n",
      "Epoch 336/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3813 - accuracy: 0.8406 - val_loss: 1.8900 - val_accuracy: 0.6542\n",
      "Epoch 337/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3795 - accuracy: 0.8587 - val_loss: 1.9328 - val_accuracy: 0.6542\n",
      "Epoch 338/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4030 - accuracy: 0.8355 - val_loss: 1.9586 - val_accuracy: 0.6667\n",
      "Epoch 339/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3977 - accuracy: 0.8378 - val_loss: 2.0946 - val_accuracy: 0.6625\n",
      "Epoch 340/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3869 - accuracy: 0.8443 - val_loss: 2.0046 - val_accuracy: 0.6542\n",
      "Epoch 341/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3860 - accuracy: 0.8406 - val_loss: 2.2333 - val_accuracy: 0.5417\n",
      "Epoch 342/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3600 - accuracy: 0.8503 - val_loss: 2.0440 - val_accuracy: 0.6417\n",
      "Epoch 343/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3677 - accuracy: 0.8536 - val_loss: 2.0023 - val_accuracy: 0.6417\n",
      "Epoch 344/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3727 - accuracy: 0.8471 - val_loss: 2.0124 - val_accuracy: 0.6208\n",
      "Epoch 345/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3964 - accuracy: 0.8401 - val_loss: 2.2116 - val_accuracy: 0.5583\n",
      "Epoch 346/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3550 - accuracy: 0.8540 - val_loss: 2.2500 - val_accuracy: 0.6625\n",
      "Epoch 347/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3777 - accuracy: 0.8531 - val_loss: 2.0287 - val_accuracy: 0.6583\n",
      "Epoch 348/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3654 - accuracy: 0.8582 - val_loss: 1.8762 - val_accuracy: 0.6542\n",
      "Epoch 349/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3158 - accuracy: 0.8716 - val_loss: 2.5110 - val_accuracy: 0.5750\n",
      "Epoch 350/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3398 - accuracy: 0.8587 - val_loss: 2.0528 - val_accuracy: 0.6542\n",
      "Epoch 351/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4003 - accuracy: 0.8378 - val_loss: 1.8694 - val_accuracy: 0.6417\n",
      "Epoch 352/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3255 - accuracy: 0.8591 - val_loss: 2.1665 - val_accuracy: 0.5958\n",
      "Epoch 353/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3411 - accuracy: 0.8703 - val_loss: 1.8461 - val_accuracy: 0.6625\n",
      "Epoch 354/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3775 - accuracy: 0.8508 - val_loss: 1.9248 - val_accuracy: 0.6417\n",
      "Epoch 355/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3291 - accuracy: 0.8712 - val_loss: 1.8347 - val_accuracy: 0.6375\n",
      "Epoch 356/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3459 - accuracy: 0.8605 - val_loss: 1.9509 - val_accuracy: 0.5750\n",
      "Epoch 357/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3323 - accuracy: 0.8614 - val_loss: 1.9353 - val_accuracy: 0.6375\n",
      "Epoch 358/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3679 - accuracy: 0.8582 - val_loss: 1.8952 - val_accuracy: 0.7125\n",
      "Epoch 359/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3380 - accuracy: 0.8661 - val_loss: 2.9356 - val_accuracy: 0.4583\n",
      "Epoch 360/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3775 - accuracy: 0.8452 - val_loss: 1.9607 - val_accuracy: 0.7083\n",
      "Epoch 361/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3351 - accuracy: 0.8721 - val_loss: 1.9139 - val_accuracy: 0.6833\n",
      "Epoch 362/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3326 - accuracy: 0.8707 - val_loss: 2.0587 - val_accuracy: 0.6250\n",
      "Epoch 363/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3442 - accuracy: 0.8591 - val_loss: 2.0330 - val_accuracy: 0.6458\n",
      "Epoch 364/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3161 - accuracy: 0.8753 - val_loss: 1.8822 - val_accuracy: 0.6583\n",
      "Epoch 365/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3576 - accuracy: 0.8573 - val_loss: 2.1080 - val_accuracy: 0.6458\n",
      "Epoch 366/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3620 - accuracy: 0.8628 - val_loss: 1.6870 - val_accuracy: 0.6542\n",
      "Epoch 367/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3358 - accuracy: 0.8689 - val_loss: 1.8659 - val_accuracy: 0.6500\n",
      "Epoch 368/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3596 - accuracy: 0.8652 - val_loss: 2.1686 - val_accuracy: 0.6458\n",
      "Epoch 369/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3237 - accuracy: 0.8647 - val_loss: 1.9582 - val_accuracy: 0.6625\n",
      "Epoch 370/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3444 - accuracy: 0.8665 - val_loss: 1.9493 - val_accuracy: 0.6625\n",
      "Epoch 371/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3460 - accuracy: 0.8568 - val_loss: 2.1233 - val_accuracy: 0.6208\n",
      "Epoch 372/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3664 - accuracy: 0.8563 - val_loss: 2.0915 - val_accuracy: 0.5917\n",
      "Epoch 373/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3311 - accuracy: 0.8670 - val_loss: 1.7313 - val_accuracy: 0.6792\n",
      "Epoch 374/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3057 - accuracy: 0.8767 - val_loss: 2.0409 - val_accuracy: 0.6667\n",
      "Epoch 375/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3919 - accuracy: 0.8383 - val_loss: 2.1792 - val_accuracy: 0.5625\n",
      "Epoch 376/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3685 - accuracy: 0.8536 - val_loss: 1.7453 - val_accuracy: 0.6375\n",
      "Epoch 377/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3143 - accuracy: 0.8749 - val_loss: 1.9623 - val_accuracy: 0.7042\n",
      "Epoch 378/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3398 - accuracy: 0.8698 - val_loss: 1.9067 - val_accuracy: 0.6250\n",
      "Epoch 379/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3628 - accuracy: 0.8559 - val_loss: 1.7991 - val_accuracy: 0.6375\n",
      "Epoch 380/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3132 - accuracy: 0.8795 - val_loss: 2.0120 - val_accuracy: 0.6792\n",
      "Epoch 381/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3207 - accuracy: 0.8758 - val_loss: 2.3431 - val_accuracy: 0.6083\n",
      "Epoch 382/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3246 - accuracy: 0.8661 - val_loss: 2.0506 - val_accuracy: 0.6708\n",
      "Epoch 383/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3346 - accuracy: 0.8614 - val_loss: 2.1583 - val_accuracy: 0.6292\n",
      "Epoch 384/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3379 - accuracy: 0.8619 - val_loss: 2.2099 - val_accuracy: 0.6417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3421 - accuracy: 0.8540 - val_loss: 2.0112 - val_accuracy: 0.6375\n",
      "Epoch 386/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3173 - accuracy: 0.8665 - val_loss: 1.9779 - val_accuracy: 0.6333\n",
      "Epoch 387/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3107 - accuracy: 0.8740 - val_loss: 2.0311 - val_accuracy: 0.6708\n",
      "Epoch 388/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3174 - accuracy: 0.8661 - val_loss: 2.3291 - val_accuracy: 0.6167\n",
      "Epoch 389/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3237 - accuracy: 0.8628 - val_loss: 2.3214 - val_accuracy: 0.6083\n",
      "Epoch 390/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3076 - accuracy: 0.8832 - val_loss: 2.2372 - val_accuracy: 0.6500\n",
      "Epoch 391/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2903 - accuracy: 0.8874 - val_loss: 2.5474 - val_accuracy: 0.6042\n",
      "Epoch 392/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3283 - accuracy: 0.8758 - val_loss: 2.1249 - val_accuracy: 0.6208\n",
      "Epoch 393/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3125 - accuracy: 0.8716 - val_loss: 2.3819 - val_accuracy: 0.5792\n",
      "Epoch 394/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3214 - accuracy: 0.8684 - val_loss: 2.3267 - val_accuracy: 0.6292\n",
      "Epoch 395/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3126 - accuracy: 0.8777 - val_loss: 2.7746 - val_accuracy: 0.5292\n",
      "Epoch 396/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3105 - accuracy: 0.8703 - val_loss: 2.2303 - val_accuracy: 0.6833\n",
      "Epoch 397/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3241 - accuracy: 0.8772 - val_loss: 1.9262 - val_accuracy: 0.6667\n",
      "Epoch 398/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3714 - accuracy: 0.8596 - val_loss: 1.9784 - val_accuracy: 0.6375\n",
      "Epoch 399/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3265 - accuracy: 0.8735 - val_loss: 2.1829 - val_accuracy: 0.6667\n",
      "Epoch 400/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3044 - accuracy: 0.8726 - val_loss: 2.1619 - val_accuracy: 0.6667\n",
      "Epoch 401/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3141 - accuracy: 0.8823 - val_loss: 2.1425 - val_accuracy: 0.6583\n",
      "Epoch 402/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3187 - accuracy: 0.8763 - val_loss: 2.2450 - val_accuracy: 0.6417\n",
      "Epoch 403/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2915 - accuracy: 0.8828 - val_loss: 2.1344 - val_accuracy: 0.6375\n",
      "Epoch 404/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2835 - accuracy: 0.8781 - val_loss: 2.0576 - val_accuracy: 0.7042\n",
      "Epoch 405/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3067 - accuracy: 0.8818 - val_loss: 2.4123 - val_accuracy: 0.6917\n",
      "Epoch 406/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3661 - accuracy: 0.8647 - val_loss: 2.0670 - val_accuracy: 0.6667\n",
      "Epoch 407/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2890 - accuracy: 0.8823 - val_loss: 1.9090 - val_accuracy: 0.6667\n",
      "Epoch 408/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3131 - accuracy: 0.8809 - val_loss: 2.2550 - val_accuracy: 0.6750\n",
      "Epoch 409/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3179 - accuracy: 0.8730 - val_loss: 2.1758 - val_accuracy: 0.6667\n",
      "Epoch 410/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3482 - accuracy: 0.8591 - val_loss: 2.2048 - val_accuracy: 0.6667\n",
      "Epoch 411/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2838 - accuracy: 0.8934 - val_loss: 2.2710 - val_accuracy: 0.6833\n",
      "Epoch 412/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2917 - accuracy: 0.8920 - val_loss: 3.1675 - val_accuracy: 0.6250\n",
      "Epoch 413/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4253 - accuracy: 0.8605 - val_loss: 2.0501 - val_accuracy: 0.5708\n",
      "Epoch 414/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3140 - accuracy: 0.8795 - val_loss: 1.9328 - val_accuracy: 0.6875\n",
      "Epoch 415/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2892 - accuracy: 0.8846 - val_loss: 2.3488 - val_accuracy: 0.5917\n",
      "Epoch 416/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3007 - accuracy: 0.8892 - val_loss: 1.9501 - val_accuracy: 0.6833\n",
      "Epoch 417/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2948 - accuracy: 0.8753 - val_loss: 2.1641 - val_accuracy: 0.6500\n",
      "Epoch 418/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2950 - accuracy: 0.8892 - val_loss: 2.2531 - val_accuracy: 0.6458\n",
      "Epoch 419/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2779 - accuracy: 0.8916 - val_loss: 2.2330 - val_accuracy: 0.6292\n",
      "Epoch 420/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3103 - accuracy: 0.8777 - val_loss: 2.0979 - val_accuracy: 0.6792\n",
      "Epoch 421/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3199 - accuracy: 0.8679 - val_loss: 2.0020 - val_accuracy: 0.6833\n",
      "Epoch 422/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2705 - accuracy: 0.8953 - val_loss: 2.3407 - val_accuracy: 0.6708\n",
      "Epoch 423/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2610 - accuracy: 0.8948 - val_loss: 2.3217 - val_accuracy: 0.5917\n",
      "Epoch 424/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3069 - accuracy: 0.8814 - val_loss: 1.8280 - val_accuracy: 0.6792\n",
      "Epoch 425/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2860 - accuracy: 0.8851 - val_loss: 2.2658 - val_accuracy: 0.6750\n",
      "Epoch 426/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2755 - accuracy: 0.8888 - val_loss: 2.6305 - val_accuracy: 0.6583\n",
      "Epoch 427/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2942 - accuracy: 0.8874 - val_loss: 2.2015 - val_accuracy: 0.6708\n",
      "Epoch 428/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3385 - accuracy: 0.8767 - val_loss: 2.1523 - val_accuracy: 0.6333\n",
      "Epoch 429/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3311 - accuracy: 0.8730 - val_loss: 2.0668 - val_accuracy: 0.6583\n",
      "Epoch 430/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2696 - accuracy: 0.8999 - val_loss: 2.1284 - val_accuracy: 0.6958\n",
      "Epoch 431/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3243 - accuracy: 0.8707 - val_loss: 2.1149 - val_accuracy: 0.6500\n",
      "Epoch 432/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2941 - accuracy: 0.8828 - val_loss: 2.0949 - val_accuracy: 0.6417\n",
      "Epoch 433/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2933 - accuracy: 0.8888 - val_loss: 2.1416 - val_accuracy: 0.6625\n",
      "Epoch 434/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2709 - accuracy: 0.8939 - val_loss: 2.2829 - val_accuracy: 0.6333\n",
      "Epoch 435/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2651 - accuracy: 0.9013 - val_loss: 2.2191 - val_accuracy: 0.6750\n",
      "Epoch 436/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2893 - accuracy: 0.8851 - val_loss: 2.2007 - val_accuracy: 0.6667\n",
      "Epoch 437/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2766 - accuracy: 0.8971 - val_loss: 2.0588 - val_accuracy: 0.7000\n",
      "Epoch 438/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3084 - accuracy: 0.8851 - val_loss: 1.7923 - val_accuracy: 0.6708\n",
      "Epoch 439/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2903 - accuracy: 0.8865 - val_loss: 2.1879 - val_accuracy: 0.6292\n",
      "Epoch 440/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2821 - accuracy: 0.8883 - val_loss: 2.0443 - val_accuracy: 0.6958\n",
      "Epoch 441/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3143 - accuracy: 0.8828 - val_loss: 1.9804 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2767 - accuracy: 0.8879 - val_loss: 2.2081 - val_accuracy: 0.6750\n",
      "Epoch 443/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3031 - accuracy: 0.8953 - val_loss: 2.1386 - val_accuracy: 0.6750\n",
      "Epoch 444/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2972 - accuracy: 0.8911 - val_loss: 2.0033 - val_accuracy: 0.7000\n",
      "Epoch 445/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2819 - accuracy: 0.8795 - val_loss: 2.1867 - val_accuracy: 0.6833\n",
      "Epoch 446/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2564 - accuracy: 0.8971 - val_loss: 2.0203 - val_accuracy: 0.6625\n",
      "Epoch 447/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2743 - accuracy: 0.8967 - val_loss: 2.2120 - val_accuracy: 0.6542\n",
      "Epoch 448/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2719 - accuracy: 0.8874 - val_loss: 2.9557 - val_accuracy: 0.5083\n",
      "Epoch 449/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2826 - accuracy: 0.8911 - val_loss: 2.3088 - val_accuracy: 0.6500\n",
      "Epoch 450/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2622 - accuracy: 0.8911 - val_loss: 3.0419 - val_accuracy: 0.5958\n",
      "Epoch 451/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3120 - accuracy: 0.8791 - val_loss: 2.6106 - val_accuracy: 0.6417\n",
      "Epoch 452/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3044 - accuracy: 0.8818 - val_loss: 2.1035 - val_accuracy: 0.6458\n",
      "Epoch 453/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2973 - accuracy: 0.8772 - val_loss: 2.0960 - val_accuracy: 0.6833\n",
      "Epoch 454/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2650 - accuracy: 0.8957 - val_loss: 2.0800 - val_accuracy: 0.6875\n",
      "Epoch 455/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2780 - accuracy: 0.8879 - val_loss: 2.6240 - val_accuracy: 0.6458\n",
      "Epoch 456/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2850 - accuracy: 0.8846 - val_loss: 1.9537 - val_accuracy: 0.7083\n",
      "Epoch 457/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2600 - accuracy: 0.8957 - val_loss: 2.1197 - val_accuracy: 0.7042\n",
      "Epoch 458/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2772 - accuracy: 0.8804 - val_loss: 2.3030 - val_accuracy: 0.6458\n",
      "Epoch 459/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2763 - accuracy: 0.8911 - val_loss: 2.0731 - val_accuracy: 0.7167\n",
      "Epoch 460/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2396 - accuracy: 0.9096 - val_loss: 2.5008 - val_accuracy: 0.6083\n",
      "Epoch 461/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2701 - accuracy: 0.8962 - val_loss: 2.1910 - val_accuracy: 0.7000\n",
      "Epoch 462/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2538 - accuracy: 0.9018 - val_loss: 2.2432 - val_accuracy: 0.6708\n",
      "Epoch 463/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2776 - accuracy: 0.8953 - val_loss: 2.1441 - val_accuracy: 0.6875\n",
      "Epoch 464/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2595 - accuracy: 0.9032 - val_loss: 2.2805 - val_accuracy: 0.6917\n",
      "Epoch 465/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2552 - accuracy: 0.9018 - val_loss: 2.4179 - val_accuracy: 0.6542\n",
      "Epoch 466/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2806 - accuracy: 0.8888 - val_loss: 2.3064 - val_accuracy: 0.6292\n",
      "Epoch 467/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2603 - accuracy: 0.9022 - val_loss: 2.2287 - val_accuracy: 0.6792\n",
      "Epoch 468/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2463 - accuracy: 0.9087 - val_loss: 2.2405 - val_accuracy: 0.6875\n",
      "Epoch 469/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2778 - accuracy: 0.8930 - val_loss: 2.3599 - val_accuracy: 0.6917\n",
      "Epoch 470/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2549 - accuracy: 0.8957 - val_loss: 2.5822 - val_accuracy: 0.6375\n",
      "Epoch 471/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2652 - accuracy: 0.9004 - val_loss: 2.1741 - val_accuracy: 0.6625\n",
      "Epoch 472/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2952 - accuracy: 0.8897 - val_loss: 2.0597 - val_accuracy: 0.6667\n",
      "Epoch 473/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2877 - accuracy: 0.8851 - val_loss: 2.0785 - val_accuracy: 0.6250\n",
      "Epoch 474/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2924 - accuracy: 0.8832 - val_loss: 2.1421 - val_accuracy: 0.6958\n",
      "Epoch 475/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2565 - accuracy: 0.8981 - val_loss: 2.2525 - val_accuracy: 0.6917\n",
      "Epoch 476/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2625 - accuracy: 0.9055 - val_loss: 2.3524 - val_accuracy: 0.6958\n",
      "Epoch 477/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2509 - accuracy: 0.8994 - val_loss: 2.1708 - val_accuracy: 0.6750\n",
      "Epoch 478/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2393 - accuracy: 0.8985 - val_loss: 2.5392 - val_accuracy: 0.6833\n",
      "Epoch 479/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2341 - accuracy: 0.9124 - val_loss: 2.5559 - val_accuracy: 0.6375\n",
      "Epoch 480/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2495 - accuracy: 0.9124 - val_loss: 2.7016 - val_accuracy: 0.6833\n",
      "Epoch 481/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2848 - accuracy: 0.8869 - val_loss: 2.2587 - val_accuracy: 0.6917\n",
      "Epoch 482/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2575 - accuracy: 0.8948 - val_loss: 2.2394 - val_accuracy: 0.6750\n",
      "Epoch 483/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2732 - accuracy: 0.9027 - val_loss: 2.2161 - val_accuracy: 0.6333\n",
      "Epoch 484/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2775 - accuracy: 0.8916 - val_loss: 2.4171 - val_accuracy: 0.6250\n",
      "Epoch 485/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2604 - accuracy: 0.8897 - val_loss: 2.6175 - val_accuracy: 0.6417\n",
      "Epoch 486/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2874 - accuracy: 0.8906 - val_loss: 2.1135 - val_accuracy: 0.6833\n",
      "Epoch 487/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2558 - accuracy: 0.8981 - val_loss: 2.0781 - val_accuracy: 0.7042\n",
      "Epoch 488/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2446 - accuracy: 0.9055 - val_loss: 2.1765 - val_accuracy: 0.6750\n",
      "Epoch 489/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2721 - accuracy: 0.9004 - val_loss: 2.2654 - val_accuracy: 0.7083\n",
      "Epoch 490/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2522 - accuracy: 0.8985 - val_loss: 2.2623 - val_accuracy: 0.7000\n",
      "Epoch 491/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2381 - accuracy: 0.9041 - val_loss: 2.1622 - val_accuracy: 0.7042\n",
      "Epoch 492/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2705 - accuracy: 0.8925 - val_loss: 2.8438 - val_accuracy: 0.6292\n",
      "Epoch 493/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2512 - accuracy: 0.9008 - val_loss: 2.2373 - val_accuracy: 0.6667\n",
      "Epoch 494/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3106 - accuracy: 0.8842 - val_loss: 2.0676 - val_accuracy: 0.6833\n",
      "Epoch 495/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2919 - accuracy: 0.8860 - val_loss: 2.0760 - val_accuracy: 0.6833\n",
      "Epoch 496/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2422 - accuracy: 0.9041 - val_loss: 2.5144 - val_accuracy: 0.6625\n",
      "Epoch 497/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2909 - accuracy: 0.8842 - val_loss: 2.4121 - val_accuracy: 0.6500\n",
      "Epoch 498/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2804 - accuracy: 0.8943 - val_loss: 2.3032 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2392 - accuracy: 0.9096 - val_loss: 2.2408 - val_accuracy: 0.6917\n",
      "Epoch 500/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2254 - accuracy: 0.9096 - val_loss: 2.3263 - val_accuracy: 0.6958\n",
      "Epoch 501/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2193 - accuracy: 0.9087 - val_loss: 2.3406 - val_accuracy: 0.6708\n",
      "Epoch 502/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2495 - accuracy: 0.9027 - val_loss: 2.5332 - val_accuracy: 0.5750\n",
      "Epoch 503/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3273 - accuracy: 0.8837 - val_loss: 1.9070 - val_accuracy: 0.6958\n",
      "Epoch 504/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2563 - accuracy: 0.9110 - val_loss: 2.6729 - val_accuracy: 0.6250\n",
      "Epoch 505/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3158 - accuracy: 0.8911 - val_loss: 2.6128 - val_accuracy: 0.5542\n",
      "Epoch 506/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2680 - accuracy: 0.8920 - val_loss: 2.2615 - val_accuracy: 0.6750\n",
      "Epoch 507/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2629 - accuracy: 0.8967 - val_loss: 2.2926 - val_accuracy: 0.6792\n",
      "Epoch 508/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2571 - accuracy: 0.9036 - val_loss: 2.5220 - val_accuracy: 0.6167\n",
      "Epoch 509/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2214 - accuracy: 0.9203 - val_loss: 2.3865 - val_accuracy: 0.6333\n",
      "Epoch 510/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2579 - accuracy: 0.8985 - val_loss: 2.4264 - val_accuracy: 0.6625\n",
      "Epoch 511/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2401 - accuracy: 0.9133 - val_loss: 2.3391 - val_accuracy: 0.6750\n",
      "Epoch 512/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2534 - accuracy: 0.9059 - val_loss: 2.5685 - val_accuracy: 0.6667\n",
      "Epoch 513/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2518 - accuracy: 0.9018 - val_loss: 2.4109 - val_accuracy: 0.6708\n",
      "Epoch 514/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2352 - accuracy: 0.9073 - val_loss: 2.4260 - val_accuracy: 0.6667\n",
      "Epoch 515/800\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2243 - accuracy: 0.9129 - val_loss: 2.4600 - val_accuracy: 0.6792\n",
      "Epoch 516/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2568 - accuracy: 0.9004 - val_loss: 2.6095 - val_accuracy: 0.6625\n",
      "Epoch 517/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2225 - accuracy: 0.9212 - val_loss: 2.7654 - val_accuracy: 0.6375\n",
      "Epoch 518/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2497 - accuracy: 0.9092 - val_loss: 2.6055 - val_accuracy: 0.5917\n",
      "Epoch 519/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2390 - accuracy: 0.9092 - val_loss: 2.5948 - val_accuracy: 0.6708\n",
      "Epoch 520/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2734 - accuracy: 0.8976 - val_loss: 2.4021 - val_accuracy: 0.6875\n",
      "Epoch 521/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2755 - accuracy: 0.9064 - val_loss: 2.3593 - val_accuracy: 0.6708\n",
      "Epoch 522/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2327 - accuracy: 0.9078 - val_loss: 2.4307 - val_accuracy: 0.6708\n",
      "Epoch 523/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2512 - accuracy: 0.9092 - val_loss: 2.4532 - val_accuracy: 0.6625\n",
      "Epoch 524/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2322 - accuracy: 0.9101 - val_loss: 2.5924 - val_accuracy: 0.6375\n",
      "Epoch 525/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2487 - accuracy: 0.9050 - val_loss: 2.4391 - val_accuracy: 0.6792\n",
      "Epoch 526/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2354 - accuracy: 0.9082 - val_loss: 2.3978 - val_accuracy: 0.6875\n",
      "Epoch 527/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2375 - accuracy: 0.9129 - val_loss: 2.5497 - val_accuracy: 0.6958\n",
      "Epoch 528/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2338 - accuracy: 0.9082 - val_loss: 3.1669 - val_accuracy: 0.6542\n",
      "Epoch 529/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2418 - accuracy: 0.9092 - val_loss: 2.5494 - val_accuracy: 0.7042\n",
      "Epoch 530/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2347 - accuracy: 0.9133 - val_loss: 3.0006 - val_accuracy: 0.5375\n",
      "Epoch 531/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2356 - accuracy: 0.9036 - val_loss: 2.7616 - val_accuracy: 0.6250\n",
      "Epoch 532/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2553 - accuracy: 0.9073 - val_loss: 2.6243 - val_accuracy: 0.6125\n",
      "Epoch 533/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2342 - accuracy: 0.9115 - val_loss: 2.6309 - val_accuracy: 0.6792\n",
      "Epoch 534/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2430 - accuracy: 0.9050 - val_loss: 2.4717 - val_accuracy: 0.6917\n",
      "Epoch 535/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2274 - accuracy: 0.9166 - val_loss: 1.9672 - val_accuracy: 0.7000\n",
      "Epoch 536/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2278 - accuracy: 0.9101 - val_loss: 2.3925 - val_accuracy: 0.6208\n",
      "Epoch 537/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2451 - accuracy: 0.8990 - val_loss: 2.5143 - val_accuracy: 0.7125\n",
      "Epoch 538/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2473 - accuracy: 0.9027 - val_loss: 2.4689 - val_accuracy: 0.6583\n",
      "Epoch 539/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2598 - accuracy: 0.9055 - val_loss: 2.2513 - val_accuracy: 0.6875\n",
      "Epoch 540/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2651 - accuracy: 0.8994 - val_loss: 2.1394 - val_accuracy: 0.7167\n",
      "Epoch 541/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2602 - accuracy: 0.9059 - val_loss: 2.4519 - val_accuracy: 0.6542\n",
      "Epoch 542/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2226 - accuracy: 0.9106 - val_loss: 2.5338 - val_accuracy: 0.5875\n",
      "Epoch 543/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2143 - accuracy: 0.9268 - val_loss: 2.7791 - val_accuracy: 0.6417\n",
      "Epoch 544/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2651 - accuracy: 0.9092 - val_loss: 2.8481 - val_accuracy: 0.5833\n",
      "Epoch 545/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3621 - accuracy: 0.8814 - val_loss: 1.9468 - val_accuracy: 0.6625\n",
      "Epoch 546/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2279 - accuracy: 0.9110 - val_loss: 2.2344 - val_accuracy: 0.7125\n",
      "Epoch 547/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2424 - accuracy: 0.9064 - val_loss: 2.0216 - val_accuracy: 0.7083\n",
      "Epoch 548/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2345 - accuracy: 0.9124 - val_loss: 2.4365 - val_accuracy: 0.6750\n",
      "Epoch 549/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2336 - accuracy: 0.9120 - val_loss: 2.3306 - val_accuracy: 0.6708\n",
      "Epoch 550/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2360 - accuracy: 0.9027 - val_loss: 2.1642 - val_accuracy: 0.6750\n",
      "Epoch 551/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2373 - accuracy: 0.9120 - val_loss: 2.4314 - val_accuracy: 0.6458\n",
      "Epoch 552/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2225 - accuracy: 0.9152 - val_loss: 2.4858 - val_accuracy: 0.6583\n",
      "Epoch 553/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2208 - accuracy: 0.9161 - val_loss: 3.1378 - val_accuracy: 0.5667\n",
      "Epoch 554/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3166 - accuracy: 0.8930 - val_loss: 2.3053 - val_accuracy: 0.6708\n",
      "Epoch 555/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2288 - accuracy: 0.9096 - val_loss: 2.3204 - val_accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 556/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2115 - accuracy: 0.9217 - val_loss: 2.6071 - val_accuracy: 0.6167\n",
      "Epoch 557/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2217 - accuracy: 0.9120 - val_loss: 2.8306 - val_accuracy: 0.6125\n",
      "Epoch 558/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2319 - accuracy: 0.9022 - val_loss: 2.5758 - val_accuracy: 0.6833\n",
      "Epoch 559/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2235 - accuracy: 0.9184 - val_loss: 2.3504 - val_accuracy: 0.6917\n",
      "Epoch 560/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2386 - accuracy: 0.9120 - val_loss: 2.2562 - val_accuracy: 0.6583\n",
      "Epoch 561/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2128 - accuracy: 0.9138 - val_loss: 2.3433 - val_accuracy: 0.7250\n",
      "Epoch 562/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2101 - accuracy: 0.9240 - val_loss: 2.6088 - val_accuracy: 0.6833\n",
      "Epoch 563/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2341 - accuracy: 0.9110 - val_loss: 2.7001 - val_accuracy: 0.6750\n",
      "Epoch 564/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1935 - accuracy: 0.9175 - val_loss: 2.6149 - val_accuracy: 0.6875\n",
      "Epoch 565/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1991 - accuracy: 0.9198 - val_loss: 2.7620 - val_accuracy: 0.6542\n",
      "Epoch 566/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2245 - accuracy: 0.9106 - val_loss: 2.6399 - val_accuracy: 0.6958\n",
      "Epoch 567/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2000 - accuracy: 0.9175 - val_loss: 2.6580 - val_accuracy: 0.6417\n",
      "Epoch 568/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2209 - accuracy: 0.9147 - val_loss: 2.8724 - val_accuracy: 0.6667\n",
      "Epoch 569/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2332 - accuracy: 0.9161 - val_loss: 2.6424 - val_accuracy: 0.6833\n",
      "Epoch 570/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2147 - accuracy: 0.9217 - val_loss: 2.8404 - val_accuracy: 0.6708\n",
      "Epoch 571/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2748 - accuracy: 0.9064 - val_loss: 2.6352 - val_accuracy: 0.7208\n",
      "Epoch 572/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2903 - accuracy: 0.8999 - val_loss: 2.4133 - val_accuracy: 0.6500\n",
      "Epoch 573/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2206 - accuracy: 0.9189 - val_loss: 2.3105 - val_accuracy: 0.6792\n",
      "Epoch 574/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2045 - accuracy: 0.9259 - val_loss: 2.3881 - val_accuracy: 0.6583\n",
      "Epoch 575/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2023 - accuracy: 0.9245 - val_loss: 2.7237 - val_accuracy: 0.6458\n",
      "Epoch 576/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2199 - accuracy: 0.9133 - val_loss: 2.8500 - val_accuracy: 0.6625\n",
      "Epoch 577/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1847 - accuracy: 0.9282 - val_loss: 2.7666 - val_accuracy: 0.6333\n",
      "Epoch 578/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2331 - accuracy: 0.9147 - val_loss: 2.7868 - val_accuracy: 0.6333\n",
      "Epoch 579/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2064 - accuracy: 0.9194 - val_loss: 2.6926 - val_accuracy: 0.6375\n",
      "Epoch 580/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2210 - accuracy: 0.9157 - val_loss: 2.8697 - val_accuracy: 0.6083\n",
      "Epoch 581/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2164 - accuracy: 0.9189 - val_loss: 2.8002 - val_accuracy: 0.6625\n",
      "Epoch 582/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1986 - accuracy: 0.9272 - val_loss: 2.7692 - val_accuracy: 0.6667\n",
      "Epoch 583/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2354 - accuracy: 0.9161 - val_loss: 2.5943 - val_accuracy: 0.6833\n",
      "Epoch 584/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2005 - accuracy: 0.9282 - val_loss: 2.7216 - val_accuracy: 0.6792\n",
      "Epoch 585/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2184 - accuracy: 0.9166 - val_loss: 2.7992 - val_accuracy: 0.6625\n",
      "Epoch 586/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2145 - accuracy: 0.9184 - val_loss: 2.6614 - val_accuracy: 0.6542\n",
      "Epoch 587/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2040 - accuracy: 0.9240 - val_loss: 3.2390 - val_accuracy: 0.6292\n",
      "Epoch 588/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2422 - accuracy: 0.9004 - val_loss: 2.4537 - val_accuracy: 0.7000\n",
      "Epoch 589/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2047 - accuracy: 0.9212 - val_loss: 2.7971 - val_accuracy: 0.6375\n",
      "Epoch 590/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1881 - accuracy: 0.9212 - val_loss: 2.8853 - val_accuracy: 0.6833\n",
      "Epoch 591/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2282 - accuracy: 0.9189 - val_loss: 2.6101 - val_accuracy: 0.6083\n",
      "Epoch 592/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2177 - accuracy: 0.9249 - val_loss: 2.7077 - val_accuracy: 0.6500\n",
      "Epoch 593/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2230 - accuracy: 0.9203 - val_loss: 2.7934 - val_accuracy: 0.7000\n",
      "Epoch 594/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2398 - accuracy: 0.9166 - val_loss: 2.5862 - val_accuracy: 0.6417\n",
      "Epoch 595/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1754 - accuracy: 0.9314 - val_loss: 2.6840 - val_accuracy: 0.6667\n",
      "Epoch 596/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1977 - accuracy: 0.9240 - val_loss: 2.5970 - val_accuracy: 0.6833\n",
      "Epoch 597/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2530 - accuracy: 0.9198 - val_loss: 2.2923 - val_accuracy: 0.6708\n",
      "Epoch 598/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2337 - accuracy: 0.9106 - val_loss: 2.7513 - val_accuracy: 0.6500\n",
      "Epoch 599/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2239 - accuracy: 0.9129 - val_loss: 2.5557 - val_accuracy: 0.6792\n",
      "Epoch 600/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2074 - accuracy: 0.9263 - val_loss: 2.4064 - val_accuracy: 0.7042\n",
      "Epoch 601/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1798 - accuracy: 0.9291 - val_loss: 2.5364 - val_accuracy: 0.6875\n",
      "Epoch 602/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1930 - accuracy: 0.9235 - val_loss: 2.8588 - val_accuracy: 0.6458\n",
      "Epoch 603/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2237 - accuracy: 0.9171 - val_loss: 2.7546 - val_accuracy: 0.6333\n",
      "Epoch 604/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1847 - accuracy: 0.9305 - val_loss: 2.5857 - val_accuracy: 0.6667\n",
      "Epoch 605/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2170 - accuracy: 0.9171 - val_loss: 2.7085 - val_accuracy: 0.6208\n",
      "Epoch 606/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1997 - accuracy: 0.9291 - val_loss: 2.6286 - val_accuracy: 0.6833\n",
      "Epoch 607/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2138 - accuracy: 0.9212 - val_loss: 2.4932 - val_accuracy: 0.7000\n",
      "Epoch 608/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1914 - accuracy: 0.9300 - val_loss: 2.9310 - val_accuracy: 0.6458\n",
      "Epoch 609/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1939 - accuracy: 0.9235 - val_loss: 2.5013 - val_accuracy: 0.6583\n",
      "Epoch 610/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2267 - accuracy: 0.9184 - val_loss: 2.5793 - val_accuracy: 0.6875\n",
      "Epoch 611/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2422 - accuracy: 0.9198 - val_loss: 2.5591 - val_accuracy: 0.6667\n",
      "Epoch 612/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2263 - accuracy: 0.9115 - val_loss: 2.3559 - val_accuracy: 0.7042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 613/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2105 - accuracy: 0.9208 - val_loss: 2.6147 - val_accuracy: 0.7000\n",
      "Epoch 614/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1891 - accuracy: 0.9240 - val_loss: 2.7715 - val_accuracy: 0.6625\n",
      "Epoch 615/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1879 - accuracy: 0.9300 - val_loss: 3.1062 - val_accuracy: 0.6417\n",
      "Epoch 616/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2037 - accuracy: 0.9208 - val_loss: 2.5751 - val_accuracy: 0.6792\n",
      "Epoch 617/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2204 - accuracy: 0.9198 - val_loss: 2.5505 - val_accuracy: 0.6708\n",
      "Epoch 618/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2063 - accuracy: 0.9249 - val_loss: 2.9374 - val_accuracy: 0.5833\n",
      "Epoch 619/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2004 - accuracy: 0.9254 - val_loss: 2.6704 - val_accuracy: 0.6833\n",
      "Epoch 620/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1834 - accuracy: 0.9235 - val_loss: 2.9560 - val_accuracy: 0.6750\n",
      "Epoch 621/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1883 - accuracy: 0.9314 - val_loss: 2.9953 - val_accuracy: 0.6500\n",
      "Epoch 622/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2321 - accuracy: 0.9133 - val_loss: 2.6689 - val_accuracy: 0.6167\n",
      "Epoch 623/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2296 - accuracy: 0.9124 - val_loss: 2.8854 - val_accuracy: 0.6500\n",
      "Epoch 624/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2048 - accuracy: 0.9217 - val_loss: 2.3737 - val_accuracy: 0.6833\n",
      "Epoch 625/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1983 - accuracy: 0.9282 - val_loss: 2.3251 - val_accuracy: 0.6667\n",
      "Epoch 626/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2259 - accuracy: 0.9152 - val_loss: 2.7987 - val_accuracy: 0.6292\n",
      "Epoch 627/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2234 - accuracy: 0.9175 - val_loss: 2.4667 - val_accuracy: 0.6708\n",
      "Epoch 628/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1983 - accuracy: 0.9268 - val_loss: 2.5974 - val_accuracy: 0.6750\n",
      "Epoch 629/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1926 - accuracy: 0.9259 - val_loss: 2.6630 - val_accuracy: 0.6833\n",
      "Epoch 630/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1860 - accuracy: 0.9272 - val_loss: 2.5320 - val_accuracy: 0.6750\n",
      "Epoch 631/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1828 - accuracy: 0.9268 - val_loss: 2.7765 - val_accuracy: 0.6792\n",
      "Epoch 632/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2123 - accuracy: 0.9189 - val_loss: 2.4894 - val_accuracy: 0.7125\n",
      "Epoch 633/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1794 - accuracy: 0.9411 - val_loss: 2.6337 - val_accuracy: 0.6292\n",
      "Epoch 634/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1843 - accuracy: 0.9286 - val_loss: 2.9289 - val_accuracy: 0.6375\n",
      "Epoch 635/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1969 - accuracy: 0.9259 - val_loss: 2.6767 - val_accuracy: 0.6833\n",
      "Epoch 636/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1944 - accuracy: 0.9282 - val_loss: 2.8090 - val_accuracy: 0.7042\n",
      "Epoch 637/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1985 - accuracy: 0.9259 - val_loss: 2.7829 - val_accuracy: 0.7000\n",
      "Epoch 638/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1756 - accuracy: 0.9356 - val_loss: 2.7130 - val_accuracy: 0.6875\n",
      "Epoch 639/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1866 - accuracy: 0.9300 - val_loss: 2.6845 - val_accuracy: 0.6667\n",
      "Epoch 640/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1876 - accuracy: 0.9286 - val_loss: 2.7152 - val_accuracy: 0.6667\n",
      "Epoch 641/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2177 - accuracy: 0.9268 - val_loss: 2.8316 - val_accuracy: 0.6792\n",
      "Epoch 642/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2232 - accuracy: 0.9217 - val_loss: 2.5463 - val_accuracy: 0.6333\n",
      "Epoch 643/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2086 - accuracy: 0.9249 - val_loss: 2.3451 - val_accuracy: 0.7083\n",
      "Epoch 644/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2061 - accuracy: 0.9222 - val_loss: 2.5475 - val_accuracy: 0.6375\n",
      "Epoch 645/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1998 - accuracy: 0.9249 - val_loss: 2.6252 - val_accuracy: 0.6542\n",
      "Epoch 646/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2211 - accuracy: 0.9198 - val_loss: 2.5238 - val_accuracy: 0.7125\n",
      "Epoch 647/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1863 - accuracy: 0.9296 - val_loss: 2.5957 - val_accuracy: 0.6625\n",
      "Epoch 648/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1860 - accuracy: 0.9240 - val_loss: 2.6225 - val_accuracy: 0.6583\n",
      "Epoch 649/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2430 - accuracy: 0.9198 - val_loss: 2.2922 - val_accuracy: 0.6792\n",
      "Epoch 650/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1670 - accuracy: 0.9402 - val_loss: 2.5327 - val_accuracy: 0.6917\n",
      "Epoch 651/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1688 - accuracy: 0.9333 - val_loss: 2.8143 - val_accuracy: 0.6375\n",
      "Epoch 652/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1789 - accuracy: 0.9314 - val_loss: 2.7445 - val_accuracy: 0.6917\n",
      "Epoch 653/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1900 - accuracy: 0.9272 - val_loss: 2.8220 - val_accuracy: 0.6750\n",
      "Epoch 654/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1905 - accuracy: 0.9259 - val_loss: 3.1061 - val_accuracy: 0.6125\n",
      "Epoch 655/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1833 - accuracy: 0.9305 - val_loss: 2.9066 - val_accuracy: 0.6958\n",
      "Epoch 656/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1932 - accuracy: 0.9291 - val_loss: 3.2476 - val_accuracy: 0.6667\n",
      "Epoch 657/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2160 - accuracy: 0.9194 - val_loss: 2.5836 - val_accuracy: 0.7000\n",
      "Epoch 658/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2007 - accuracy: 0.9291 - val_loss: 2.4546 - val_accuracy: 0.6500\n",
      "Epoch 659/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1932 - accuracy: 0.9314 - val_loss: 2.6282 - val_accuracy: 0.6500\n",
      "Epoch 660/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1823 - accuracy: 0.9305 - val_loss: 2.9148 - val_accuracy: 0.6500\n",
      "Epoch 661/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1758 - accuracy: 0.9319 - val_loss: 2.8334 - val_accuracy: 0.6292\n",
      "Epoch 662/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2201 - accuracy: 0.9180 - val_loss: 2.4573 - val_accuracy: 0.6792\n",
      "Epoch 663/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1790 - accuracy: 0.9351 - val_loss: 2.8208 - val_accuracy: 0.6625\n",
      "Epoch 664/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1827 - accuracy: 0.9305 - val_loss: 2.7395 - val_accuracy: 0.6792\n",
      "Epoch 665/800\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.1650 - accuracy: 0.9388 - val_loss: 2.7047 - val_accuracy: 0.7083\n",
      "Epoch 666/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1865 - accuracy: 0.9323 - val_loss: 2.7659 - val_accuracy: 0.7167\n",
      "Epoch 667/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1682 - accuracy: 0.9342 - val_loss: 3.2436 - val_accuracy: 0.6500\n",
      "Epoch 668/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2070 - accuracy: 0.9240 - val_loss: 2.7100 - val_accuracy: 0.6667\n",
      "Epoch 669/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1907 - accuracy: 0.9314 - val_loss: 2.4754 - val_accuracy: 0.6625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1783 - accuracy: 0.9319 - val_loss: 2.6240 - val_accuracy: 0.6833\n",
      "Epoch 671/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1677 - accuracy: 0.9435 - val_loss: 2.8942 - val_accuracy: 0.7083\n",
      "Epoch 672/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1684 - accuracy: 0.9411 - val_loss: 3.1296 - val_accuracy: 0.6625\n",
      "Epoch 673/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2046 - accuracy: 0.9226 - val_loss: 2.6942 - val_accuracy: 0.6792\n",
      "Epoch 674/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1926 - accuracy: 0.9347 - val_loss: 2.7254 - val_accuracy: 0.6917\n",
      "Epoch 675/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1821 - accuracy: 0.9333 - val_loss: 3.2209 - val_accuracy: 0.5917\n",
      "Epoch 676/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1745 - accuracy: 0.9323 - val_loss: 2.8719 - val_accuracy: 0.6875\n",
      "Epoch 677/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2427 - accuracy: 0.9171 - val_loss: 2.6626 - val_accuracy: 0.6667\n",
      "Epoch 678/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1767 - accuracy: 0.9402 - val_loss: 2.5889 - val_accuracy: 0.7042\n",
      "Epoch 679/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2040 - accuracy: 0.9203 - val_loss: 2.7294 - val_accuracy: 0.6875\n",
      "Epoch 680/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1892 - accuracy: 0.9300 - val_loss: 2.5982 - val_accuracy: 0.6708\n",
      "Epoch 681/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1526 - accuracy: 0.9444 - val_loss: 2.5388 - val_accuracy: 0.7250\n",
      "Epoch 682/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1970 - accuracy: 0.9212 - val_loss: 2.6282 - val_accuracy: 0.7042\n",
      "Epoch 683/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1599 - accuracy: 0.9398 - val_loss: 2.7714 - val_accuracy: 0.6542\n",
      "Epoch 684/800\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.1827 - accuracy: 0.9254 - val_loss: 3.1295 - val_accuracy: 0.6250\n",
      "Epoch 685/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1835 - accuracy: 0.9365 - val_loss: 2.7617 - val_accuracy: 0.6708\n",
      "Epoch 686/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1851 - accuracy: 0.9235 - val_loss: 2.7369 - val_accuracy: 0.6417\n",
      "Epoch 687/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1548 - accuracy: 0.9430 - val_loss: 2.7976 - val_accuracy: 0.6750\n",
      "Epoch 688/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1643 - accuracy: 0.9379 - val_loss: 2.7274 - val_accuracy: 0.6958\n",
      "Epoch 689/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1916 - accuracy: 0.9328 - val_loss: 2.8983 - val_accuracy: 0.6583\n",
      "Epoch 690/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2009 - accuracy: 0.9254 - val_loss: 3.1367 - val_accuracy: 0.6792\n",
      "Epoch 691/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2025 - accuracy: 0.9282 - val_loss: 2.6016 - val_accuracy: 0.7125\n",
      "Epoch 692/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1860 - accuracy: 0.9296 - val_loss: 3.2996 - val_accuracy: 0.6542\n",
      "Epoch 693/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1935 - accuracy: 0.9337 - val_loss: 2.7581 - val_accuracy: 0.6458\n",
      "Epoch 694/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1510 - accuracy: 0.9402 - val_loss: 2.8078 - val_accuracy: 0.6417\n",
      "Epoch 695/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1863 - accuracy: 0.9351 - val_loss: 2.8545 - val_accuracy: 0.6750\n",
      "Epoch 696/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1926 - accuracy: 0.9310 - val_loss: 2.5489 - val_accuracy: 0.6833\n",
      "Epoch 697/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1818 - accuracy: 0.9398 - val_loss: 2.4704 - val_accuracy: 0.6667\n",
      "Epoch 698/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1689 - accuracy: 0.9361 - val_loss: 2.9801 - val_accuracy: 0.6500\n",
      "Epoch 699/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1925 - accuracy: 0.9291 - val_loss: 2.5651 - val_accuracy: 0.6750\n",
      "Epoch 700/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1542 - accuracy: 0.9467 - val_loss: 2.9494 - val_accuracy: 0.6458\n",
      "Epoch 701/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1738 - accuracy: 0.9310 - val_loss: 2.8050 - val_accuracy: 0.6500\n",
      "Epoch 702/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1646 - accuracy: 0.9388 - val_loss: 2.7874 - val_accuracy: 0.6292\n",
      "Epoch 703/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2047 - accuracy: 0.9272 - val_loss: 2.4677 - val_accuracy: 0.6750\n",
      "Epoch 704/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1533 - accuracy: 0.9449 - val_loss: 2.5267 - val_accuracy: 0.6958\n",
      "Epoch 705/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1553 - accuracy: 0.9402 - val_loss: 2.8068 - val_accuracy: 0.6417\n",
      "Epoch 706/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1511 - accuracy: 0.9393 - val_loss: 2.6068 - val_accuracy: 0.7042\n",
      "Epoch 707/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1909 - accuracy: 0.9286 - val_loss: 2.8006 - val_accuracy: 0.6958\n",
      "Epoch 708/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1672 - accuracy: 0.9361 - val_loss: 3.2597 - val_accuracy: 0.6458\n",
      "Epoch 709/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2129 - accuracy: 0.9268 - val_loss: 2.8568 - val_accuracy: 0.6792\n",
      "Epoch 710/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1578 - accuracy: 0.9384 - val_loss: 2.8572 - val_accuracy: 0.6833\n",
      "Epoch 711/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1648 - accuracy: 0.9356 - val_loss: 2.4327 - val_accuracy: 0.7042\n",
      "Epoch 712/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1788 - accuracy: 0.9300 - val_loss: 2.9674 - val_accuracy: 0.6583\n",
      "Epoch 713/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1949 - accuracy: 0.9347 - val_loss: 2.7793 - val_accuracy: 0.6625\n",
      "Epoch 714/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1834 - accuracy: 0.9407 - val_loss: 2.9118 - val_accuracy: 0.5667\n",
      "Epoch 715/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1662 - accuracy: 0.9337 - val_loss: 2.9166 - val_accuracy: 0.6792\n",
      "Epoch 716/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1660 - accuracy: 0.9356 - val_loss: 3.0115 - val_accuracy: 0.6750\n",
      "Epoch 717/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1489 - accuracy: 0.9449 - val_loss: 2.6067 - val_accuracy: 0.6833\n",
      "Epoch 718/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1678 - accuracy: 0.9374 - val_loss: 3.0774 - val_accuracy: 0.6625\n",
      "Epoch 719/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1605 - accuracy: 0.9453 - val_loss: 3.1998 - val_accuracy: 0.6208\n",
      "Epoch 720/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1738 - accuracy: 0.9342 - val_loss: 3.2871 - val_accuracy: 0.6583\n",
      "Epoch 721/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1775 - accuracy: 0.9323 - val_loss: 4.0367 - val_accuracy: 0.5500\n",
      "Epoch 722/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1997 - accuracy: 0.9231 - val_loss: 3.2237 - val_accuracy: 0.6208\n",
      "Epoch 723/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1988 - accuracy: 0.9296 - val_loss: 2.6868 - val_accuracy: 0.6750\n",
      "Epoch 724/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1849 - accuracy: 0.9286 - val_loss: 2.6858 - val_accuracy: 0.6500\n",
      "Epoch 725/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1932 - accuracy: 0.9286 - val_loss: 2.8206 - val_accuracy: 0.6708\n",
      "Epoch 726/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1450 - accuracy: 0.9449 - val_loss: 2.7270 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 727/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1608 - accuracy: 0.9351 - val_loss: 3.1461 - val_accuracy: 0.6250\n",
      "Epoch 728/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1671 - accuracy: 0.9379 - val_loss: 3.0428 - val_accuracy: 0.6708\n",
      "Epoch 729/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1702 - accuracy: 0.9337 - val_loss: 3.0512 - val_accuracy: 0.6625\n",
      "Epoch 730/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1674 - accuracy: 0.9370 - val_loss: 3.2056 - val_accuracy: 0.6542\n",
      "Epoch 731/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1755 - accuracy: 0.9296 - val_loss: 2.9120 - val_accuracy: 0.7125\n",
      "Epoch 732/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2060 - accuracy: 0.9184 - val_loss: 2.9288 - val_accuracy: 0.6833\n",
      "Epoch 733/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1767 - accuracy: 0.9356 - val_loss: 2.9472 - val_accuracy: 0.6583\n",
      "Epoch 734/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1398 - accuracy: 0.9486 - val_loss: 2.8842 - val_accuracy: 0.6750\n",
      "Epoch 735/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1373 - accuracy: 0.9523 - val_loss: 2.9947 - val_accuracy: 0.6458\n",
      "Epoch 736/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1473 - accuracy: 0.9435 - val_loss: 3.2062 - val_accuracy: 0.6875\n",
      "Epoch 737/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1585 - accuracy: 0.9402 - val_loss: 3.0262 - val_accuracy: 0.6833\n",
      "Epoch 738/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1585 - accuracy: 0.9416 - val_loss: 3.0126 - val_accuracy: 0.6667\n",
      "Epoch 739/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1669 - accuracy: 0.9393 - val_loss: 3.1250 - val_accuracy: 0.6708\n",
      "Epoch 740/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.2133 - accuracy: 0.9291 - val_loss: 2.9499 - val_accuracy: 0.6750\n",
      "Epoch 741/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1522 - accuracy: 0.9435 - val_loss: 3.0250 - val_accuracy: 0.6625\n",
      "Epoch 742/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2199 - accuracy: 0.9180 - val_loss: 2.8619 - val_accuracy: 0.6708\n",
      "Epoch 743/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1725 - accuracy: 0.9374 - val_loss: 2.9034 - val_accuracy: 0.6792\n",
      "Epoch 744/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1662 - accuracy: 0.9388 - val_loss: 2.7944 - val_accuracy: 0.6833\n",
      "Epoch 745/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1613 - accuracy: 0.9449 - val_loss: 2.9673 - val_accuracy: 0.6542\n",
      "Epoch 746/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1637 - accuracy: 0.9421 - val_loss: 2.9119 - val_accuracy: 0.6792\n",
      "Epoch 747/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1686 - accuracy: 0.9328 - val_loss: 2.8316 - val_accuracy: 0.6958\n",
      "Epoch 748/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1405 - accuracy: 0.9462 - val_loss: 3.0520 - val_accuracy: 0.6375\n",
      "Epoch 749/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1355 - accuracy: 0.9500 - val_loss: 3.1372 - val_accuracy: 0.6625\n",
      "Epoch 750/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1753 - accuracy: 0.9319 - val_loss: 3.0364 - val_accuracy: 0.6750\n",
      "Epoch 751/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1766 - accuracy: 0.9365 - val_loss: 3.4236 - val_accuracy: 0.5792\n",
      "Epoch 752/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2218 - accuracy: 0.9231 - val_loss: 2.4925 - val_accuracy: 0.7042\n",
      "Epoch 753/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1614 - accuracy: 0.9416 - val_loss: 2.8387 - val_accuracy: 0.6583\n",
      "Epoch 754/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1683 - accuracy: 0.9407 - val_loss: 2.7796 - val_accuracy: 0.6750\n",
      "Epoch 755/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1539 - accuracy: 0.9472 - val_loss: 3.0249 - val_accuracy: 0.6417\n",
      "Epoch 756/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1507 - accuracy: 0.9430 - val_loss: 2.9392 - val_accuracy: 0.6750\n",
      "Epoch 757/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1632 - accuracy: 0.9379 - val_loss: 3.1783 - val_accuracy: 0.6375\n",
      "Epoch 758/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1711 - accuracy: 0.9398 - val_loss: 2.8619 - val_accuracy: 0.6417\n",
      "Epoch 759/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1559 - accuracy: 0.9388 - val_loss: 3.0331 - val_accuracy: 0.6625\n",
      "Epoch 760/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1867 - accuracy: 0.9319 - val_loss: 3.0242 - val_accuracy: 0.6708\n",
      "Epoch 761/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1917 - accuracy: 0.9240 - val_loss: 3.1120 - val_accuracy: 0.6750\n",
      "Epoch 762/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1734 - accuracy: 0.9411 - val_loss: 2.9858 - val_accuracy: 0.6875\n",
      "Epoch 763/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1356 - accuracy: 0.9490 - val_loss: 3.3340 - val_accuracy: 0.6292\n",
      "Epoch 764/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1505 - accuracy: 0.9449 - val_loss: 3.4679 - val_accuracy: 0.6000\n",
      "Epoch 765/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1674 - accuracy: 0.9402 - val_loss: 2.8330 - val_accuracy: 0.6542\n",
      "Epoch 766/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1573 - accuracy: 0.9439 - val_loss: 3.6988 - val_accuracy: 0.5458\n",
      "Epoch 767/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1611 - accuracy: 0.9449 - val_loss: 3.0695 - val_accuracy: 0.6500\n",
      "Epoch 768/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1956 - accuracy: 0.9328 - val_loss: 2.9332 - val_accuracy: 0.6542\n",
      "Epoch 769/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2059 - accuracy: 0.9263 - val_loss: 3.0329 - val_accuracy: 0.6542\n",
      "Epoch 770/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1317 - accuracy: 0.9560 - val_loss: 3.0927 - val_accuracy: 0.6917\n",
      "Epoch 771/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1733 - accuracy: 0.9361 - val_loss: 2.8960 - val_accuracy: 0.6958\n",
      "Epoch 772/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1691 - accuracy: 0.9393 - val_loss: 2.8434 - val_accuracy: 0.7292\n",
      "Epoch 773/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1553 - accuracy: 0.9384 - val_loss: 2.8505 - val_accuracy: 0.7000\n",
      "Epoch 774/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1777 - accuracy: 0.9384 - val_loss: 2.7259 - val_accuracy: 0.6750\n",
      "Epoch 775/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1421 - accuracy: 0.9490 - val_loss: 3.0181 - val_accuracy: 0.6542\n",
      "Epoch 776/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1509 - accuracy: 0.9467 - val_loss: 3.0793 - val_accuracy: 0.6958\n",
      "Epoch 777/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1565 - accuracy: 0.9421 - val_loss: 3.1037 - val_accuracy: 0.6833\n",
      "Epoch 778/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1569 - accuracy: 0.9398 - val_loss: 2.9937 - val_accuracy: 0.6792\n",
      "Epoch 779/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1374 - accuracy: 0.9467 - val_loss: 3.1129 - val_accuracy: 0.6708\n",
      "Epoch 780/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1411 - accuracy: 0.9500 - val_loss: 2.9714 - val_accuracy: 0.7000\n",
      "Epoch 781/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1417 - accuracy: 0.9486 - val_loss: 3.0495 - val_accuracy: 0.7292\n",
      "Epoch 782/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1655 - accuracy: 0.9416 - val_loss: 3.0446 - val_accuracy: 0.7000\n",
      "Epoch 783/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1392 - accuracy: 0.9486 - val_loss: 3.1656 - val_accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 784/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1788 - accuracy: 0.9314 - val_loss: 2.8170 - val_accuracy: 0.6792\n",
      "Epoch 785/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1654 - accuracy: 0.9411 - val_loss: 3.0252 - val_accuracy: 0.6875\n",
      "Epoch 786/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1483 - accuracy: 0.9500 - val_loss: 2.8743 - val_accuracy: 0.6917\n",
      "Epoch 787/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1538 - accuracy: 0.9425 - val_loss: 3.0313 - val_accuracy: 0.6792\n",
      "Epoch 788/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1313 - accuracy: 0.9500 - val_loss: 2.8906 - val_accuracy: 0.6833\n",
      "Epoch 789/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1638 - accuracy: 0.9421 - val_loss: 4.0367 - val_accuracy: 0.5750\n",
      "Epoch 790/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1697 - accuracy: 0.9384 - val_loss: 2.9241 - val_accuracy: 0.6875\n",
      "Epoch 791/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1406 - accuracy: 0.9523 - val_loss: 3.6997 - val_accuracy: 0.5500\n",
      "Epoch 792/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1968 - accuracy: 0.9296 - val_loss: 2.7988 - val_accuracy: 0.6458\n",
      "Epoch 793/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1615 - accuracy: 0.9365 - val_loss: 3.1760 - val_accuracy: 0.6875\n",
      "Epoch 794/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1392 - accuracy: 0.9518 - val_loss: 3.2031 - val_accuracy: 0.7083\n",
      "Epoch 795/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1228 - accuracy: 0.9541 - val_loss: 3.4850 - val_accuracy: 0.6958\n",
      "Epoch 796/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1785 - accuracy: 0.9333 - val_loss: 2.8966 - val_accuracy: 0.6583\n",
      "Epoch 797/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1448 - accuracy: 0.9504 - val_loss: 3.3003 - val_accuracy: 0.6667\n",
      "Epoch 798/800\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.1547 - accuracy: 0.9384 - val_loss: 2.9511 - val_accuracy: 0.6958\n",
      "Epoch 799/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1412 - accuracy: 0.9462 - val_loss: 3.1467 - val_accuracy: 0.6958\n",
      "Epoch 800/800\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.1389 - accuracy: 0.9453 - val_loss: 2.8084 - val_accuracy: 0.7167\n"
     ]
    }
   ],
   "source": [
    "model_d = modelizer(x_train, x_test, d_train, d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c7435e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
       "         0,  1],\n",
       "       [ 2,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  2],\n",
       "       [ 0,  0, 20,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  6,  0,  1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0, 11,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  3,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  2, 12,  1,  0,  0,  0,  1,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  1,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0, 11,  1,  0,  0,  0,  0,\n",
       "         0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  9,  0,  0,  0,  0,\n",
       "         1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  1,\n",
       "         1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  9,  4,  1,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3, 22,  0,\n",
       "         0,  0],\n",
       "       [ 1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  2,  6,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,\n",
       "        12,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,\n",
       "         0,  8]], dtype=int64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_a.predict(x_test)\n",
    "t_a = [argmax(y3) for y3 in y_test]\n",
    "p_a = [argmax(pr) for pr in preds]\n",
    "sk.confusion_matrix(t_a,p_a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c5d021aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 20: 1,\n",
       " 40: 2,\n",
       " 60: 3,\n",
       " 80: 4,\n",
       " 100: 5,\n",
       " 120: 6,\n",
       " 140: 7,\n",
       " 160: 8,\n",
       " 180: 9,\n",
       " 200: 10,\n",
       " 220: 11,\n",
       " 240: 12,\n",
       " 260: 13,\n",
       " 280: 14,\n",
       " 300: 15,\n",
       " 320: 16,\n",
       " 340: 17}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = np.unique(np.array(y_binned).astype(int))\n",
    "keys = np.array(range(len(vals)))\n",
    "ang_dict =  {val:key for key, val in zip(keys, vals)}\n",
    "ang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c9800c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0539ec61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [val for val in ang_dict.values()]\n",
    "import seaborn as sns   \n",
    "conf_a = sk.confusion_matrix(t_a,p_a )\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(conf_a, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "37f203f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_extended(conf_matrix):\n",
    "    subtract = 0\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            if np.abs(j-i)<2: subtract += conf_matrix[i, j]\n",
    "    return np.sum(conf_matrix) - subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1504a015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89% accuracy within 30 degrees\n"
     ]
    }
   ],
   "source": [
    "print(f'{100*(len(t_a)-acc_extended(conf_a))/len(t_a):2.0f}% accuracy within 30 degrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "954663f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  1,  0,  0,  0,  0,  1,  0,  0,  1,  0,  1,  1,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  4,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  4,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  4,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  0,  1,  8,  2,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  0,  7,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  2,  7,  1,  0,  0,  1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  6,  0,  1,  0,  1,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  2,  1,  0,  0,  0,  1,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  4,  4,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  1,  1,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  1,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  5,\n",
       "         2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "         8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  1,  5,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  1,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  1, 10,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_d.predict(x_test)\n",
    "t_d = [argmax(y3) for y3 in d_test]\n",
    "p_d = [argmax(pr) for pr in preds]\n",
    "sk.confusion_matrix(t_d,p_d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c3e599b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0,\n",
       " 20: 1,\n",
       " 30: 2,\n",
       " 40: 3,\n",
       " 50: 4,\n",
       " 60: 5,\n",
       " 70: 6,\n",
       " 80: 7,\n",
       " 90: 8,\n",
       " 100: 9,\n",
       " 110: 10,\n",
       " 120: 11,\n",
       " 130: 12,\n",
       " 140: 13,\n",
       " 150: 14,\n",
       " 160: 15,\n",
       " 170: 16,\n",
       " 180: 17,\n",
       " 190: 18,\n",
       " 200: 19,\n",
       " 210: 20,\n",
       " 220: 21,\n",
       " 230: 22,\n",
       " 240: 23,\n",
       " 250: 24,\n",
       " 260: 25,\n",
       " 270: 26,\n",
       " 280: 27,\n",
       " 290: 28}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = np.unique(np.array(d_binned).astype(int))\n",
    "keys = np.array(range(len(vals)))\n",
    "dist_dict =  {val:key for key, val in zip(keys[:-1], vals[:-1])}\n",
    "dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f3c2558d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (15), usually from a call to set_ticks, does not match the number of ticklabels (29).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-b7f0de1d18be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'True labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Confusion Matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_ticklabels\u001b[1;34m(self, ticklabels, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1709\u001b[0m             \u001b[1;31m# remove all tick labels, so only error for > 0 ticklabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1711\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1712\u001b[0m                     \u001b[1;34m\"The number of FixedLocator locations\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m                     \u001b[1;34mf\" ({len(locator.locs)}), usually from a call to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The number of FixedLocator locations (15), usually from a call to set_ticks, does not match the number of ticklabels (29)."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [val for val in dist_dict.values()]\n",
    "import seaborn as sns   \n",
    "conf_d = sk.confusion_matrix(t_d,p_d )\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(conf_d, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "88dbd498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94% accuracy within 15 meters\n"
     ]
    }
   ],
   "source": [
    "print(f'{100*(len(t_a)-acc_extended(conf_d))/len(t_a):2.0f}% accuracy within 15 meters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61071a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.save('saved_model/model_angle_1.h5')\n",
    "model_d.save('saved_model/model_distance_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d70864",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, sr = librosa.load('./AudioClips/'+files[2][20])\n",
    "files[2][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e515ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.predict(np.reshape(obs[1000:16000], [1, 15000, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802867d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAAIenv",
   "language": "python",
   "name": "saaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
